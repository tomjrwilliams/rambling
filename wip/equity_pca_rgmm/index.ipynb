{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Equity: Rolling Factor Clustering\"\n",
        "author: \"Tom Williams\"\n",
        "date: \"2023-08-08\"\n",
        "categories: []\n",
        "draft: false\n",
        "execute:\n",
        "  daemon: false\n",
        "---"
      ],
      "id": "d1911f35"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Work In Progress\n",
        "\n",
        "In this post, we demonstrate how to use Gaussian Mixture Models to cluster the factors extracted from a rolling PCA over daily equity return data.\n",
        "\n",
        "## Setup\n"
      ],
      "id": "a14fd2d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| code-fold: true\n",
        "#| code-summary: \"Auto reload\"\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "id": "b4369371",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| code-fold: true\n",
        "#| code-summary: \"Environment\"\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "sys.path.append(\"C:/hc/src\")\n",
        "sys.path.append(\"C:/hc/rambling\")\n",
        "sys.path.append(\"C:/hc/xfactors/src\")\n",
        "sys.path.append(\"C:/hc/xtuples/src\")\n",
        "os.environ[\"MODULE\"] = \"c:/hc/src/\""
      ],
      "id": "54ea3d4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "#| code-summary: \"Imports\"\n",
        "import numpy\n",
        "import pandas\n",
        "import jax\n",
        "import jax.numpy\n",
        "\n",
        "import xtuples as xt\n",
        "import xfactors as xf\n",
        "\n",
        "import hcbt.data.prices.int as prices\n",
        "import hcbt.algos.universe.int as universe\n",
        "import hcbt.algos.universe.configs as configs"
      ],
      "id": "c0f6e56d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data\n"
      ],
      "id": "22743775"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "#| code-summary: \"Returns\"\n",
        "def vol_scale(df, alpha = 2 / 90):\n",
        "    std = df.ewm(alpha = alpha).std()\n",
        "    return df.divide(std)"
      ],
      "id": "936f2ef0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "#| code-summary: \"Returns\"\n",
        "df_returns = prices.returns_df(\n",
        "    xf.utils.dates.y(2005),\n",
        "    xf.utils.dates.y(2023, m=4),\n",
        "    indices=configs.INDICES,\n",
        ")\n",
        "# df_returns = vol_scale(vol_scale)"
      ],
      "id": "c906b4cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "#| code-summary: \"Returns\"\n",
        "df_returns.head()"
      ],
      "id": "0da6ec37",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The returns are from bloomberg, and include all cash and non-cash adjustments (in production we have a separate internal ticker for the dividend stream from a given name, but that's a little over-complicated for our purposes here).\n",
        "\n",
        "We'll also load the relevant index membership mapping tables for our universe:\n"
      ],
      "id": "c3a22a6d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "#| code-summary: \"Index Membership\"\n",
        "dfs_indices = universe.rolling_indices(\n",
        "    xf.utils.dates.y(2005),\n",
        "    xf.utils.dates.y(2023, m=4),\n",
        "    indices=configs.INDICES,\n",
        ")\n",
        "df_universe = universe.index_union(dfs_indices)"
      ],
      "id": "a1f01d7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "#| code-summary: \"Sector Membership\"\n",
        "dfs_sectors = universe.rolling_indices(\n",
        "    xf.utils.dates.y(2005),\n",
        "    xf.utils.dates.y(2023, m=4),\n",
        "    sectors=configs.GICS_SECTORS,\n",
        ")"
      ],
      "id": "e669188e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model\n",
        "\n",
        "\n",
        "Rolling pca -> clustering\n"
      ],
      "id": "b13c7b49"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "#| code-summary: \"Model\"\n",
        "N_STAGES = 5\n",
        "model, STAGES = xf.Model().init_stages(N_STAGES)\n",
        "INPUTS, SETUP, PCA, FLATTEN, PARAMS, GMM = STAGES\n",
        "\n",
        "def define_model(n_factors):\n",
        "    model, stages = xf.Model().init_stages(N_STAGES)\n",
        "    assert stages.len() == STAGES.len()\n",
        "\n",
        "    n = n_factors\n",
        "\n",
        "    model = (\n",
        "        model.add_input(xf.nodes.inputs.dfs.Input_DataFrame_Wide_Rolling(\n",
        "            step=2,\n",
        "            window=4,\n",
        "            unit=\"M\",\n",
        "            allow_missing_columns=False,\n",
        "            allow_missing_indices=True,\n",
        "            allow_new_columns=False,\n",
        "            allow_new_indices=True,\n",
        "            na_threshold_columns=0.,\n",
        "            na_threshold_indices=0.,\n",
        "        ))\n",
        "        .add_input(xf.nodes.inputs.dfs.Input_DataFrame_Wide(\n",
        "            allow_missing_columns=True,\n",
        "            allow_missing_indices=False,\n",
        "            allow_new_columns=True,\n",
        "            allow_new_indices=False,\n",
        "            na_threshold_columns=0.,\n",
        "            na_threshold_indices=0.,\n",
        "        ))\n",
        "        .add_node(SETUP, xf.nodes.inputs.dfs.Slice_DataFrame_Wide_Rolling_Columns(\n",
        "            rolling=xf.Loc.result(INPUTS, 0),\n",
        "            slicing=xf.Loc.result(INPUTS, 1),\n",
        "            scale=xf.utils.scaling.Unit_Sum(axis=1)\n",
        "        ), static = True)\n",
        "        .add_node(SETUP, xf.nodes.cov.vanilla.VCov(\n",
        "            data=xf.Loc.result(INPUTS, 0),\n",
        "        ), static = True)\n",
        "        .add_node(PCA, xf.nodes.pca.rolling.PCA_Rolling(\n",
        "            n=n,\n",
        "            data=xf.Loc.result(INPUTS, 0),\n",
        "        ), static=True)\n",
        "\n",
        "        # TODO: STRUCTURE - need to map up into the same size space\n",
        "        # ie. sector average\n",
        "\n",
        "        .add_node(FLATTEN, xf.nodes.control.shapes.Concatenate(\n",
        "            axis=0,\n",
        "            loc=xf.Loc.result(PCA, 0),\n",
        "        ), static=True)\n",
        "        .add_node(PARAMS, xf.nodes.params.random.Gaussian(\n",
        "            shape=(N_CLUSTERS, N_COLS,),\n",
        "        ))\n",
        "        .add_node(PARAMS, xf.nodes.params.random.Gaussian(\n",
        "            shape=(N_CLUSTERS, N_COLS, N_COLS,),\n",
        "        ))\n",
        "        # .add_node(PARAMS, xf.nodes.params.random.GaussianSoftmax(\n",
        "        #     shape=(data[0].shape[0], N_CLUSTERS,),\n",
        "        # ))\n",
        "        .add_node(PARAMS, xf.nodes.params.random.GaussianSoftmax(\n",
        "            shape=(N_CLUSTERS,),\n",
        "        ))\n",
        "        .add_node(GMM, xf.nodes.clustering.gmm.BGMM_EM(\n",
        "            k=N_CLUSTERS,\n",
        "            data=xf.Loc.result(FLATTEN, 0),\n",
        "            mu=xf.Loc.param(PARAMS, 0),\n",
        "            cov=xf.Loc.param(PARAMS, 1),\n",
        "        ), random = True)\n",
        "        .add_constraint(xf.nodes.constraints.loss.Constraint_Maximise(\n",
        "            data=xf.Loc.result(GMM, 0, 1),\n",
        "        ))\n",
        "        .add_constraint(xf.nodes.constraints.loss.Constraint_Maximise(\n",
        "            data=xf.Loc.result(GMM, 0, 2),\n",
        "        ))\n",
        "        .add_constraint(xf.nodes.constraints.linalg.Constraint_VOrthogonal(\n",
        "            data=xf.Loc.param(PARAMS, 1),\n",
        "        ))\n",
        "        .add_constraint(xf.nodes.constraints.linalg.Constraint_L1_MM_Diag(\n",
        "            raw=xf.Loc.param(PARAMS, 1),\n",
        "        ))\n",
        "\n",
        "    return model"
      ],
      "id": "5b3ed289",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "#| code-summary: \"Run\"\n",
        "def fit_model(d_start, d_end, n_factors):\n",
        "\n",
        "    (lambda _df: _df.assign(\n",
        "        index=xf.utils.dates.date_index(_df.index.values)\n",
        "    ).set_index(\"index\"))(df_returns.loc[\n",
        "        (df_returns.index >= d_start) & (df_returns.index <= d_end)\n",
        "    ]),\n",
        "\n",
        "    model = define_model(n_factors).init(data).optimise(\n",
        "        data,\n",
        "        iters = 2500,\n",
        "        rand_init=10, \n",
        "        max_error_unchanged=0.5,\n",
        "    )\n",
        "    results = model.apply(data)\n",
        "    params = model.params[PARAMS]\n",
        "    \n",
        "    mu = params[0]\n",
        "    cov = params[1]\n",
        "    probs = results[GMM][0][0]\n",
        "    \n",
        "    cov = numpy.round(numpy.matmul(\n",
        "        numpy.transpose(cov, (0, 2, 1)),\n",
        "        cov,\n",
        "    ), 3)\n",
        "\n",
        "    labels = probs.argmax(axis=1)\n",
        "\n",
        "    # group structured average (ie. sector average of pc weights)\n",
        "    # by cluster label\n",
        "\n",
        "    # and then plot against cluster mu\n",
        "\n",
        "    # separately, per cluster, plot the covar\n",
        "\n",
        "    return df, "
      ],
      "id": "524698a8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}