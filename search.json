[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "tomblr",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tomblr",
    "section": "",
    "text": "Rates: PCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nTom Williams\n\n\n\n\n\n\n\n\nEquity: PCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\nTom Williams\n\n\n\n\n\n\n\n\nIntuition: Orthogonality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nTom Williams\n\n\n\n\n\n\n\n\nIntuition: PCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nTom Williams\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/orthogonality/index.html",
    "href": "posts/orthogonality/index.html",
    "title": "Intuition: Orthogonality",
    "section": "",
    "text": "This post is a collection of little visualisations intended to help one build up intuition for the notions of vector and matrix orthogonality.\n\nimport operator\nimport numpy\nimport pandas\nimport jax\nimport jaxopt\n\nimport xtuples as xt\nimport xfactors as xf\n\nc:\\python39\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning:\n\nA NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.2\n\n\n\n\nOrthogonal vectors\nTwo vectors are orthogonal to one another if their dot product is zero, where the dot product can be implemented as:\n\ndef dot(x1, x2):\n    return sum(map(operator.product, x1, x2))\n\nThe dot product of two vectors gives the magnitude of the projection of one onto the other.\nVisually, this represents the length of the shadow that one vector casts on the other (if both are drawn as rays from the origin).\n\ndef unit_vector_projection(v, unit_v):\n    dot = jax.numpy.dot(v, unit_v)\n    return dot * unit_v, unit_v * jax.numpy.sign(v)\n\nTo visualise this, we can plot the dot product of a set of random unit magnitude 2d vectors, with the 2d basis vectors of the same component signs:\n\ndef unit_vector_plot(rows, cols):\n    unit_v = jax.numpy.eye(2)\n    return xf.visuals.graphs.vector_ray_plot(\n        xt.iTuple.range(rows)\n        .map(lambda _: xt.iTuple.range(cols).map(\n            lambda _: (\n                lambda v: jax.numpy.stack([\n                    v,\n                    *unit_vector_projection(v, unit_v[0]),\n                    *unit_vector_projection(v, unit_v[1]),\n                ])\n            )(xf.utils.rand.norm_gaussian((2,)))\n        )),\n        markers=True,\n        color=None,\n    )\n\nWhere the marker part-way along each basis vector (pointing along the x or y axis), is placed at the dot product of said basis vector with the respective random unit vector:\n\nunit_vector_plot(3, 3)\n\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n\n                                                \n\n\nAs one can see, given their magnitudes, the larger the dot product of two vectors, the closer they are to pointing in the same direction.\nConversely, the smaller their dot product, the closer they are to perpendicular.\nWe can make this more explicit by noting that the dot product is equal to the product of the magnitude of the two vectors and the cosine of the angle between them.\nAs such, a zero dot product indicates that the angle between the two vectors is pi / 2 radians (or 90 degrees) - ie. that they’re perpendicular to one another.\nSimilarly, as cos(0) = 1, the dot product of a vector with itself is simply equal to it’s squared magnitude.\n\n\nUnit sphere\nThe set of all 2D vectors of unit magnitude forms a circle:\n\nxf.visuals.graphs.vector_ray_plot(\n    xf.utils.rand.norm_gaussian((25, 2,)),\n    color=None,\n)\n\n\n                                                \n\n\nThe set of all 3D unit vectors forms a sphere:\n\nxf.visuals.graphs.vector_ray_plot(\n    xf.utils.rand.norm_gaussian((25, 3,)),\n    _3d=True,\n    color=None,\n)\n\n\n                                                \n\n\nAnd so on into higher dimensions.\n\n\nOrthogonal matrices\nIf two vectors are both unit norm and orthogonal to one another, then we say that they are orthonormal.\nIf each of the vectors in a square matrix is orthonormal w.r.t. each of the others, we say that the matrix is orthogonal.\nAs such, we can see that:\n\neach orthogonal matrix is a rotation of the equivalent dimensionality basis vector matrix.\nthe columns / rows of each orthogonal matrix sit on the relevant dimensionality unit (hyper) sphere.\n\nFor instance, in two dimensions, each of the colored pairs of vectors in the below is just a rotation of the blue unit basis vectors (along the x, y axes), around our unit circle:\n\nxf.visuals.graphs.vector_ray_plot(\n    numpy.concatenate([\n        numpy.expand_dims(numpy.eye(2), 0),\n        xf.utils.rand.orthogonal(2, shape = (5,))\n    ]),\n    color=\"g\",\n)\n\n\n                                                \n\n\nAnd, in three dimensions, each of the colored triplets of vectors in the below is just a rotation of the blue unit basis vectors (along the x, y, z axes), around our unit sphere:\n\nxf.visuals.graphs.vector_ray_plot(\n    numpy.concatenate([\n        numpy.expand_dims(numpy.eye(3), 0),\n        xf.utils.rand.orthogonal(3, shape = (5,))\n    ]),\n    _3d=True,\n    color=\"g\",\n)\n\n\n                                                \n\n\nWe can thus interpret an orthogonal matrix as a rotation, mapping us from a space in which our axes point in the directions of our basis vectors, to a space in which our axes point in the directions of our matrix.\n\n\nMatrix multiplication\nA matrix multiplication is just lots of separate dot products.\nIn terms of the above, it is the size of the shadow cast by each row of the left matrix, on each column of the right matrix.\nI like to think of it as shooting particles in the directions and magnitudes of the rows of the left matrix, onto ‘deflectors’ with the orientation and magnitudes of the columns of the right matrix.\nFor instance, given a couple of random unit norm matrices:\n\nN = 2\nM1 = xf.utils.rand.norm_gaussian((N, N,))\nM2 = xf.utils.rand.norm_gaussian((N, N,))\ndef render_matrix(m):\n    return xf.visuals.rendering.render_df_color_range(\n        pandas.DataFrame(m),\n        v_min=-1.,\n        v_max=.1,\n    )\ndisplay(render_matrix(M1))\ndisplay(render_matrix(M2))\n\nAttributeError: 'ColormapRegistry' object has no attribute 'get_cmap'\n\n\n<pandas.io.formats.style.Styler at 0x21dd3d33df0>\n\n\nAttributeError: 'ColormapRegistry' object has no attribute 'get_cmap'\n\n\n<pandas.io.formats.style.Styler at 0x21dd61708e0>\n\n\nWe can plot out each of the separate dot product projections, akin to how we did above, as so:\n\ndef vector_projection(v1, v2):\n    dot = jax.numpy.dot(v1, v2)\n    return abs(dot) * v2, v2\n\nxf.visuals.graphs.vector_ray_plot(\n    [\n        [\n            numpy.stack([\n                M1[r],\n                *vector_projection(M1[r], M2[:, c]),\n            ])\n            for c in range(N)\n        ]\n        for r in range(N)\n    ],\n    share_x=True,\n    share_y=True,\n    markers=True,\n    color=None,\n)\n\n\n                                                \n\n\nBefore comparing the magnitude of the projections above (how far they are from the origin), to the cells of the below:\n\nrender_matrix(numpy.matmul(M1, M2))\n\nAttributeError: 'ColormapRegistry' object has no attribute 'get_cmap'\n\n\n<pandas.io.formats.style.Styler at 0x21dd61e8400>\n\n\nConfirming that each cell of the matrix multiplication is just the magnitude of the projection of the relevant row / column of our input matrices.\n\n\nOrthogonality constraint\nGiven the definition of an orthogonal matrix, we can see that multiplication by its transpose will return the relevant size identity matrix:\n\neach of the pairs outside of the main diagonal are orthogonal, and so will return a dot product zero.\neach pair in the main diagonal will just be the dot product of a unit norm vector with itself: ie. 1.\n\n\nM_orth = xf.utils.rand.orthogonal(2)\nrender_matrix(numpy.matmul(M_orth, M_orth.T))\n\nAttributeError: 'ColormapRegistry' object has no attribute 'get_cmap'\n\n\n<pandas.io.formats.style.Styler at 0x21dd62d21c0>\n\n\nAs such, we can construct a rough measure for how orthogonal a given matrix is: the mean squared error between the matmul of a matrix with its transpose, and the relevant size identity matrix.\n\ndef orthogonality_loss(X):\n    XXt = jax.numpy.matmul(X, X.T)\n    I = jax.numpy.eye(XXt.shape[0])\n    return jax.numpy.square(XXt - I).mean()\n\nWe can then ‘orthogonalise’ a matrix using gradient descent, by minisiming this loss function:\n\ndef orthogonalise(X):\n    solver = jaxopt.GradientDescent(fun=orthogonality_loss)\n    res = solver.run(X)\n    params, state = res\n    return params\n\nWhich we can use on each of our random matrices above, as so:\n\nM1_orth = orthogonalise(M1)\nM2_orth = orthogonalise(M2)\nxf.visuals.graphs.vector_ray_plot(\n    [\n        numpy.stack([M_orig, M_orth])\n        for M_orig, M_orth in zip([M1, M2], [M1_orth, M2_orth])\n    ],\n    markers=True,\n    color=\"g\",\n)\n\n\n                                                \n\n\nAs you can see, the non-orthogonal blue matrix has been morphed into the orthogonal red matrix.\nWe can verify their orthogonality by taking their matmul with their transpose:\n\ndisplay(render_matrix(numpy.matmul(M1_orth, M1_orth.T)))\ndisplay(render_matrix(numpy.matmul(M2_orth, M2_orth.T)))\n\nAttributeError: 'ColormapRegistry' object has no attribute 'get_cmap'\n\n\n<pandas.io.formats.style.Styler at 0x21dd0b923a0>\n\n\nAttributeError: 'ColormapRegistry' object has no attribute 'get_cmap'\n\n\n<pandas.io.formats.style.Styler at 0x21dd8773730>\n\n\nAnd confirming that we’re left with the relevant size identity matrix.\n\n\nRotation\nTurning back to the interpretation of an orthogonal matrix as a rotation, let’s now plot the transformation of a bunch of random points by each of our two orthogonal matrices.\n\ncloud = xf.utils.rand.gaussian((10, 2,))\ndef plot_cloud(M_, T = False):\n    return xf.visuals.graphs.vector_ray_plot(\n        [\n            [\n                cloud, M_, numpy.matmul(cloud, M_),\n            ] + ([] if not T else [\n                M_.T,\n                numpy.matmul(numpy.matmul(cloud, M_), M_.T)\n            ])\n        ],\n        markers=True,\n        range_x=[-3., 3.],\n        range_y=[-3., 3.],\n    )\n\nWhere the left graph is our point cloud, the middle is our orthogonal rotation matrix, and the right is the point cloud after rotation by said matrix:\n\nplot_cloud(M1_orth)\n\n\n                                                \n\n\n\nplot_cloud(M2_orth)\n\n\n                                                \n\n\nAs one can see:\n\nthe magnitude of each member of our cloud has been preserved (they’re the same length relative to the origin)\nthe angles between each member of our cloud have been preserved (pick a pair of colors, and compare their angles in left vs right).\n\nConfirming that (our) orthogonal matrices represent a scale and angle preserving rotation.\n\n\nTranspose and Inverse\nAs we saw above, multiplying an orthogonal matrix by its transpose leaves us with the identity matrix.\nAs such, an orthogonal matrix’s tranpose must be equal to its inverse: as M M.T = I = M M-1.\nWe can confirm this by rotating our point cloud back, by multiplication with M.T:\n\nplot_cloud(M1_orth, T = True)\n\n\n                                                \n\n\nWe can find some intuition for this in terms of our interpretation of an orthogonal matrix, above, as a scale-preserving rotation.\nRoughly speaking:\n\nthe inverse of a matrix ‘reverses’ the linear transformation induced by multiplication by said matrix.\nthe transpose of a matrix reflects a rotation on the main diagonal, forming the ‘opposite’ reflection.\n\nThis ‘opposite’ reflection just reverses our original rotation - taking us from our point cloud on the left, back to itself on the right - and is hence our matrix inverse.\n\n\nRelated posts\nOrthogonality is key to understanding PCA, and is how we derive the closed form expression for multi-variate linear regression."
  },
  {
    "objectID": "posts/pca/index.html",
    "href": "posts/pca/index.html",
    "title": "Intuition: PCA",
    "section": "",
    "text": "This is a reference post on PCA in which we walk through a simple toy example, to help build intuition for the geometry involved.\n\nimport operator\nimport numpy\nimport pandas\nimport jax\nimport jaxopt\n\nimport xtuples as xt\nimport xfactors as xf\n\nc:\\python39\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning:\n\nA NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.2\n\n\n\n\nGenerative model\nWe’ll start by defining our example data, assuming a 3D latent factor space, a 4D feature space, and 100 example data points:\n\nN_FACTORS = 3\nFACTORS = list(range(N_FACTORS))\n\nN_SAMPLES = 100\nSAMPLES = list(range(N_SAMPLES))\n\nN_FEATURES = 4\nFEATURES = list(range(N_FEATURES))\n\n\nFactors\nFirst, we’ll generate some random, diagonal covariance, gaussian noise, that we’ll refer to as our ‘factors’:\n\nfactors = xf.utils.rand.gaussian((N_SAMPLES, N_FACTORS))\nfactors_df = pandas.DataFrame(\n    factors, index=SAMPLES, columns=FACTORS,\n)\n\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nAs we can see, the sample covariance is (roughly), unit diagonal:\n\nxf.visuals.rendering.render_df_color_range(factors_df.cov())\n\nAttributeError: 'ColormapRegistry' object has no attribute 'get_cmap'\n\n\n<pandas.io.formats.style.Styler at 0x2dc6dd8e310>\n\n\nThe covariance between two variables increases (decreases) the more one variable is above (below) it’s mean, conditional on the other variable also being above it’s mean.\nAs such, if we plot our individual factors against one another in 2D we can see their zero-covariance, in the spherical shape of the resulting densities:\n\nxf.visuals.graphs.df_density_pair_chart(factors_df, key=\"factor\")\n\n\n                                                \n\n\nSimply follow either the x or y axis, and note that the conditional average of the other axis (the other variable) remains constant: implying (roughly) zero covariance.\nIf we plot the 1D density, we can see that each factor also has (roughly) the same unit variance gaussian distribution:\n\nxf.visuals.graphs.df_line_chart(\n    xf.visuals.densities.gaussian_kde_1d_df(\n        {\n            i: factors[:, i]\n            for i in range(N_FACTORS)\n        },\n    ),\n    x=\"position\",\n    y=\"density\",\n    color=\"key\",\n)\n\n\n                                                \n\n\n\n\nFeatures\nGiven this, we’ll now generate another random gaussian weight matrix:\n\nweights = xf.utils.rand.gaussian((N_FACTORS, N_FEATURES))\n\nWhich we can render as so:\n\nweights_df = pandas.DataFrame(\n    weights, columns=FEATURES, index=FACTORS\n)\nxf.visuals.rendering.render_df_color_range(weights_df)\n\nAttributeError: 'ColormapRegistry' object has no attribute 'get_cmap'\n\n\n<pandas.io.formats.style.Styler at 0x2dc470328e0>\n\n\nAnd which we’ll then multiply with our factors, to get some features:\n\nfeatures = factors @ weights\nfeatures_df = pandas.DataFrame(\n    features, columns=FEATURES, index=SAMPLES\n)\n\nAs one can see in the resulting 2D density plot, we’ve now added some linear dependence to our dataset:\n\nfor chart in xf.visuals.graphs.df_density_pair_chart(\n    features_df, key=\"feature\", separate=True\n):\n    display(chart)\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nWhich we can confirm by calculating our feature (sample) covariance matrix:\n\nxf.visuals.rendering.render_df_color_range(features_df.cov())\n\nAttributeError: 'ColormapRegistry' object has no attribute 'get_cmap'\n\n\n<pandas.io.formats.style.Styler at 0x2dc74de5940>\n\n\n\n\nProblem statement\nGiven our feature matrix, and assuming the distribution of our factors (but not the values), our goal is to impute sensible values for both the weights and the factors.\n\n\n\nRotation and Scaling\nVisually, we can separate out the distributional changes affected by our weight matrix into two steps:\n\na rescaling\na rotation\n\nSo, to reverse our weight matrix (taking us from features back to factors), we presumably need to:\n\nun-rotate our features, so that - in terms of the 2D densities above - they ‘sit’ back on the original x, y axes.\nde-scale them back to unit variance.\n\nTo visualise this, we can cheat slightly and, assuming the methodology we’re in the process of deriving, de-rotate those 2D densities from above:\n\n\nDe-Rotate\ndef threshold(df, k, quantile):\n    return df[df[k] > numpy.quantile(df[k], quantile)]\n\ndef de_rotate(df):\n    df = xf.visuals.graphs.f_df_density_pair_df(\n        FEATURES, \"pair\",\n    )(df)\n    z = \"density\"\n    dfs = {\n        pair: df[df[\"pair\"] == pair]\n        for pair in df[\"pair\"].unique()\n    }\n    dfs_threshold = {\n        pair: threshold(_df, z, .7)\n        for pair, _df in dfs.items()\n    }\n    ws = {\n        pair: jax.numpy.linalg.eig(\n            jax.numpy.cov(numpy.stack([\n                _df[\"x\"].values, _df[\"y\"].values\n            ]))\n        )[1].real\n        for pair, _df in dfs_threshold.items()\n    }\n    return [\n        pandas.DataFrame(\n            numpy.matmul(\n                _df[[\"x\", \"y\"]].values, ws[pair]\n            ),\n            columns=[\"x\", \"y\"],\n            index=_df.index,\n        ).assign(density=numpy.exp(_df[z]), pair=_df[\"pair\"])\n        for pair, _df in dfs.items()\n    ]\n\n\nWhich we’ll plot with separate color scales, so you can see more clearly how the density is now aligned with the conventional x, y axes:\n\nfor derotated_df in de_rotate(features_df):\n    display(xf.visuals.graphs.df_scatter_chart(\n        derotated_df,\n        x=\"x\",\n        y=\"y\",\n        color=\"density\",\n        height=400,\n        width=600,\n    ))\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nAs one can see, where the distributions are non-spherical, they now ‘point’ out along the conventional x, y axes.\nWhere they were roughly spherical to begin with, the rotation hasn’t actually changed the (bi-variate) distribution, as a sphere is invariant under rotation.\nThen, to get back to our factors, we simply have to re-scale the de-rotated distributions above, back to unit variance.\n\nSolution\nLinear transformations are composable, which means that we can separate out these two steps into separate matrices, before multiplying them back together (to get a single embedding matrix).\nSo, to go from features to factors, we’re looking for two matrices:\n\none to de-rotate our features back into factors.\none to de-scale our factors back to unit variance.\n\nA scaling matrix just stretches each dimension individually, so we can presume that to be (non-unit) diagonal.\nAssuming we use an orthogonal rotation matrix, we can use the same matrix to rotate back and forth between our features and our factors.\nAs such, we’re looking for:\n\nan orthogonal rotation matrix\na diagonal scaling matrix.\n\n\n\n\nCovariance\nCovariance under a linear transformation is defined as so:\n\ndef linear_cov(W, X):\n    return W @ numpy.cov(X) @ W.T\n\nAs we saw above, we’re going to assume that we can model our weight matrix as the product of an orthogonal rotation matrix, and a diagonal scaling matrix.\nAs such, we can write our covariance function (for our features), as so:\n\ndef feature_cov(Orthog, Diag, X):\n    return (Orthog @ Diag) @ numpy.cov(X) @ (Orthog @ Diag).T\n\nWhich we can simplify to:\n\ndef feature_cov(Orthog, Diag, X):\n    return Orthog @ numpy.cov(X) @ (Diag ** 2) @ Orthog.T\n\nThis, intuitively, makes sense:\n\nFirst we multiply our factor covariance by the square of our scaling matrix, as (co) variance is a quadratic dispersion measure.\nThen, we multiply through by our rotation matrix, twice: one to capture any change in scale from the projection itself, and then once to actually apply the rotation in question.\n\n\n\nEigen vectors\nGiven our assumption that the factors have diagonal covariance, we can make one further simplication to the above:\n\ndef feature_cov(Orthog, Diag, X):\n    return Orthog @ numpy.diag(numpy.cov(X)) @ (Diag ** 2) @ Orthog.T\n\nIf we right multiply through both sides by our rotation matrix:\n\ndef rotated_feature_cov(Orthog, Diag, X, features):\n    return numpy.cov(features) @ Orthog == (\n        Orthog @ numpy.diag(numpy.cov(X)) @ (Diag ** 2)\n        @ Orthog.T @ Orthog\n    )\n\nThe Orthog.T @ Orthog term (on the far right) cancels to I.\n\ndef rotated_feature_cov(Orthog, Diag, X, features):\n    return numpy.cov(features) @ Orthog == (\n        Orthog @ numpy.diag(numpy.cov(X)) @ (Diag ** 2)\n    )\n\nAs such, we can see that multiplication of our feature covariance by each of the vectors in our rotation matrix simply returns a scaled version of said vector, with the scaling term given by the relevant diagonal element in diag(cov(X)) @ (Diag ** 2).\nThis, in turn, then tells us that our rotation matrix is in fact comprised of eigenvectors of our feature covariance matrix.\nGiven that our factor distributions were assumed to have unit variance, we can then also see that the diagonal scaling matrix we’re looking for is simply comprised of the square root of the respective eigenvalues.\n\n\nSolution\nAs such, to find our matrices Orthog and Diag we simply need to do an eigen-decomposition of our feature covariance matrix, which we can do as so:\n\neigvals, eigvecs = jax.numpy.linalg.eig(\n    jax.numpy.cov(features.T)\n)\nscale = jax.numpy.sqrt(eigvals.real)\northog = eigvecs.real\n\nAs one can see, we’re only left with 3 (roughly) non-zero eigenvalues:\n\nnumpy.round(eigvals.real, 2)\n\nArray([13.139999  ,  4.94      ,  0.        ,  0.22999999], dtype=float32)\n\n\nThese capture the degree of variation in the feature space, explained by each component (how ‘spread’ out the features are along the axis given by the relevant eigenvector).\nAs such, we can then first sort our matrices by their eigenvalue, before disregarding the smallest.\nHere, we’re assuming we know how many latent factors there were (so we keep only the first N_FACTORS components), but in a more general problem, we could infer this number by the count of non (nearly) zero eigenvalues.\n\norder = jax.numpy.flip(jax.numpy.argsort(eigvals))\nscale = scale[order][:N_FACTORS]\northog = orthog[:, order[:N_FACTORS]]\n\nWe can de-rotate our features into factor space, by multiplication with our orthogonal rotation matrix, as so:\n\nfactors_scaled = features @ orthog\nxf.visuals.graphs.df_density_pair_chart(\n    pandas.DataFrame(\n        factors_scaled,\n        columns=FACTORS,\n        index=SAMPLES,\n    ), \n    key=\"factor\"\n)\n\n\n                                                \n\n\nBefore de-scaling them, leaving us with our original unit variance noise, by dividing by the sqrt of the eigenvalues:\n\nfactors_descaled = numpy.divide(\n    factors_scaled,\n    xf.expand_dims(scale, 0, 1)\n)\nxf.visuals.graphs.df_density_pair_chart(\n    pandas.DataFrame(\n        factors_descaled,\n        columns=FACTORS,\n        index=SAMPLES,\n    ), \n    key=\"factor\"\n)\n\n\n                                                \n\n\nTo go back up to our features, we can simply re-scale up the factors, and then rotate back into feature space:\n\nfeatures_roundtrip = orthog @ numpy.multiply(\n    factors_descaled,\n    xf.expand_dims(scale, 0, 1)\n).T\nfor chart in xf.visuals.graphs.df_density_pair_chart(\n    pandas.DataFrame(\n        features_roundtrip.T,\n        columns=FEATURES,\n        index=SAMPLES,\n    ), \n    key=\"feature\",\n    separate=True,\n):\n    display(chart)\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nWhich, as you can see, returns us back to where we started.\n\n\nDicussion\nOur rotation matrix, above, allows us to map our feature space into the (presumably) much smaller (implied) factor space, whilst nonetheless retaining most of the ‘information’ of the original features.\nThis helps us combat the ‘curse of dimensionality’ (the tendency for distance measures to explode as the number of dimensions increases), as we can train downstream models using the smaller dimensional (implied) factors, rather than the (potentially much) larger dimensional features.\nThe embedding matrix itself also encodes useful information about the correlation structure of our features, which can often be interpreted directly (for instance, see the below two posts analysing single name equity returns and bond / swap yield curves, respectively).\nPCA also naturally lends itself to a number of different extensions - particularly once we move from a strict eigen-decomposition, as above, onto a more general gradient based optimisation approach, such as in this post on ppca.\n\n\nRelated posts\nBelow are two practical examples of using PCA:\n\nTo extract equity market factors, from daily single-name equity returns.\nTo extract yield curve factors, daily per-tenor bond and swap curve yields.\n\nAlso, see here for a primer on orthogonality (useful for understanding the decomposition above)."
  },
  {
    "objectID": "posts/pca_equity/index.html",
    "href": "posts/pca_equity/index.html",
    "title": "Equity: PCA",
    "section": "",
    "text": "In this post, I’ll demonstrate how to use PCA to extract market factors from daily equity return data."
  },
  {
    "objectID": "posts/pca_equity/index.html#setup",
    "href": "posts/pca_equity/index.html#setup",
    "title": "Equity: PCA",
    "section": "Setup",
    "text": "Setup\n\nimport numpy\nimport pandas\nimport jax\nimport jax.numpy\n\nimport xtuples as xt\nimport xfactors as xf\n\nc:\\python39\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning:\n\nA NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.2\n\n\n\n\nData\nWe’ll start by loading some daily close to close total returns data, given a rolling universe of major european equity indices:\n\ndf_returns = xf.bt.data.returns.returns_df(\n    indices=xf.bt.data.indices.EU,\n)\n\nThe returns are from bloomberg, and include all cash and non-cash adjustments (in production we have a separate internal ticker for the dividend stream from a given name, but that’s a little over-complicated for our purposes here).\nWe’ll also load the relevant index membership mapping tables for our universe:\n\ndfs_indices = xf.bt.data.universes.universe_mapping(\n    indices=xf.bt.data.indices.EU,\n)\ndf_universe = xf.bt.data.universes.universe_union(dfs_indices)\n\nAnd some GICS sector mapping tables, as these will be useful later:\n\ndfs_sectors = xf.bt.data.universes.universe_mapping(\n    sectors=xf.bt.data.gics.SECTORS,\n)\n\nVanilla PCA doesn’t accomodate missing data, so given a particular target date range, we’ll filter to only those tickers within our universe for the entire period, as so:\n\ndef in_universe(ticker, df, threshold = 1.):\n    if ticker not in df.columns:\n        return False\n    return (\n        df[ticker].sum() / len(df.index)\n    ) >= threshold\n\nWhich we’ll then wrap into a convenience function.\n\n\nget_returns\ndef get_returns(d_start, d_end, threshold=1.):\n    tickers = xt.iTuple(df_returns.columns).filter(\n        in_universe,\n        df=xf.utils.dfs.index_date_filter(df_universe, d_start, d_end),\n        threshold=threshold,\n    ).pipe(list) \n    return xf.utils.dfs.index_date_filter(\n        df_returns, d_start, d_end\n    )[tickers].dropna()\n\n\n\n\nModel\nPCA uses eigendecomposition to extract an orthogonal embedding matrix, together with a diagonal scaling matrix, from the covariance matrix of a given data set.\n\nimport functools\n@functools.lru_cache(maxsize=10)\ndef fit_pca(d_start, d_end, n):\n    df = get_returns(d_start, d_end)\n    eigvals, eigvecs = jax.numpy.linalg.eig(\n        jax.numpy.cov(df.values.T)\n    )\n    order = jax.numpy.flip(jax.numpy.argsort(eigvals))\n    eigvals = eigvals.real[order[:n]]\n    eigvecs = eigvecs.real[:, order[:n]]\n    return eigvals, eigvecs, df\n_ = fit_pca(xf.utils.dates.y(2022), xf.utils.dates.y(2023), n=3)\n\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nThe embedding matrix can then be used to project the data into a lower dimensional factor space, where the relative ‘importance’ of each factor is given by the relevant term in the diagonal scaling matrix (importance being defined as how much of the original data variance each factor ‘explains’).\n\nimport functools\ndef apply_pca(d_start, d_end, n):\n    eigvals, eigvecs, df = fit_pca(d_start, d_end, n)\n    return eigvals, eigvecs, df.values @ eigvecs, df\n\nSee the reference post PCA for a more detailed walk-through of the linear algebra.\n\n\nResults\nAs expected, the factors we extract are linearly independent (have diagonal covariance / correlation):\n\ndef pca_factor_corr(d_start, d_end, n):\n    _, _, factors, _ = apply_pca(d_start, d_end, n)\n    return xf.visuals.rendering.render_df_color_range(\n        pandas.DataFrame(factors).corr(),\n        v_min=-1.,\n        v_max=.1,\n    )\npca_factor_corr(xf.utils.dates.y(2022), xf.utils.dates.y(2023), n=3)\n\nAttributeError: 'ColormapRegistry' object has no attribute 'get_cmap'\n\n\n<pandas.io.formats.style.Styler at 0x29940f6c760>\n\n\nAnd are each univariate gaussian, with descending variance from PC0 down to PC2 (seen in the increasing concentration of the gaussians in the plot below):\n\ndef pca_factor_density_chart(d_start, d_end, n):\n    _, _, factors, df = apply_pca(d_start, d_end, n)\n    return xf.visuals.graphs.df_density_chart(\n        xf.utils.dfs.melt_with_index(\n            pandas.DataFrame(\n                factors,\n                columns=list(range(factors.shape[1])),\n                index=df.index,\n            ),\n            variable_as=\"factor\",\n        ),\n        \"factor\",\n        \"value\",\n    )\npca_factor_density_chart(xf.utils.dates.y(2022), xf.utils.dates.y(2023), n=3)\n\n\n                                                \n\n\nTurning to our factor weights:\n\n\nFactors\ndef pca_weights(d_start, d_end, n):\n    _, eigvecs, _, df = apply_pca(d_start, d_end, n)\n    return pandas.DataFrame(\n        eigvecs.T,\n        columns=df.columns,\n        index=list(range(eigvecs.shape[1]))\n    )\npca_weights(xf.utils.dates.y(2022), xf.utils.dates.y(2023), n=3)\n\n\n\n\n\n\n  \n    \n      \n      ACA FP Equity\n      AI FP Equity\n      AIR FP Equity\n      ALO FP Equity\n      APAM NA Equity\n      BN FP Equity\n      BNP FP Equity\n      CA FP Equity\n      CAP FP Equity\n      CS FP Equity\n      ...\n      IBE SQ Equity\n      ITX SQ Equity\n      NDA FH Equity\n      SAN SQ Equity\n      BG AV Equity\n      BIRG ID Equity\n      BKT SQ Equity\n      CABK SQ Equity\n      EBS AV Equity\n      SAB SQ Equity\n    \n  \n  \n    \n      0\n      -0.060335\n      -0.040464\n      -0.066809\n      -0.076080\n      -0.074317\n      -0.028223\n      -0.065206\n      -0.018692\n      -0.062472\n      -0.048011\n      ...\n      -0.027217\n      -0.046161\n      -0.050864\n      -0.062572\n      -0.054376\n      -0.054569\n      -0.043741\n      -0.042946\n      -0.076183\n      -0.058908\n    \n    \n      1\n      -0.070421\n      0.015270\n      -0.042955\n      -0.033241\n      -0.033736\n      -0.024529\n      -0.084168\n      -0.036765\n      0.041158\n      -0.054157\n      ...\n      0.017942\n      -0.017879\n      -0.040327\n      -0.087104\n      -0.058716\n      -0.095620\n      -0.106794\n      -0.115320\n      -0.105358\n      -0.143965\n    \n    \n      2\n      0.029703\n      0.000996\n      0.004294\n      0.035606\n      -0.085797\n      0.046632\n      0.034311\n      -0.026233\n      0.018673\n      0.005721\n      ...\n      -0.024447\n      0.025555\n      -0.012960\n      0.011640\n      0.021233\n      0.043145\n      0.032449\n      0.003381\n      0.053300\n      0.037243\n    \n  \n\n3 rows × 370 columns\n\n\n\nOne way to visualise them is with a simple bar chart:\n\ndef pca_factor_weight_chart(d_start, d_end, n):\n    weights = pca_weights(d_start, d_end, n)\n    n = len(weights.index)\n    return xf.visuals.graphs.df_facet_bar_chart(\n        xf.utils.dfs.melt_with_index(weights, index_as=\"factor\"),\n        x=\"variable\",\n        y=\"value\",\n        facet=\"factor\",\n    )\npca_factor_weight_chart(xf.utils.dates.y(2022), xf.utils.dates.y(2023), n=3)\n\n\n                                                \n\n\nHowever, as one can see, given that we end up with a single weight per ticker per factor, it’s fairly difficult to know how to interpret each of the above.\nThe one thing we can tell from the above, is that our first factor is (more or less) the same sign for equity ticker in our universe, representing some kind of general market beta factor (a point we’ll return to in the discussion below).\n\n\nSector & Index Aggregation\nTo try and make sense of our weights, we can try averaging them up into something more manageable.\nFor instance, using a function mapping us from a ticker to a GICS sector:\n\n\nSector Tickers\ndef sector_tickers(d_start, d_end, n):\n    _, _, df = fit_pca(d_start, d_end, n)\n    tickers = xt.iTuple(df.columns)\n    return {\n        s: tickers.filter(\n            in_universe, df = dfs_sectors[s], threshold=1.\n        ) for s in sorted(dfs_sectors.keys())\n    }\n\n\nCertain industries will be under or over-represented in our universe, in a given period:\n\ndef sector_map_chart(d_start, d_end, n):\n    sector_map = sector_tickers(d_start, d_end, n)\n    return xf.visuals.graphs.df_bar_chart(\n        pandas.DataFrame({\n            s: [len(v)] for s, v in sector_map.items()\n        }).melt(),\n        x=\"variable\",\n        y=\"value\",\n    )\nsector_map_chart(xf.utils.dates.y(2022), xf.utils.dates.y(2023), n=3)\n\n\n                                                \n\n\nSo, we’ll scale our average relative to the sector representation during the period in question, returning something like the below:\n\n\nSector Weights Chart\ndef pca_sector_weights_chart(d_start, d_end, n):\n    weights = pca_weights(d_start, d_end, n)\n    sector_map = sector_tickers(d_start, d_end, n)\n    sector_weights = pandas.DataFrame({\n        s: weights[tickers.pipe(list)].sum(axis=1)\n        for s, tickers in sector_map.items()\n    })\n    scaled_weights = pandas.DataFrame({\n        s: sector_weights[s] / len(ts)\n        for s, ts in sector_map.items()\n    })\n    return xf.visuals.graphs.df_facet_bar_chart(\n        xf.utils.dfs.melt_with_index(scaled_weights, index_as=\"factor\"),\n        x=\"variable\",\n        y=\"value\",\n        facet=\"factor\",\n    )\npca_sector_weights_chart(xf.utils.dates.y(2022), xf.utils.dates.y(2023), n=3)\n\n\n\n                                                \n\n\nAs one can see, this is much more easily interpretible.\nWe can go through a similar process for averaging our weights by equity index.\n\n\nIndex Tickers\ndef index_tickers(d_start, d_end, n):\n    _, _, df = fit_pca(d_start, d_end, n)\n    tickers = xt.iTuple(df.columns)\n    return {\n        i: tickers.filter(\n            in_universe, df = dfs_indices[i], threshold=1.\n        ) for i in sorted(dfs_indices.keys())\n    }\n\n\nWhere, again, certain indices will be over or under-represented in our universe:\n\ndef index_map_chart(d_start, d_end, n):\n    index_map = index_tickers(d_start, d_end, n)\n    return xf.visuals.graphs.df_bar_chart(\n        pandas.DataFrame({\n            i: [len(v)] for i, v in index_map.items()\n        }).melt(),\n        x=\"variable\",\n        y=\"value\",\n    )\nindex_map_chart(xf.utils.dates.y(2022), xf.utils.dates.y(2023), n=3)\n\n\n                                                \n\n\nWhich we’ll account for in our averaging just as we did for our sectors, giving us back something like the below:\n\n\nIndex Weights Chart\ndef pca_index_weights_chart(d_start, d_end, n):\n    weights = pca_weights(d_start, d_end, n)\n    index_map = index_tickers(d_start, d_end, n)\n    index_weights = pandas.DataFrame({\n        i: weights[tickers.pipe(list)].sum(axis=1)\n        for i, tickers in index_map.items()\n    })\n    scaled_weights = pandas.DataFrame({\n        i: index_weights[i] / len(ts)\n        for i, ts in index_map.items()\n    })\n    return xf.visuals.graphs.df_facet_bar_chart(\n        xf.utils.dfs.melt_with_index(index_weights, index_as=\"factor\"),\n        x=\"variable\",\n        y=\"value\",\n        facet=\"factor\",\n    )\npca_index_weights_chart(xf.utils.dates.y(2022), xf.utils.dates.y(2023), n=3)\n\n\n\n                                                \n\n\nWith which, we can get back to the fun part - interpretation!\n\n\nInterpretation - By Sector\nLet’s try plotting our factor sector weights for the period 2020 - 2022:\n\npca_sector_weights_chart(xf.utils.dates.y(2020), xf.utils.dates.y(2022), n=3)\n\n\n                                                \n\n\nAs one can see, the first factor has the same sign for every sector, and if we dig into particular components, seems to roughly match our intuitive notion of index beta (ie. higher beta sectors having larger weights):\n\nHigher beta: 10 (Energy), 20 (Industrials), 25 (Discretionary), 40 (Financials).\nLower beta: 30 (Staples), 35 (Health), 50 (Media / Telcos), 55 (Utilities).\n\nWhilst the exact loadings will vary from period to period, the first - and most important (in a variance-explanatory sense) - component will nearly always tend to be some kind of general market-beta factor.\nThe second and third factors will, however, vary a little more from period to period.\nFor instance, whilst for 2020 - 2022, the second factor seems to be picking up something like a risk-on vs risk-off factor, separating between:\n\nEnergy (10), Financials (40), Real Estate (60).\nMaterials (15), Health(35), Tech (45), Utilities (55).\n\nIf we run the same procedure over 2007 to 2010:\n\npca_sector_weights_chart(xf.utils.dates.y(2007), xf.utils.dates.y(2010), n=3)\n\n\n                                                \n\n\nOur second factor is almost entirely dominated by Financials and Real Estate (40 and 60) versus everything else, and our third by Energy and Materials (10 and 15) versus everything else.\nThis, given the 2007-2010 (housing driven) financial crisis (and the ensuing commodity market reaction to global growth concerns), does actually make sense.\nLet’s now compare to the period 2014 to 2016:\n\npca_sector_weights_chart(xf.utils.dates.y(2014), xf.utils.dates.y(2016), n=3)\n\n\n                                                \n\n\nHere, the Energy and Materials (10 and 15) vs everything else is much more prominent, given the global commodity wobble (at least partly driven by the rise in US shale oil production).\nWe also see an extremely concentrated third component of financials (40) versus everything else, probably driven by another flare up of the 2010’s EU sovereign debt crises.\nWhat we’re seeing here is that because PCA doesn’t have any underlying model for how different equities relate together - it just strips out the ways in which they happened to co-move during a given period - it doesn’t have anything to anchor itself on, and thus returns (samples of) somewhat different factors, depending on which period we run it on.\nWhilst this might reflect a genuine causal difference in the factors driving overall single-name equity returns during different periods, it also might reflect sheer co-occurence (correlation, as they say, does not necessarily mean causation).\n\n\nInterpretation - By Index\nWe get similar results (though personally much less useful), if we instead aggregate by index.\nFor instance, we can see the same commodity wobble in the Norwegian (OBX) and to some extent UK (UKX) weighting over 2014 to 2016 (given their respective commodity exposures):\n\npca_index_weights_chart(xf.utils.dates.y(2014), xf.utils.dates.y(2016), n=3)\n\n\n                                                \n\n\nWhereas we can see something like a peripherals (Spain & Portugal) vs European core factor, if we run over the peak of the EU sovereign debt crisis (2010-2013):\n\npca_index_weights_chart(xf.utils.dates.y(2010), xf.utils.dates.y(2013), n=3)\n\n\n                                                \n\n\nWhere we can see how the EU financials index (SX7E) is moving with the IBEX and PSI (lending support to the idea that this is some kind of financial stress factor).\n\n\nLimitations\nAs noted above, vanilla PCA - at least in the eigendecomposition form above - requires a full data matrix, with no missing values. This can be quite limiting, especially when dealing with real data (as opposed to the above, somewhat stylised, daily equity returns).\nThat being said, this is fairly simple to deal with, for instance, by reformulating our decomposition in terms of a constrained minimisation, or by calculating our covariance using some kind of kernel function (rather than the data directly).\nMore concering, as we’ve seen, is that without any kind of prior model for how different features ‘should’ be related to one another, the factors we extract can often vary quite significantly when ran over different samples periods.\nIndeed, given that we fit a single weight for each ticker in our universe in a given period, the problem is in fact somewhat worse. Under the hood of the sector / index aggregation above, our weights were defined over entirely different sets of tickers (specifically, their different daily return covariances), as our universe rolled through time.\nAs our universe changes, we’re thus left without any factor weights for new tickers entering our universe (forcing us, for instance, to continually refit our factor matrices).\nThere are, however, a number of ways that we can deal with this - for instance, by constraining our rolling weights towards stability in sector space (see here).\n\n\nRelated posts\nSee here for a similar post using PCA to decompose bond / swap yield curve factors.\nSee here for a toy PCA example intended to help build up intuition for the geometry of the eigendecomposition above, and here for some notes on orthogonality (on which said decomposition relies)."
  },
  {
    "objectID": "posts/rates_pca/index.html",
    "href": "posts/rates_pca/index.html",
    "title": "Rates: PCA",
    "section": "",
    "text": "In this post, I’ll demonstrate how to use PCA to extract a set of yield curve factors from historic market data.\nI’ll then provide an intuitive interpretation for the factors in terms of the level, slope, and curvature of the yield curve through time.\nI’ll conclude an example of much the shape of the extracted factors can vary through time, and the implications of this w.r.t. next steps.\n\nSetup\n\nimport numpy\nimport pandas\nimport jax\nimport jax.numpy\n\nimport xtuples as xt\nimport xfactors as xf\n\nc:\\python39\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning:\n\nA NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.2\n\n\n\n\n\nData\nWe’ll start by loading some yield data for US and DE swap and bond curves.\n\n\nReturns\ndfs_curves = xf.bt.data.curves.curve_dfs(\n    curves=xt.iTuple([\n        \"YCSW0023\",\n        \"YCGT0025\",\n        \"YCSW0045\",\n        \"YCGT0016\",\n    ]),\n    dp=\"../xfactors/__local__/csvs\"\n)\ndfs_curves = {\n    curve: xf.utils.dfs.apply_na_threshold(\n        df, na_threshold=(0., 0.,), na_padding=(0.2, 0.4,)\n    )\n    for curve, df in dfs_curves.items()\n}\n\n\nWhere, here, we filter to only those rows containing data for at least … of tenors, out of those tenors containing data for at least … of rows.\nFor an example using the full data-set, including missing entries, see here for example.\nThis gives us back, US bond yields back to 2010 (with the 2005-2010 period filterd for lack of data):\n\n\nUSD-G Yields: 2005-2023\nxf.bt.data.curves.curve_chart(\n    dfs_curves, \n    xf.utils.dates.y(2005), \n    xf.utils.dates.y(2023), \n    \"USD-G\"\n)\n\n\nc:\\python39\\lib\\site-packages\\_plotly_utils\\basevalidators.py:107: FutureWarning:\n\nThe behavior of DatetimeProperties.to_pydatetime is deprecated, in a future version this will return a Series containing python datetime objects instead of an ndarray. To retain the old behavior, call `np.array` on the result\n\n\n\n\n                                                \n\n\nAnd US swap yields for the full period 2005 to 2023:\n\n\nUSD-S Yields: 2005-2023\nxf.bt.data.curves.curve_chart(\n    dfs_curves, \n    xf.utils.dates.y(2005),\n    xf.utils.dates.y(2023),\n    \"USD-S\"\n)\n\n\nc:\\python39\\lib\\site-packages\\_plotly_utils\\basevalidators.py:107: FutureWarning:\n\nThe behavior of DatetimeProperties.to_pydatetime is deprecated, in a future version this will return a Series containing python datetime objects instead of an ndarray. To retain the old behavior, call `np.array` on the result\n\n\n\n\n                                                \n\n\nAs you can see, we have significantly more tenors available in our swap curve.\n\n\nPCA\nPCA decomposes a given data matrix into a matrix of orthonormal factor loadings, and a matrix of independent unit-gaussian factor embeddings (see the linked posts for more detail).\nThere are roughly two ways to do this:\n\nEigen-decomposition of the input data covariance matrix.\nConstrained minimisation of the re-constructed data matrix (following a round-trip encoding into factor space, and decoding back up into the original data space).\n\nFor now, we’ll go with the first of these, see here for background on the second, and here for an example using the same rates data as here.\n\n\nSector Weights Chart\nimport functools\n@functools.lru_cache(maxsize=10)\ndef fit_pca(curve, d_start, d_end, n):\n    df = dfs_curves[curve]\n    df = xf.utils.dfs.index_date_filter(\n        df, date_start=d_start, date_end=d_end\n    )\n    eigvals, eigvecs = xf.nodes.pca.vanilla.PCA.f(\n        jax.numpy.transpose(df.values), n = n\n    )\n    eigvecs = xf.utils.funcs.set_signs_to(\n        eigvecs.real, axis=1, i=0, signs=[1, -1, -1]\n    )\n    return df, eigvals.real, eigvecs\n\n\nWe can plot the resulting factor loadings as bar charts.\n\n\nSector Weights Chart\ndef pca_weights_chart(curve, d_start, d_end, n):\n    df, eigvals, eigvecs = fit_pca(curve, d_start, d_end, n)\n    weights = pandas.DataFrame(\n        eigvecs.T,\n        columns=df.columns,\n        index=list(range(eigvals.shape[0]))\n    )\n    return xf.visuals.graphs.df_facet_bar_chart(\n        xf.utils.dfs.melt_with_index(weights, index_as=\"factor\"),\n        x=\"variable\",\n        y=\"value\",\n        facet=\"factor\",\n        title=\"{}: {} - {}\".format(curve, d_start, d_end)\n    )\n\n\nAnd the resulting embeddings as time series, in a similar form to the above.\n\n\nSector Weights Chart\ndef pca_path_chart(curve, d_start, d_end, n):\n    df, eigvals, eigvecs = fit_pca(curve, d_start, d_end, n)\n    factors = jax.numpy.matmul(df.values, eigvecs)\n    factors_df = pandas.DataFrame(\n        factors,\n        columns=list(range(eigvals.shape[0])),\n        index=df.index,\n    )\n    return xf.visuals.graphs.df_facet_line_chart(\n        xf.utils.dfs.melt_with_index(factors_df, index_as=\"date\"),\n        x=\"date\",\n        y=\"value\",\n        facet=\"variable\",\n        title=\"{}: {} - {}\".format(curve, d_start, d_end)\n    )\n\n\n\n\nFactors - Loadings\nFor instance, below are the first three factors extracted from a history of the USD government bond curve spanning from 2005 through to April 2023:\n\n\nFactor Loadings: USD-G\npca_weights_chart(\"USD-G\", xf.utils.dates.y(2005), xf.utils.dates.y(2023), 3)\n\n\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nc:\\python39\\lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:3662: UserWarning:\n\n'kind' argument to argsort is ignored; only 'stable' sorts are supported.\n\n\n\n\n                                                \n\n\nAs one can see, there is a clear structure to the three components.\nThe first ‘level’ component - capturing the majority of the variance of our yield curve - has loadings with all the same sign, downward sloping from a peak amongst the first few tenors, and is conventionally positive.\nThe second component slopes (roughly) monotonically from one sign to another (conventionally from negative to positive), with a single zero point, and is known as the ‘slope’ factor.\nThe third generally has two zero points, with the front- and back-end of the curve weighted counter to the belly, and is referred to as the ‘curvature’.\nAs one can see, we get a very similar result from the USD swap curve:\n\n\nFactor Loadings - USD-S\npca_weights_chart(\"USD-S\", xf.utils.dates.y(2005), xf.utils.dates.y(2023), 3)\n\n\nc:\\python39\\lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:3662: UserWarning:\n\n'kind' argument to argsort is ignored; only 'stable' sorts are supported.\n\n\n\n\n                                                \n\n\n\n\nFactors - Embeddings\nLet’s then plot the path of the factor embeddings (here, for the US swaps curve):\n\n\nSector Weights Chart\npca_path_chart(\"USD-S\", xf.utils.dates.y(2005), xf.utils.dates.y(2023), 3)\n\n\nc:\\python39\\lib\\site-packages\\_plotly_utils\\basevalidators.py:107: FutureWarning:\n\nThe behavior of DatetimeProperties.to_pydatetime is deprecated, in a future version this will return a Series containing python datetime objects instead of an ndarray. To retain the old behavior, call `np.array` on the result\n\n\n\n\n                                                \n\n\nOne can clearly see the 2005, 2018 and 2022 hiking cycles, and the 07-08 post-GFC rate cuts, in the level component.\nThe 2022-23 flattening is very clearly visible in the slope component (blunting the rise in the level component on the back end of the curve).\nWe can also very clearly see the steady march flatter from 2013 through to covid, as back-end swap rates first compressed in through to 2016, before the front-end then rose up to meet them into the 2018 Powell pivot.\nOne can arguably also see evidence of intermittent flattening bouts (roughly) aligning with the 08-10 and 10-11 and 12-13 rounds of quantitative easing (with QE designed to push down the back-end of the curve).\n\n\nBonds vs Swaps\nAs one can see, the factor paths are highly correlated between the two USD curves we have.\nHere, the government curve history:\n\n\nSector Weights Chart\npca_path_chart(\"USD-G\", xf.utils.dates.y(2005), xf.utils.dates.y(2023), 3)\n\n\nc:\\python39\\lib\\site-packages\\_plotly_utils\\basevalidators.py:107: FutureWarning:\n\nThe behavior of DatetimeProperties.to_pydatetime is deprecated, in a future version this will return a Series containing python datetime objects instead of an ndarray. To retain the old behavior, call `np.array` on the result\n\n\n\n\n                                                \n\n\nAnd, here, the USD swaps curve decomposition for the same time range (as we have less complete data for the bond history - see here for an excercise in interpolating the rest using matrix completion):\n\n\nSector Weights Chart\npca_path_chart(\"USD-S\", xf.utils.dates.y(2010), xf.utils.dates.y(2023), 3)\n\n\nc:\\python39\\lib\\site-packages\\_plotly_utils\\basevalidators.py:107: FutureWarning:\n\nThe behavior of DatetimeProperties.to_pydatetime is deprecated, in a future version this will return a Series containing python datetime objects instead of an ndarray. To retain the old behavior, call `np.array` on the result\n\n\n\n\n                                                \n\n\nWhere the key difference that I would highlight is the relative difference in flattening going into 2023, with the government curve remaining relatively steeper (ie. with longer dated bonds proportionately wider than the equivalent swaps).\n\n\nDiscussion\nLet’s try running the same decomposition, but on a smaller subset of our yield history.\nFor instance, on our swap curve, but between only 2010 and 2015:\n\n\nSector Weights Chart\npca_weights_chart(\"USD-S\", xf.utils.dates.y(2010), xf.utils.dates.y(2015), 3)\n\n\n\n                                                \n\n\nAs one can see, the shape of level component has both almost entirely inverted, and has more or completely flattened at the front end.\nFurthermore, our slope component is no longer monotonic, having developed a peak of it’s own around the 18 month point.\nWe get a similar result from the bond curve:\n\n\nSector Weights Chart\npca_weights_chart(\"USD-G\", xf.utils.dates.y(2010), xf.utils.dates.y(2015), 3)\n\n\n\n                                                \n\n\nHere, the level component is even slightly negative at the front-end.\nThese loadings make some sense: during this period fed rates were firmly anchored at the zero lower bound, so there was relatively little market volatility in front-end rates.\nYou can see this if you scroll back up to the yield curve time series above (observe the first few tenors pinned close to zero).\nAs such, the first two components - each of which simply captures co-variation in yields - have very small loadings on the front end (as these tenors simply didn’t move very much during this period).\nWe also see something very similar in the EUR swaps factors for 2017 through 2019:\n\n\nSector Weights Chart\npca_weights_chart(\"EUR-S\", xf.utils.dates.y(2017), xf.utils.dates.y(2019), 3)\n\n\nc:\\python39\\lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:3662: UserWarning:\n\n'kind' argument to argsort is ignored; only 'stable' sorts are supported.\n\n\n\n\n                                                \n\n\nWhich, again, makes sense if you refer back to the relevant period in the yield curve history:\n\n\nSector Weights Chart\nxf.bt.data.curves.curve_chart(dfs_curves, xf.utils.dates.y(2005), xf.utils.dates.y(2023), \"EUR-S\")\n\n\nc:\\python39\\lib\\site-packages\\_plotly_utils\\basevalidators.py:107: FutureWarning:\n\nThe behavior of DatetimeProperties.to_pydatetime is deprecated, in a future version this will return a Series containing python datetime objects instead of an ndarray. To retain the old behavior, call `np.array` on the result\n\n\n\n\n                                                \n\n\nAnd, to some extent, in the German government bond curve from 2012 to 2016:\n\n\nSector Weights Chart\npca_weights_chart(\"EUR-DE\", xf.utils.dates.y(2012), xf.utils.dates.y(2016), 3)\n\n\nc:\\python39\\lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:3662: UserWarning:\n\n'kind' argument to argsort is ignored; only 'stable' sorts are supported.\n\n\n\n\n                                                \n\n\nWhich, again, makes sense given the relative volatility of the front and long end of the curve during this period:\n\n\nSector Weights Chart\nxf.bt.data.curves.curve_chart(dfs_curves, xf.utils.dates.y(2005), xf.utils.dates.y(2023), \"EUR-DE\")\n\n\nc:\\python39\\lib\\site-packages\\_plotly_utils\\basevalidators.py:107: FutureWarning:\n\nThe behavior of DatetimeProperties.to_pydatetime is deprecated, in a future version this will return a Series containing python datetime objects instead of an ndarray. To retain the old behavior, call `np.array` on the result\n\n\n\n\n                                                \n\n\nThese are, admitteldy, particularly striking examples of factor instability - coinciding as they did with periods of especially unusual central bank monetary policy.\nThat being said, it is nonetheless clear that, without further prior structure imposed on the shape of our components, the factors extracted by PCA can vary significantly through time (though, less than that observed from equivalent analysis in equity markets).\n\n\nConclusion\nPCA is a useful first step in decomposing a yield curve into a smaller number of more manageable factors.\nHowever, vanilla PCA - particularly in the eigen-decomposition form we used above - suffers from a couple of key limitations:\n\nincompatibility with missing data (hence the truncated history of our USD bond decomposition).\nfactor loading instability, given the relatively un-opionated nature of the decomposition (diagonal variance maximisation).\n\nWe can naively deal with the first of these by calculating our covariance matrix pair-wise, in a way that’s robust to missing data, as in here.\nHowever, this can lead to inaccurate covariance estimates for tenors under-represented in our dataset. This can be somewhat improved upon by moving to a covariance kernel defined over tenor space, however we’re still unable to calculate the embedding values for dates with missing tenor yields.\nAn alternative approach, which does allow for latent embedding predictions over dates with missing yields, is to use matrix completion, in which we fit our factor loadings using a constrained L2 minimisation rather than eigen-decomposition.\nThis, then, can be naturally combined with our covariance tenor-kernel, to allow us to make yield predictions for entirely unseen new tenors.\nA final step, and one that naturally deals with both missing data and the second limitation (of factor loading instability), is to move away from discrete loading vectors entirely, and to instead use parametric factors function, on which we can impose a greater deal of prior knowledge (again, fit by a constrained l2 minimisation)."
  },
  {
    "objectID": "wip/ab/index.html",
    "href": "wip/ab/index.html",
    "title": "Intuition: Alpha Beta Filtering",
    "section": "",
    "text": "Work In Progress"
  },
  {
    "objectID": "wip/equity_pca_rgmm/index.html",
    "href": "wip/equity_pca_rgmm/index.html",
    "title": "Equity: Rolling Factor Clustering",
    "section": "",
    "text": "Work In Progress\nIn this post, we demonstrate how to use Gaussian Mixture Models to cluster the factors extracted from a rolling PCA over daily equity return data."
  },
  {
    "objectID": "wip/gmm/index.html",
    "href": "wip/gmm/index.html",
    "title": "Intuition: Gaussian Mixture Model",
    "section": "",
    "text": "Work In Progress"
  },
  {
    "objectID": "wip/gp/index.html",
    "href": "wip/gp/index.html",
    "title": "Intuition: Gaussian Process",
    "section": "",
    "text": "Work In Progress"
  },
  {
    "objectID": "wip/kalman/index.html",
    "href": "wip/kalman/index.html",
    "title": "Intuition: Kalman Filtering",
    "section": "",
    "text": "Work In Progress"
  },
  {
    "objectID": "wip/kmeans/index.html",
    "href": "wip/kmeans/index.html",
    "title": "Intuition: K-Means",
    "section": "",
    "text": "Work In Progress"
  },
  {
    "objectID": "wip/linreg/index.html",
    "href": "wip/linreg/index.html",
    "title": "Intuition: Multivariate Linear Regression",
    "section": "",
    "text": "Work In Progress"
  },
  {
    "objectID": "wip/pca_k/index.html",
    "href": "wip/pca_k/index.html",
    "title": "Intuition: PCA (Kernel)",
    "section": "",
    "text": "Work In Progress"
  },
  {
    "objectID": "wip/ppca/index.html",
    "href": "wip/ppca/index.html",
    "title": "Intuition: PPCA",
    "section": "",
    "text": "Work In Progress\nOrthog constrained L2 reproduction\nDirect cov eigen l2 constraint\nMatrix completion"
  },
  {
    "objectID": "wip/rates_gp/index.html",
    "href": "wip/rates_gp/index.html",
    "title": "Rates: Gaussian Process, Tenor Kernel",
    "section": "",
    "text": "Work In Progress\nIn this post, we demonstrate how to use our tenor covariance kernel to make yield curve predictions using a Gaussian Process (GP).\nWe then show how to use such a Gaussian Process to extend our previous work on yield curve reversion to also include probability density estimates."
  },
  {
    "objectID": "wip/rates_ksmoothing/index.html",
    "href": "wip/rates_ksmoothing/index.html",
    "title": "Rates: Kernel Matrix Completion",
    "section": "",
    "text": "Work In Progress\nIn this post, we show how to combine our previous work on tenor kernels with matrix completion methods to make yield predictions for completely new tenors on the curve.\nWe illustrate the value of this approach by deriving a synthetic swap spread history from interpolated histories of a pair of bond and swap curves."
  },
  {
    "objectID": "wip/rates_ns_family/index.html",
    "href": "wip/rates_ns_family/index.html",
    "title": "Rates: Nelson Siegel Family",
    "section": "",
    "text": "Work In Progress"
  },
  {
    "objectID": "wip/rates_pca_rgmm/index.html",
    "href": "wip/rates_pca_rgmm/index.html",
    "title": "Rates: Rolling Factor Clustering",
    "section": "",
    "text": "Work In Progress"
  },
  {
    "objectID": "wip/rates_pca_smoothing/index.html",
    "href": "wip/rates_pca_smoothing/index.html",
    "title": "Rates: Matrix Completion & Reversion",
    "section": "",
    "text": "Work In Progress\nIn this post, we demonstrate how to use matrix completion to make predictions for missing values in a given yield curve history.\nWe then show how to use this approach to make forward looking reversion predictions for particular tenors on a given yield curve.\nWe conclude by reviewing the effectiveness of this approach in a mean squared error sense, before discussing how we might review in terms of an actual trading strategy."
  },
  {
    "objectID": "wip/rates_pca_smoothing/index.html#setup",
    "href": "wip/rates_pca_smoothing/index.html#setup",
    "title": "Rates: Matrix Completion & Reversion",
    "section": "Setup",
    "text": "Setup\n\nimport functools\nimport numpy\nimport pandas\nimport jax\nimport jax.numpy\n\nimport xtuples as xt\nimport xfactors as xf\n\nc:\\python39\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning:\n\nA NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.2\n\n\n\n\nData\nWe’ll begin by loading the same yield curve data as for our post on rates pca.\n\ndfs_curves = xf.bt.data.curves.curve_dfs(\n    curves=xt.iTuple([\n        \"YCSW0023\",\n        \"YCGT0025\",\n        \"YCSW0045\",\n        \"YCGT0016\",\n    ]),\n    dp=\"../xfactors/__local__/csvs\"\n)\n\nIn this case, however, we’ll retain the full data-set, complete with missing values.\n\n\nUSD-G Yields: 2005-2023\nxf.bt.data.curves.curve_chart(\n    dfs_curves, \n    xf.utils.dates.y(2005), \n    xf.utils.dates.y(2023), \n    \"USD-G\"\n)\n\n\nc:\\python39\\lib\\site-packages\\_plotly_utils\\basevalidators.py:107: FutureWarning:\n\nThe behavior of DatetimeProperties.to_pydatetime is deprecated, in a future version this will return a Series containing python datetime objects instead of an ndarray. To retain the old behavior, call `np.array` on the result\n\n\n\n\n                                                \n\n\nThis is particularly obvious for our government bond curve, which gets pretty sparse pre-2010.\n\n\nPPCA: Constrained L2 Minimisation\nWe’ll now use matrix completion to attempt to fill in the missing values.\nWe saw in this post on ppca that one can fit an equivalent set of factor loadings and embeddings to that of pca using a constrained L2 minimisation of the reconstruction error, rather than an eigen-decomposition.\nHere, we’ll go one step further, we’ll attempt to jointly fit both the orthogonal factor loading matrix, and the particular missing values from our original data-set, under the same constrained l2 minimisation."
  },
  {
    "objectID": "wip/rates_pf/index.html",
    "href": "wip/rates_pf/index.html",
    "title": "Rates: Parametric Factors",
    "section": "",
    "text": "Work In Progress"
  },
  {
    "objectID": "wip/rates_rpf/index.html",
    "href": "wip/rates_rpf/index.html",
    "title": "Rates: Rolling Parametric Factors",
    "section": "",
    "text": "Work In Progress"
  },
  {
    "objectID": "wip.html",
    "href": "wip.html",
    "title": "wip",
    "section": "",
    "text": "Intuition: Alpha Beta Filtering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTom Williams\n\n\n\n\n\n\n\n\nEquity: Rolling Factor Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTom Williams\n\n\n\n\n\n\n\n\nIntuition: Gaussian Mixture Model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTom Williams\n\n\n\n\n\n\n\n\nIntuition: Gaussian Process\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTom Williams\n\n\n\n\n\n\n\n\nIntuition: Kalman Filtering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTom Williams\n\n\n\n\n\n\n\n\nIntuition: K-Means\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTom Williams\n\n\n\n\n\n\n\n\nIntuition: Multivariate Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTom Williams\n\n\n\n\n\n\n\n\nIntuition: PCA (Kernel)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTom Williams\n\n\n\n\n\n\n\n\nIntuition: PPCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTom Williams\n\n\n\n\n\n\n\n\nRates: Gaussian Process, Tenor Kernel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTom Williams\n\n\n\n\n\n\n\n\nRates: Kernel Matrix Completion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTom Williams\n\n\n\n\n\n\n\n\nRates: Nelson Siegel Family\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTom Williams\n\n\n\n\n\n\n\n\nRates: Rolling Factor Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTom Williams\n\n\n\n\n\n\n\n\nRates: Matrix Completion & Reversion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTom Williams\n\n\n\n\n\n\n\n\nRates: Parametric Factors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTom Williams\n\n\n\n\n\n\n\n\nRates: Rolling Parametric Factors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTom Williams\n\n\n\n\n\n\nNo matching items"
  }
]