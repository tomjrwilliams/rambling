[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "rambling",
    "section": "",
    "text": "PCA: Equity Returns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\nTom Williams\n\n\n\n\n\n\n  \n\n\n\n\nIntuition: Orthogonality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nTom Williams\n\n\n\n\n\n\n  \n\n\n\n\nIntuition: PCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nTom Williams\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/orthogonality/index.html",
    "href": "posts/orthogonality/index.html",
    "title": "Intuition: Orthogonality",
    "section": "",
    "text": "This post is a collection of little visualisations intended to help one build up intuition for the notions of vector and matrix orthogonality.\n\nimport operator\nimport numpy\nimport pandas\nimport jax\nimport jaxopt\nimport latexify\n\nimport xtuples as xt\nimport xfactors as xf\n\n\nOrthogonal vectors\nTwo vectors are orthogonal to one another if their dot product is zero, where the dot product is as below (forgive the slightly odd formulation - I’m still getting the hang of latexify).\n\n\n\\[ \\displaystyle \\mathrm{dot}(x1, x2) = \\sum \\left({\\mathrm{map}\\left(operator.product, x1, x2\\right)}\\right) \\]\n\n\nThe dot product of two vectors gives the magnitude of the projection of one onto the other.\nVisually, this represents the length of the shadow that one vector casts on the other (if both are drawn as rays from the origin).\n\ndef unit_vector_projection(v, unit_v):\n    dot = jax.numpy.dot(v, unit_v)\n    return dot * unit_v, unit_v * jax.numpy.sign(v)\n\nTo visualise this, we can plot the dot product of a set of random unit magnitude 2d vectors, with the 2d basis vectors of the same component signs:\n\ndef unit_vector_plot(rows, cols):\n    unit_v = jax.numpy.eye(2)\n    return xf.graphs.vector_ray_plot(\n        xt.iTuple.range(rows)\n        .map(lambda _: xt.iTuple.range(cols).map(\n            lambda _: (\n                lambda v: jax.numpy.stack([\n                    v,\n                    *unit_vector_projection(v, unit_v[0]),\n                    *unit_vector_projection(v, unit_v[1]),\n                ])\n            )(xf.rand.norm_gaussian((2,)))\n        )),\n        markers=True,\n        color=None,\n    )\n\nWhere the marker part-way along each basis vector (pointing along the x or y axis), is placed at the dot product of said basis vector with the respective random unit vector:\n\nunit_vector_plot(3, 3)\n\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n\n                                                \n\n\nAs one can see, given their magnitudes, the larger the dot product of two vectors, the closer they are to pointing in the same direction.\nConversely, the smaller their dot product, the closer they are to perpendicular.\nWe can make this more explicit by noting that the dot product is equal to the product of the magnitude of the two vectors and the cosine of the angle between them.\nAs such, a zero dot product indicates that the angle between the two vectors is pi / 2 radians (or 90 degrees) - ie. that they’re perpendicular to one another.\nSimilarly, as cos(0) = 1, the dot product of a vector with itself is simply equal to it’s squared magnitude.\n\n\nUnit sphere\nThe set of all 2D vectors of unit magnitude forms a circle:\n\nxf.graphs.vector_ray_plot(\n    xf.rand.norm_gaussian((25, 2,)),\n    color=None,\n)\n\n\n                                                \n\n\nThe set of all 3D unit vectors forms a sphere:\n\nxf.graphs.vector_ray_plot(\n    xf.rand.norm_gaussian((25, 3,)),\n    _3d=True,\n    color=None,\n)\n\n\n                                                \n\n\nAnd so on into higher dimensions.\n\n\nOrthogonal matrices\nIf two vectors are both unit norm and orthogonal to one another, then we say that they are orthonormal.\nIf each of the vectors in a square matrix is orthonormal w.r.t. each of the others, we say that the matrix is orthogonal.\nAs such, we can see that:\n\neach orthogonal matrix is a rotation of the equivalent dimensionality basis vector matrix.\nthe columns / rows of each orthogonal matrix sit on the relevant dimensionality unit (hyper) sphere.\n\nFor instance, in two dimensions, each of the colored pairs of vectors in the below is just a rotation of the blue unit basis vectors (along the x, y axes), around our unit circle:\n\nxf.graphs.vector_ray_plot(\n    numpy.concatenate([\n        numpy.expand_dims(numpy.eye(2), 0),\n        xf.rand.orthogonal(2, shape = (5,))\n    ]),\n    color=\"g\",\n)\n\n\n                                                \n\n\nAnd, in three dimensions, each of the colored triplets of vectors in the below is just a rotation of the blue unit basis vectors (along the x, y, z axes), around our unit sphere:\n\nxf.graphs.vector_ray_plot(\n    numpy.concatenate([\n        numpy.expand_dims(numpy.eye(3), 0),\n        xf.rand.orthogonal(3, shape = (5,))\n    ]),\n    _3d=True,\n    color=\"g\",\n)\n\n\n                                                \n\n\nWe can thus interpret an orthogonal matrix as a rotation, mapping us from a space in which our axes point in the directions of our basis vectors, to a space in which our axes point in the directions of our matrix.\n\n\nMatrix multiplication\nA matrix multiplication is just lots of separate dot products.\nIn terms of the above, it is the size of the shadow cast by each row of the left matrix, on each column of the right matrix.\nI like to think of it as shooting particles in the directions and magnitudes of the rows of the left matrix, onto ‘deflectors’ with the orientation and magnitudes of the columns of the right matrix.\nFor instance, given a couple of random unit norm matrices:\n\nN = 2\nM1 = xf.rand.norm_gaussian((N, N,))\nM2 = xf.rand.norm_gaussian((N, N,))\ndef render_matrix(m):\n    return xf.rendering.render_df_color_range(\n        pandas.DataFrame(m),\n        v_min=-1.,\n        v_max=.1,\n    )\ndisplay(render_matrix(M1))\ndisplay(render_matrix(M2))\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n-00.1\n-01.0\n\n\n1\n0.79\n-0.62\n\n\n\n\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n0.82\n0.57\n\n\n1\n-01.0\n-0.09\n\n\n\n\n\nWe can plot out each of the separate dot product projections, akin to how we did above, as so:\n\ndef vector_projection(v1, v2):\n    dot = jax.numpy.dot(v1, v2)\n    return abs(dot) * v2, v2\n\nxf.graphs.vector_ray_plot(\n    [\n        [\n            numpy.stack([\n                M1[r],\n                *vector_projection(M1[r], M2[:, c]),\n            ])\n            for c in range(N)\n        ]\n        for r in range(N)\n    ],\n    share_x=True,\n    share_y=True,\n    markers=True,\n    color=None,\n)\n\n\n                                                \n\n\nBefore comparing the magnitude of the projections above (how far they are from the origin), to the cells of the below:\n\nrender_matrix(numpy.matmul(M1, M2))\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n0.91\n0.03\n\n\n1\n1.26\n0.5\n\n\n\n\n\nConfirming that each cell of the matrix multiplication is just the magnitude of the projection of the relevant row / column of our input matrices.\n\n\nOrthogonality constraint\nGiven the definition of an orthogonal matrix, we can see that multiplication by its transpose will return the relevant size identity matrix:\n\neach of the pairs outside of the main diagonal are orthogonal, and so will return a dot product zero.\neach pair in the main diagonal will just be the dot product of a unit norm vector with itself: ie. 1.\n\n\nM_orth = xf.rand.orthogonal(2)\nrender_matrix(numpy.matmul(M_orth, M_orth.T))\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n1.0\n-0.0\n\n\n1\n-0.0\n1.0\n\n\n\n\n\nAs such, we can construct a rough measure for how orthogonal a given matrix is: the mean squared error between the matmul of a matrix with its transpose, and the relevant size identity matrix.\n\ndef orthogonality_loss(X):\n    XXt = jax.numpy.matmul(X, X.T)\n    I = jax.numpy.eye(XXt.shape[0])\n    return jax.numpy.square(XXt - I).mean()\n\nWe can then ‘orthogonalise’ a matrix using gradient descent, by minisiming this loss function:\n\ndef orthogonalise(X):\n    solver = jaxopt.GradientDescent(fun=orthogonality_loss)\n    res = solver.run(X)\n    params, state = res\n    return params\n\nWhich we can use on each of our random matrices above, as so:\n\nM1_orth = orthogonalise(M1)\nM2_orth = orthogonalise(M2)\nxf.graphs.vector_ray_plot(\n    [\n        numpy.stack([M_orig, M_orth])\n        for M_orig, M_orth in zip([M1, M2], [M1_orth, M2_orth])\n    ],\n    markers=True,\n    color=\"g\",\n)\n\n\n                                                \n\n\nAs you can see, the non-orthogonal blue matrix has been morphed into the orthogonal red matrix.\nWe can verify their orthogonality by taking their matmul with their transpose:\n\ndisplay(render_matrix(numpy.matmul(M1_orth, M1_orth.T)))\ndisplay(render_matrix(numpy.matmul(M2_orth, M2_orth.T)))\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n1.0\n0.0\n\n\n1\n0.0\n1.0\n\n\n\n\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n1.0\n-0.0\n\n\n1\n-0.0\n1.0\n\n\n\n\n\nAnd confirming that we’re left with the relevant size identity matrix.\n\n\nRotation\nTurning back to the interpretation of an orthogonal matrix as a rotation, let’s now plot the transformation of a bunch of random points by each of our two orthogonal matrices.\n\ncloud = xf.rand.gaussian((10, 2,))\ndef plot_cloud(M_, T = False):\n    return xf.graphs.vector_ray_plot(\n        [\n            [\n                cloud, M_, numpy.matmul(cloud, M_),\n            ] + ([] if not T else [\n                M_.T,\n                numpy.matmul(numpy.matmul(cloud, M_), M_.T)\n            ])\n        ],\n        markers=True,\n        range_x=[-3., 3.],\n        range_y=[-3., 3.],\n    )\n\nWhere the left graph is our point cloud, the middle is our orthogonal rotation matrix, and the right is the point cloud after rotation by said matrix:\n\nplot_cloud(M1_orth)\n\n\n                                                \n\n\n\nplot_cloud(M2_orth)\n\n\n                                                \n\n\nAs one can see:\n\nthe magnitude of each member of our cloud has been preserved (they’re the same length relative to the origin)\nthe angles between each member of our cloud have been preserved (pick a pair of colors, and compare their angles in left vs right).\n\nConfirming that (our) orthogonal matrices represent a scale and angle preserving rotation.\n\n\nTranspose and Inverse\nAs we saw above, multiplying an orthogonal matrix by its transpose leaves us with the identity matrix.\nAs such, an orthogonal matrix’s tranpose must be equal to its inverse: as M M.T = I = M M-1.\nWe can confirm this by rotating our point cloud back, by multiplication with M.T:\n\nplot_cloud(M1_orth, T = True)\n\n\n                                                \n\n\nWe can find some intuition for this in terms of our interpretation of an orthogonal matrix, above, as a scale-preserving rotation.\nRoughly speaking:\n\nthe inverse of a matrix ‘reverses’ the linear transformation induced by multiplication by said matrix.\nthe transpose of a matrix reflects a rotation on the main diagonal, forming the ‘opposite’ reflection.\n\nThis ‘opposite’ reflection just reverses our original rotation - taking us from our point cloud on the left, back to itself on the right - and is hence our matrix inverse.\n\n\nRelated posts\nOrthogonality is key to understanding PCA, and is how we derive the closed form expression for multi-variate linear regression."
  },
  {
    "objectID": "posts/pca/index.html",
    "href": "posts/pca/index.html",
    "title": "Intuition: PCA",
    "section": "",
    "text": "This is a reference post on PCA in which we walk through a toy example using random data, to help one build intuition for the geometry involved.\n\nimport operator\nimport numpy\nimport pandas\nimport jax\nimport jaxopt\n\nimport xtuples as xt\nimport xfactors as xf\n\n\nGenerative model\nWe’ll start by defining our example data, assuming a 3D latent factor space, a 4D feature space, and 100 example data points:\n\nN_FACTORS = 3\nFACTORS = list(range(N_FACTORS))\n\nN_SAMPLES = 100\nSAMPLES = list(range(N_SAMPLES))\n\nN_FEATURES = 4\nFEATURES = list(range(N_FEATURES))\n\n\nFactors\nFirst, we’ll generate some random, diagonal covariance, gaussian noise, that we’ll refer to as our ‘factors’:\n\nfactors = xf.rand.gaussian((N_SAMPLES, N_FACTORS))\nfactors_df = pandas.DataFrame(\n    factors, index=SAMPLES, columns=FACTORS,\n)\n\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nAs we can see, the sample covariance is (roughly), unit diagonal:\n\nxf.rendering.render_df_color_range(factors_df.cov())\n\n\n\n\n\n\n \n0\n1\n2\n\n\n\n\n0\n0.98\n0.04\n-0.01\n\n\n1\n0.04\n1.53\n-0.01\n\n\n2\n-0.01\n-0.01\n0.91\n\n\n\n\n\nThe covariance between two variables increases (decreases) the more one variable is above (below) it’s mean, conditional on the other variable also being above it’s mean.\nAs such, if we plot our individual factors against one another in 2D we can see their zero-covariance, in the spherical shape of the resulting densities:\n\nxf.graphs.df_density_pair_chart(factors_df, key=\"factor\")\n\n\n                                                \n\n\nSimply follow either the x or y axis, and note that the conditional average of the other axis (the other variable) remains constant: implying (roughly) zero covariance.\nIf we plot the 1D density, we can see that each factor also has (roughly) the same unit variance gaussian distribution:\n\nxf.graphs.df_line_chart(\n    xf.densities.gaussian_kde_1d_df(\n        {\n            i: factors[:, i]\n            for i in range(N_FACTORS)\n        },\n    ),\n    x=\"position\",\n    y=\"density\",\n    color=\"key\",\n)\n\n\n                                                \n\n\n\n\nFeatures\nGiven this, we’ll now generate another random gaussian weight matrix:\n\nweights = xf.rand.gaussian((N_FACTORS, N_FEATURES))\n\nWhich we can render as so:\n\nweights_df = pandas.DataFrame(\n    weights, columns=FEATURES, index=FACTORS\n)\nxf.rendering.render_df_color_range(weights_df)\n\n\n\n\n\n\n \n0\n1\n2\n3\n\n\n\n\n0\n0.81\n-2.98\n0.19\n0.72\n\n\n1\n-01.0\n0.97\n0.89\n0.89\n\n\n2\n-0.01\n-1.45\n1.15\n0.61\n\n\n\n\n\nAnd which we’ll then multiply with our factors, to get some features:\n\nfeatures = factors @ weights\nfeatures_df = pandas.DataFrame(\n    features, columns=FEATURES, index=SAMPLES\n)\n\nAs one can see in the resulting 2D density plot, we’ve now added some linear dependence to our dataset:\n\nfor chart in xf.graphs.df_density_pair_chart(\n    features_df, key=\"feature\", separate=True\n):\n    display(chart)\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nWhich we can confirm by calculating our feature (sample) covariance matrix:\n\nxf.rendering.render_df_color_range(features_df.cov())\n\n\n\n\n\n\n \n0\n1\n2\n3\n\n\n\n\n0\n2.12\n-3.69\n-01.2\n-0.79\n\n\n1\n-3.69\n11.7\n-0.79\n-1.62\n\n\n2\n-01.2\n-0.79\n2.42\n1.97\n\n\n3\n-0.79\n-1.62\n1.97\n2.08\n\n\n\n\n\n\n\nProblem statement\nGiven our feature matrix, and assuming the distribution of our factors (but not the values), our goal is to impute sensible values for both the weights and the factors.\n\n\n\nRotation and Scaling\nVisually, we can separate out the distributional changes affected by our weight matrix into two steps:\n\na rescaling\na rotation\n\nSo, to reverse our weight matrix (taking us from features back to factors), we presumably need to:\n\nun-rotate our features, so that - in terms of the 2D densities above - they ‘sit’ back on the original x, y axes.\nde-scale them back to unit variance.\n\nTo visualise this, we can cheat slightly and, assuming the methodology we’re in the process of deriving, de-rotate those 2D densities from above:\n\n\nDe-Rotate\ndef threshold(df, k, quantile):\n    return df[df[k] &gt; numpy.quantile(df[k], quantile)]\n\ndef de_rotate(df):\n    df = xf.graphs.f_df_density_pair_df(\n        FEATURES, \"pair\",\n    )(df)\n    z = \"density\"\n    dfs = {\n        pair: df[df[\"pair\"] == pair]\n        for pair in df[\"pair\"].unique()\n    }\n    dfs_threshold = {\n        pair: threshold(_df, z, .7)\n        for pair, _df in dfs.items()\n    }\n    ws = {\n        pair: jax.numpy.linalg.eig(\n            jax.numpy.cov(numpy.stack([\n                _df[\"x\"].values, _df[\"y\"].values\n            ]))\n        )[1].real\n        for pair, _df in dfs_threshold.items()\n    }\n    return [\n        pandas.DataFrame(\n            numpy.matmul(\n                _df[[\"x\", \"y\"]].values, ws[pair]\n            ),\n            columns=[\"x\", \"y\"],\n            index=_df.index,\n        ).assign(density=numpy.exp(_df[z]), pair=_df[\"pair\"])\n        for pair, _df in dfs.items()\n    ]\n\n\nWhich we’ll plot with separate color scales, so you can see more clearly how the density is now aligned with the conventional x, y axes:\n\nfor derotated_df in de_rotate(features_df):\n    display(xf.graphs.df_scatter_chart(\n        derotated_df,\n        x=\"x\",\n        y=\"y\",\n        color=\"density\",\n        height=400,\n        width=600,\n    ))\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nAs one can see, where the distributions are non-spherical, they now ‘point’ out along the conventional x, y axes.\nWhere they were roughly spherical to begin with, the rotation hasn’t actually changed the (bi-variate) distribution, as a sphere is invariant under rotation.\nThen, to get back to our factors, we simply have to re-scale the de-rotated distributions above, back to unit variance.\n\nSolution\nLinear transformations are composable, which means that we can separate out these two steps into separate matrices, before multiplying them back together (to get a single embedding matrix).\nSo, to go from features to factors, we’re looking for two matrices:\n\none to de-rotate our features back into factors.\none to de-scale our factors back to unit variance.\n\nA scaling matrix just stretches each dimension individually, so we can presume that to be (non-unit) diagonal.\nAssuming we use an orthogonal rotation matrix, we can use the same matrix to rotate back and forth between our features and our factors.\nAs such, we’re looking for:\n\nan orthogonal rotation matrix\na diagonal scaling matrix.\n\n\n\n\nCovariance\nCovariance under a linear transformation is defined as so:\n\ndef linear_cov(W, X):\n    return W @ numpy.cov(X) @ W.T\n\nAs we saw above, we’re going to assume that we can model our weight matrix as the product of an orthogonal rotation matrix, and a diagonal scaling matrix.\nAs such, we can write our covariance function (for our features), as so:\n\ndef feature_cov(Orthog, Diag, X):\n    return (Orthog @ Diag) @ numpy.cov(X) @ (Orthog @ Diag).T\n\nWhich we can simplify to:\n\ndef feature_cov(Orthog, Diag, X):\n    return Orthog @ numpy.cov(X) @ (Diag ** 2) @ Orthog.T\n\nThis, intuitively, makes sense:\n\nFirst we multiply our factor covariance by the square of our scaling matrix, as (co) variance is a quadratic dispersion measure.\nThen, we multiply through by our rotation matrix, twice: one to capture any change in scale from the projection itself, and then once to actually apply the rotation in question.\n\n\n\nEigen vectors\nGiven our assumption that the factors have diagonal covariance, we can make one further simplication to the above:\n\ndef feature_cov(Orthog, Diag, X):\n    return Orthog @ numpy.diag(numpy.cov(X)) @ (Diag ** 2) @ Orthog.T\n\nIf we right multiply through both sides by our rotation matrix:\n\ndef rotated_feature_cov(Orthog, Diag, X, features):\n    return numpy.cov(features) @ Orthog == (\n        Orthog @ numpy.diag(numpy.cov(X)) @ (Diag ** 2)\n        @ Orthog.T @ Orthog\n    )\n\nThe Orthog.T @ Orthog term (on the far right) cancels to I.\n\ndef rotated_feature_cov(Orthog, Diag, X, features):\n    return numpy.cov(features) @ Orthog == (\n        Orthog @ numpy.diag(numpy.cov(X)) @ (Diag ** 2)\n    )\n\nAs such, we can see that multiplication of our feature covariance by each of the vectors in our rotation matrix simply returns a scaled version of said vector, with the scaling term given by the relevant diagonal element in diag(cov(X)) @ (Diag ** 2).\nThis, in turn, then tells us that our rotation matrix is in fact comprised of eigenvectors of our feature covariance matrix.\nGiven that our factor distributions were assumed to have unit variance, we can then also see that the diagonal scaling matrix we’re looking for is simply comprised of the square root of the respective eigenvalues.\n\n\nSolution\nAs such, to find our matrices Orthog and Diag we simply need to do an eigen-decomposition of our feature covariance matrix, which we can do as so:\n\neigvals, eigvecs = jax.numpy.linalg.eig(\n    jax.numpy.cov(features.T)\n)\nscale = jax.numpy.sqrt(eigvals.real)\northog = eigvecs.real\n\nAs one can see, we’re only left with 3 (roughly) non-zero eigenvalues:\n\nnumpy.round(eigvals.real, 2)\n\nArray([13.139999  ,  4.94      ,  0.        ,  0.22999999], dtype=float32)\n\n\nThese capture the degree of variation in the feature space, explained by each component (how ‘spread’ out the features are along the axis given by the relevant eigenvector).\nAs such, we can then first sort our matrices by their eigenvalue, before disregarding the smallest.\nHere, we’re assuming we know how many latent factors there were (so we keep only the first N_FACTORS components), but in a more general problem, we could infer this number by the count of non (nearly) zero eigenvalues.\n\norder = jax.numpy.flip(jax.numpy.argsort(eigvals))\nscale = scale[order][:N_FACTORS]\northog = orthog[:, order[:N_FACTORS]]\n\nWe can de-rotate our features into factor space, by multiplication with our orthogonal rotation matrix, as so:\n\nfactors_scaled = features @ orthog\nxf.graphs.df_density_pair_chart(\n    pandas.DataFrame(\n        factors_scaled,\n        columns=FACTORS,\n        index=SAMPLES,\n    ), \n    key=\"factor\"\n)\n\n\n                                                \n\n\nBefore de-scaling them, leaving us with our original unit variance noise, by dividing by the sqrt of the eigenvalues:\n\nfactors_descaled = numpy.divide(\n    factors_scaled,\n    xf.expand_dims(scale, 0, 1)\n)\nxf.graphs.df_density_pair_chart(\n    pandas.DataFrame(\n        factors_descaled,\n        columns=FACTORS,\n        index=SAMPLES,\n    ), \n    key=\"factor\"\n)\n\n\n                                                \n\n\nTo go back up to our features, we can simply re-scale up the factors, and then rotate back into feature space:\n\nfeatures_roundtrip = orthog @ numpy.multiply(\n    factors_descaled,\n    xf.expand_dims(scale, 0, 1)\n).T\nfor chart in xf.graphs.df_density_pair_chart(\n    pandas.DataFrame(\n        features_roundtrip.T,\n        columns=FEATURES,\n        index=SAMPLES,\n    ), \n    key=\"feature\",\n    separate=True,\n):\n    display(chart)\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nWhich, as you can see, returns us back to where we started.\n\n\nDicussion\nOur rotation matrix, above, allows us to map our feature space into the (presumably) much smaller (implied) factor space, whilst nonetheless retaining most of the ‘information’ of the original features.\nThis helps us combat the ‘curse of dimensionality’ (the tendency for distance measures to explode as the number of dimensions increases), as we can train downstream models using the smaller dimensional (implied) factors, rather than the (potentially much) larger dimensional features.\nThe embedding matrix itself also encodes useful information about the correlation structure of our features, which can often be interpreted directly (for instance, see the below two posts analysing single name equity returns and bond / swap yield curves, respectively).\nPCA also naturally lends itself to a number of different extensions - particularly once we move from a strict eigen-decomposition, as above, onto a more general gradient based optimisation approach, such as in this post on ppca.\n\n\nRelated posts\nBelow are two practical examples of using PCA:\n\nTo extract equity market factors, from daily single-name equity returns.\nTo extract yield curve factors, daily per-tenor bond and swap curve yields.\n\nAlso, see here for a primer on orthogonality (useful for understanding the decomposition above)."
  },
  {
    "objectID": "posts/pca_equity/index.html",
    "href": "posts/pca_equity/index.html",
    "title": "PCA: Equity Returns",
    "section": "",
    "text": "In this post, we try out using PCA to extract market factors from daily equity returns."
  },
  {
    "objectID": "posts/pca_equity/index.html#setup",
    "href": "posts/pca_equity/index.html#setup",
    "title": "PCA: Equity Returns",
    "section": "Setup",
    "text": "Setup\n\nimport numpy\nimport pandas\nimport jax\nimport jax.numpy\n\nimport xtuples as xt\nimport xfactors as xf\n\nimport bt.data.prices.int as prices\nimport bt.algos.universe.int as universe\nimport bt.algos.universe.configs as configs\n\n\nData\nWe’ll start by loading some daily close to close total returns data, given a rolling universe of major european equity indices:\n\ndf_returns = prices.returns_df(\n    xf.dates.y(2005),\n    xf.dates.y(2023, m=4),\n    indices=configs.INDICES,\n) \n\nThe returns are from bloomberg, and include all cash and non-cash adjustments (in production we have a separate internal ticker for the dividend stream from a given name, but that’s a little over-complicated for our purposes here).\nWe’ll also load the relevant index membership mapping tables for our universe:\n\ndfs_indices = universe.rolling_indices(\n    xf.dates.y(2005),\n    xf.dates.y(2023, m=4),\n    indices=configs.INDICES,\n)\ndf_universe = universe.index_union(dfs_indices)\n\nAnd some GICS sector mapping tables, as these will be useful later:\n\ndfs_sectors = universe.rolling_indices(\n    xf.dates.y(2005),\n    xf.dates.y(2023, m=4),\n    sectors=configs.GICS_SECTORS,\n)\n\nVanilla PCA doesn’t accomodate missing data, so given a particular target date range, we’ll filter to only those tickers within our universe for the entire period, as so:\n\ndef in_universe(ticker, df, threshold = 1.):\n    if ticker not in df.columns:\n        return False\n    return (\n        df[ticker].sum() / len(df.index)\n    ) &gt;= threshold\n\nWhich we’ll then wrap into a convenience function.\n\n\nget_returns\ndef get_returns(d_start, d_end, threshold=1.):\n    tickers = xt.iTuple(df_returns.columns).filter(\n        in_universe,\n        df=xf.dfs.index_date_filter(df_universe, d_start, d_end),\n        threshold=threshold,\n    ).pipe(list) \n    return xf.dfs.index_date_filter(\n        df_returns, d_start, d_end\n    )[tickers].dropna()\n\n\n\n\nModel\nPCA uses eigendecomposition to extract an orthogonal embedding matrix, together with a diagonal scaling matrix, from the covariance matrix of a given data set.\n\nimport functools\n@functools.lru_cache(maxsize=10)\ndef fit_pca(d_start, d_end, n):\n    df = get_returns(d_start, d_end)\n    eigvals, eigvecs = jax.numpy.linalg.eig(\n        jax.numpy.cov(df.values.T)\n    )\n    order = jax.numpy.flip(jax.numpy.argsort(eigvals))\n    eigvals = eigvals.real[order[:n]]\n    eigvecs = eigvecs.real[:, order[:n]]\n    return eigvals, eigvecs, df\n_ = fit_pca(xf.dates.y(2022), xf.dates.y(2023), n=3)\n\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nThe embedding matrix can then be used to project the data into a lower dimensional factor space, where the relative ‘importance’ of each factor is given by the relevant term in the diagonal scaling matrix (importance being defined as how much of the original data variance each factor ‘explains’).\n\nimport functools\ndef apply_pca(d_start, d_end, n):\n    eigvals, eigvecs, df = fit_pca(d_start, d_end, n)\n    return eigvals, eigvecs, df.values @ eigvecs, df\n\nSee the reference post PCA for a more detailed walk-through of the linear algebra.\n\n\nResults\nAs expected, the factors we extract are linearly independent (have diagonal covariance / correlation):\n\ndef pca_factor_corr(d_start, d_end, n):\n    _, _, factors, _ = apply_pca(d_start, d_end, n)\n    return xf.rendering.render_df_color_range(\n        pandas.DataFrame(factors).corr(),\n        v_min=-1.,\n        v_max=.1,\n    )\npca_factor_corr(xf.dates.y(2022), xf.dates.y(2023), n=3)\n\n\n\n\n\n\n \n0\n1\n2\n\n\n\n\n0\n1.0\n0.0\n0.0\n\n\n1\n0.0\n1.0\n0.0\n\n\n2\n0.0\n0.0\n1.0\n\n\n\n\n\nAnd are each univariate gaussian, with descending variance from PC0 down to PC2 (seen in the increasing concentration of the gaussians in the plot below):\n\ndef pca_factor_density_chart(d_start, d_end, n):\n    _, _, factors, df = apply_pca(d_start, d_end, n)\n    return xf.graphs.df_density_chart(\n        xf.dfs.melt_with_index(\n            pandas.DataFrame(\n                factors,\n                columns=list(range(factors.shape[1])),\n                index=df.index,\n            ),\n            variable_as=\"factor\",\n        ),\n        \"factor\",\n        \"value\",\n    )\npca_factor_density_chart(xf.dates.y(2022), xf.dates.y(2023), n=3)\n\n\n                                                \n\n\nTurning to our factor weights:\n\n\nFactors\ndef pca_weights(d_start, d_end, n):\n    _, eigvecs, _, df = apply_pca(d_start, d_end, n)\n    return pandas.DataFrame(\n        eigvecs.T,\n        columns=df.columns,\n        index=list(range(eigvecs.shape[1]))\n    )\npca_weights(xf.dates.y(2022), xf.dates.y(2023), n=3)\n\n\n\n\n\n\n\n\n\nACA FP Equity\nAI FP Equity\nAIR FP Equity\nALO FP Equity\nAPAM NA Equity\nBN FP Equity\nBNP FP Equity\nCA FP Equity\nCAP FP Equity\nCS FP Equity\n...\nIBE SQ Equity\nITX SQ Equity\nNDA FH Equity\nSAN SQ Equity\nBG AV Equity\nBIRG ID Equity\nBKT SQ Equity\nCABK SQ Equity\nEBS AV Equity\nSAB SQ Equity\n\n\n\n\n0\n-0.060335\n-0.040464\n-0.066809\n-0.076080\n-0.074317\n-0.028223\n-0.065206\n-0.018692\n-0.062472\n-0.048011\n...\n-0.027217\n-0.046161\n-0.050864\n-0.062572\n-0.054376\n-0.054569\n-0.043741\n-0.042946\n-0.076183\n-0.058908\n\n\n1\n-0.070421\n0.015270\n-0.042955\n-0.033241\n-0.033736\n-0.024529\n-0.084168\n-0.036765\n0.041158\n-0.054157\n...\n0.017941\n-0.017879\n-0.040327\n-0.087104\n-0.058716\n-0.095620\n-0.106794\n-0.115320\n-0.105358\n-0.143965\n\n\n2\n0.029703\n0.000996\n0.004294\n0.035607\n-0.085798\n0.046632\n0.034311\n-0.026233\n0.018673\n0.005721\n...\n-0.024447\n0.025555\n-0.012960\n0.011640\n0.021233\n0.043145\n0.032449\n0.003381\n0.053300\n0.037243\n\n\n\n\n3 rows × 370 columns\n\n\n\nOne way to visualise them is with a simple bar chart:\n\ndef pca_factor_weight_chart(d_start, d_end, n):\n    weights = pca_weights(d_start, d_end, n)\n    n = len(weights.index)\n    return xf.graphs.df_facet_bar_chart(\n        xf.dfs.melt_with_index(weights, index_as=\"factor\"),\n        x=\"variable\",\n        y=\"value\",\n        facet=\"factor\",\n    )\npca_factor_weight_chart(xf.dates.y(2022), xf.dates.y(2023), n=3)\n\n\n                                                \n\n\nHowever, as one can see, given that we end up with a single weight per ticker per factor, it’s fairly difficult to know how to interpret each of the above.\nThe one thing we can tell from the above, is that our first factor is (more or less) the same sign for equity ticker in our universe, representing some kind of general market beta factor (a point we’ll return to in the discussion below).\n\n\nSector & Index Aggregation\nTo try and make sense of our weights, we can try averaging them up into something more manageable.\nFor instance, using a function mapping us from a ticker to a GICS sector:\n\n\nSector Tickers\ndef sector_tickers(d_start, d_end, n):\n    _, _, df = fit_pca(d_start, d_end, n)\n    tickers = xt.iTuple(df.columns)\n    return {\n        s: tickers.filter(\n            in_universe, df = dfs_sectors[s], threshold=1.\n        ) for s in sorted(dfs_sectors.keys())\n    }\n\n\nCertain industries will be under or over-represented in our universe, in a given period:\n\ndef sector_map_chart(d_start, d_end, n):\n    sector_map = sector_tickers(d_start, d_end, n)\n    return xf.graphs.df_bar_chart(\n        pandas.DataFrame({\n            s: [len(v)] for s, v in sector_map.items()\n        }).melt(),\n        x=\"variable\",\n        y=\"value\",\n    )\nsector_map_chart(xf.dates.y(2022), xf.dates.y(2023), n=3)\n\n\n                                                \n\n\nSo, we’ll scale our average relative to the sector representation during the period in question, returning something like the below:\n\n\nSector Weights Chart\ndef pca_sector_weights_chart(d_start, d_end, n):\n    weights = pca_weights(d_start, d_end, n)\n    sector_map = sector_tickers(d_start, d_end, n)\n    sector_weights = pandas.DataFrame({\n        s: weights[tickers.pipe(list)].sum(axis=1)\n        for s, tickers in sector_map.items()\n    })\n    scaled_weights = pandas.DataFrame({\n        s: sector_weights[s] / len(ts)\n        for s, ts in sector_map.items()\n    })\n    return xf.graphs.df_facet_bar_chart(\n        xf.dfs.melt_with_index(scaled_weights, index_as=\"factor\"),\n        x=\"variable\",\n        y=\"value\",\n        facet=\"factor\",\n    )\npca_sector_weights_chart(xf.dates.y(2022), xf.dates.y(2023), n=3)\n\n\n\n                                                \n\n\nAs one can see, this is much more easily interpretible.\nWe can go through a similar process for averaging our weights by equity index.\n\n\nIndex Tickers\ndef index_tickers(d_start, d_end, n):\n    _, _, df = fit_pca(d_start, d_end, n)\n    tickers = xt.iTuple(df.columns)\n    return {\n        i: tickers.filter(\n            in_universe, df = dfs_indices[i], threshold=1.\n        ) for i in sorted(dfs_indices.keys())\n    }\n\n\nWhere, again, certain indices will be over or under-represented in our universe:\n\ndef index_map_chart(d_start, d_end, n):\n    index_map = index_tickers(d_start, d_end, n)\n    return xf.graphs.df_bar_chart(\n        pandas.DataFrame({\n            i: [len(v)] for i, v in index_map.items()\n        }).melt(),\n        x=\"variable\",\n        y=\"value\",\n    )\nindex_map_chart(xf.dates.y(2022), xf.dates.y(2023), n=3)\n\n\n                                                \n\n\nWhich we’ll account for in our averaging just as we did for our sectors, giving us back something like the below:\n\n\nIndex Weights Chart\ndef pca_index_weights_chart(d_start, d_end, n):\n    weights = pca_weights(d_start, d_end, n)\n    index_map = index_tickers(d_start, d_end, n)\n    index_weights = pandas.DataFrame({\n        i: weights[tickers.pipe(list)].sum(axis=1)\n        for i, tickers in index_map.items()\n    })\n    scaled_weights = pandas.DataFrame({\n        i: index_weights[i] / len(ts)\n        for i, ts in index_map.items()\n    })\n    return xf.graphs.df_facet_bar_chart(\n        xf.dfs.melt_with_index(index_weights, index_as=\"factor\"),\n        x=\"variable\",\n        y=\"value\",\n        facet=\"factor\",\n    )\npca_index_weights_chart(xf.dates.y(2022), xf.dates.y(2023), n=3)\n\n\n\n                                                \n\n\nWith which, we can get back to the fun part - interpretation!\n\n\nInterpretation - By Sector\nLet’s try plotting our factor sector weights for the period 2020 - 2022:\n\npca_sector_weights_chart(xf.dates.y(2020), xf.dates.y(2022), n=3)\n\n\n                                                \n\n\nAs one can see, the first factor has the same sign for every sector, and if we dig into particular components, seems to roughly match our intuitive notion of index beta (ie. higher beta sectors having larger weights):\n\nHigher beta: 10 (Energy), 20 (Industrials), 25 (Discretionary), 40 (Financials).\nLower beta: 30 (Staples), 35 (Health), 50 (Media / Telcos), 55 (Utilities).\n\nWhilst the exact loadings will vary from period to period, the first - and most important (in a variance-explanatory sense) - component will nearly always tend to be some kind of general market-beta factor.\nThe second and third factors will, however, vary a little more from period to period.\nFor instance, whilst for 2020 - 2022, the second factor seems to be picking up something like a risk-on vs risk-off factor, separating between:\n\nEnergy (10), Financials (40), Real Estate (60).\nMaterials (15), Health(35), Tech (45), Utilities (55).\n\nIf we run the same procedure over 2007 to 2010:\n\npca_sector_weights_chart(xf.dates.y(2007), xf.dates.y(2010), n=3)\n\n\n                                                \n\n\nOur second factor is almost entirely dominated by Financials and Real Estate (40 and 60) versus everything else, and our third by Energy and Materials (10 and 15) versus everything else.\nThis, given the 2007-2010 (housing driven) financial crisis (and the ensuing commodity market reaction to global growth concerns), does actually make sense.\nLet’s now compare to the period 2014 to 2016:\n\npca_sector_weights_chart(xf.dates.y(2014), xf.dates.y(2016), n=3)\n\n\n                                                \n\n\nHere, the Energy and Materials (10 and 15) vs everything else is much more prominent, given the global commodity wobble (at least partly driven by the rise in US shale oil production).\nWe also see an extremely concentrated third component of financials (40) versus everything else, probably driven by another flare up of the 2010’s EU sovereign debt crises.\nWhat we’re seeing here is that because PCA doesn’t have any underlying model for how different equities relate together - it just strips out the ways in which they happened to co-move during a given period - it doesn’t have anything to anchor itself on, and thus returns (samples of) somewhat different factors, depending on which period we run it on.\nWhilst this might reflect a genuine causal difference in the factors driving overall single-name equity returns during different periods, it also might reflect sheer co-occurence (correlation, as they say, does not necessarily mean causation).\n\n\nInterpretation - By Index\nWe get similar results (though personally much less useful), if we instead aggregate by index.\nFor instance, we can see the same commodity wobble in the Norwegian (OBX) and to some extent UK (UKX) weighting over 2014 to 2016 (given their respective commodity exposures):\n\npca_index_weights_chart(xf.dates.y(2014), xf.dates.y(2016), n=3)\n\n\n                                                \n\n\nWhereas we can see something like a peripherals (Spain & Portugal) vs European core factor, if we run over the peak of the EU sovereign debt crisis (2010-2013):\n\npca_index_weights_chart(xf.dates.y(2010), xf.dates.y(2013), n=3)\n\n\n                                                \n\n\nWhere we can see how the EU financials index (SX7E) is moving with the IBEX and PSI (lending support to the idea that this is some kind of financial stress factor).\n\n\nLimitations\nAs noted above, vanilla PCA - at least in the eigendecomposition form above - requires a full data matrix, with no missing values. This can be quite limiting, especially when dealing with real data (as opposed to the above, somewhat stylised, daily equity returns).\nThat being said, this is fairly simple to deal with via, for instance, an irregular covariance matrix, or via some kind of kernel function.\nMore concering, as we’ve seen, is that without any kind of prior model for how different features ‘should’ be related to one another, the factors we extract can often vary quite significantly when ran over different samples periods.\nIndeed, given that we fit a single weight for each ticker in our universe in a given period, the problem is in fact somewhat worse. Under the hood of the sector / index aggregation above, our weights were defined over entirely different sets of tickers (specifically, their different daily return covariances), as our universe rolled through time.\nAs our universe changes, we’re thus left without any factor weights for new tickers entering our universe (forcing us, for instance, to continually refit our factor matrices).\nThere are, however, a number of ways that we can deal with this - for instance, by first mapping our tickers into some kind of latent space, before assigning them factor weights (see here).\n\n\nRelated posts\nSee here for a similar post using PCA to decompose bond / swap yield curve factors.\nSee here for a toy PCA example intended to help build up intuition for the geometry of the eigendecomposition above, and here for some notes on orthogonality (on which said decomposition relies)."
  }
]