[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "rambling",
    "section": "",
    "text": "Intuition: Orthogonality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nTom Williams\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/orthogonality/index.html",
    "href": "posts/orthogonality/index.html",
    "title": "Intuition: Orthogonality",
    "section": "",
    "text": "This post is a collection of little visualisations intended to help one build up intuition for the notions of vector and matrix orthogonality.\n\nimport numpy\nimport pandas\nimport jax\nimport jaxopt\n\nimport xtuples as xt\nimport xfactors as xf\n\n\nOrthogonal vectors\nTwo vectors are orthogonal to one another if their dot product is zero.\n\ndef dot(x1, x2):\n    return sum([v1 * v2 for v1, v2 in zip(x1, x2)])\n\nThe dot product of two vectors gives the magnitude of the projection of one onto the other.\nVisually, this represents the length of the shadow that one vector casts on the other (if both are drawn as rays from the origin).\n\ndef unit_vector_projection(v, unit_v):\n    dot = jax.numpy.dot(v, unit_v)\n    return dot * unit_v, unit_v * jax.numpy.sign(v)\n\nTo visualise this, we can plot the dot product of a set of random unit magnitude 2d vectors, with the 2d basis vectors of the same component signs:\n\ndef unit_vector_plot(rows, cols):\n    unit_v = jax.numpy.eye(2)\n    return xf.graphs.vector_ray_plot(\n        xt.iTuple.range(rows)\n        .map(lambda _: xt.iTuple.range(cols).map(\n            lambda _: (\n                lambda v: jax.numpy.stack([\n                    v,\n                    *unit_vector_projection(v, unit_v[0]),\n                    *unit_vector_projection(v, unit_v[1]),\n                ])\n            )(xf.rand.norm_gaussian((2,)))\n        )),\n        markers=True,\n        color=None,\n    )\n\nWhere the marker part-way along each basis vector (pointing along the x or y axis), is placed at the dot product of said basis vector with the respective random unit vector:\n\nunit_vector_plot(3, 3)\n\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n\n                                                \n\n\nAs one can see, given their magnitudes, the larger the dot product of two vectors, the closer they are to pointing in the same direction.\nConversely, the smaller their dot product, the closer they are to perpendicular.\nWe can make this more explicit by noting that the dot product is equal to the product of the magnitude of the two vectors and the cosine of the angle between them.\nAs such, a zero dot product indicates that the angle between the two vectors is pi / 2 radians (or 90 degrees) - ie. that they’re perpendicular to one another.\nAlternatively, as cos(0) = 1, the dot product of a vector with itself is simply equal to it’s squared magnitude.\n\n\nUnit sphere\nThe set of all 2D vectors of unit magnitude forms a cirle:\n\nxf.graphs.vector_ray_plot(\n    xf.rand.norm_gaussian((25, 2,)),\n    color=None,\n)\n\n\n                                                \n\n\nThe set of all 3D unit vectors forms a sphere:\n\nxf.graphs.vector_ray_plot(\n    xf.rand.norm_gaussian((25, 3,)),\n    _3d=True,\n    color=None,\n)\n\n\n                                                \n\n\nAnd so on into higher dimensions.\n\n\nOrthogonal matrices\nIf two vectors are both unit norm and orthogonal to one another, then we say that they are orthonormal.\nIf each of the vectors in a square matrix is orthonormal w.r.t. each of the others, we say that the matrix is orthogonal.\nAs such, we can see that:\n\neach orthogonal matrix is a rotation of the equivalent dimensionality basis vector matrix.\nthe columns / rows of each orthogonal matrix sit on the relevant dimensionality unit (hyper) sphere.\n\nFor instance, in two dimensions, each of the colored pairs of vectors in the below is just a rotation of the blue unit basis vectors (along the x, y axes), around our unit circle:\n\nxf.graphs.vector_ray_plot(\n    numpy.concatenate([\n        numpy.expand_dims(numpy.eye(2), 0),\n        xf.rand.orthogonal(2, shape = (5,))\n    ]),\n    color=\"g\",\n)\n\n\n                                                \n\n\nAnd, in three dimensions, each of the colored triplets of vectors in the below is just a rotation of the blue unit basis vectors (along the x, y, z axes), around our unit sphere:\n\nxf.graphs.vector_ray_plot(\n    numpy.concatenate([\n        numpy.expand_dims(numpy.eye(3), 0),\n        xf.rand.orthogonal(3, shape = (5,))\n    ]),\n    _3d=True,\n    color=\"g\",\n)\n\n\n                                                \n\n\nWe can thus interpret an orthogonal matrix as a rotation, mapping us from a space in which our axes point in the directions of our basis vectors, to a space in which our axes point in the directions of our matrix.\n\n\nMatrix multiplication\nA matrix multiplication is just lots of separate dot products.\nIn terms of the above, it is the size of the shadow cast by each row of the left matrix, on each column of the right matrix.\nI like to think of it as shooting particles in the directions and magnitudes of the rows of the left matrix, onto ‘deflectors’ with the orientation and magnitudes of the columns of the right matrix.\nFor instance, given a couple of random unit norm matrices:\n\nN = 2\nM1 = xf.rand.norm_gaussian((N, N,))\nM2 = xf.rand.norm_gaussian((N, N,))\ndef render_matrix(m):\n    return xf.rendering.render_df_color_range(\n        pandas.DataFrame(m),\n        v_min=-1.,\n        v_max=.1,\n    )\ndisplay(render_matrix(M1))\ndisplay(render_matrix(M2))\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n-00.1\n-01.0\n\n\n1\n0.79\n-0.62\n\n\n\n\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n0.82\n0.57\n\n\n1\n-01.0\n-0.09\n\n\n\n\n\nWe can plot out each of the separate dot product projections, akin to how we did above, as so:\n\ndef vector_projection(v1, v2):\n    dot = jax.numpy.dot(v1, v2)\n    return abs(dot) * v2, v2\n\nxf.graphs.vector_ray_plot(\n    [\n        [\n            numpy.stack([\n                M1[r],\n                *vector_projection(M1[r], M2[:, c]),\n            ])\n            for c in range(N)\n        ]\n        for r in range(N)\n    ],\n    share_x=True,\n    share_y=True,\n    markers=True,\n    color=None,\n)\n\n\n                                                \n\n\nBefore comparing the magnitude of the projections above (how far they are from the origin), to the cells of the below:\n\nrender_matrix(numpy.matmul(M1, M2))\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n0.91\n0.03\n\n\n1\n1.26\n0.5\n\n\n\n\n\n\n\nOrthogonality constraint\nGiven the definition of an orthogonal matrix, we can see that multiplication by its transpose will return the relevant size identity matrix:\n\neach of the pairs outside of the main diagonal are orthogonal, and so will return a dot product zero.\neach pair in the main diagonal will just be the dot product of a unit norm vector with itself, and so will return 1.\n\n\nM_orth = xf.rand.orthogonal(2)\nrender_matrix(numpy.matmul(M_orth, M_orth.T))\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n1.0\n-0.0\n\n\n1\n-0.0\n1.0\n\n\n\n\n\nAs such, we can derive a rough gauge for how orthogonal a given matrix is, as the mean squared error between the matmul of a matrix with its transpose, and the relevant size identity matrix:\n\ndef orthogonality_loss(X):\n    XXt = jax.numpy.matmul(X, X.T)\n    I = jax.numpy.eye(XXt.shape[0])\n    return jax.numpy.square(XXt - I).mean()\n\nWe can then ‘orthogonalise’ a matrix using gradient descent, by minisiming this loss function:\n\ndef orthogonalise(X):\n    solver = jaxopt.GradientDescent(fun=orthogonality_loss)\n    res = solver.run(X)\n    params, state = res\n    return params\n\nWhich we can use on each of our random matrices above, as so:\n\nM1_orth = orthogonalise(M1)\nM2_orth = orthogonalise(M2)\nxf.graphs.vector_ray_plot(\n    [\n        numpy.stack([M_orig, M_orth])\n        for M_orig, M_orth in zip([M1, M2], [M1_orth, M2_orth])\n    ],\n    markers=True,\n    color=\"g\",\n)\n\n\n                                                \n\n\nAs you can see, the non-orthogonal blue matrix has been morphed into the orthogonal red matrix.\nWe can verify their orthogonality by taking their matmul with their transpose:\n\ndisplay(render_matrix(numpy.matmul(M1_orth, M1_orth.T)))\ndisplay(render_matrix(numpy.matmul(M2_orth, M2_orth.T)))\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n1.0\n0.0\n\n\n1\n0.0\n1.0\n\n\n\n\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n1.0\n-0.0\n\n\n1\n-0.0\n1.0\n\n\n\n\n\nAnd confirming that we’re left with the relevant size identity matrix.\n\n\nRotation\nTurning back to the interpretation of an orthogonal matrix as a rotation, let’s now plot the transformation of a bunch of random points by each of our two orthogonal matrices.\n\ncloud = xf.rand.gaussian((10, 2,))\ndef plot_cloud(M_, T = False):\n    return xf.graphs.vector_ray_plot(\n        [\n            [\n                cloud, M_, numpy.matmul(cloud, M_),\n            ] + ([] if not T else [\n                M_.T,\n                numpy.matmul(numpy.matmul(cloud, M_), M_.T)\n            ])\n        ],\n        markers=True,\n        range_x=[-3., 3.],\n        range_y=[-3., 3.],\n    )\n\nFirst, M1 (where the left is our point cloud, the middle is our orthogonal rotation matrix, and the right are the same points, rotated):\n\nplot_cloud(M1_orth)\n\n\n                                                \n\n\nAnd then M2:\n\nplot_cloud(M2_orth)\n\n\n                                                \n\n\nAs one can see:\n\nthe magnitude of each member of our cloud has been preserved (they’re the same length relative to the origin)\nthe angles between each member of our cloud have been preserved (pick a pair of colors, and compare their angles in left vs right).\n\nConfirming that our orthogonal matrices represent a scale and angle preserving rotation.\n\n\nTranspose and Inverse\nAs we saw above, multiplying an orthogonal matrix by its transpose leaves us with the identity matrix.\nAs such, an orthogonal matrix’s tranpose must be equal to its inverse: as M M.T = I = M M-1.\nWe can confirm this by rotating our point cloud back, by multiplication with M.T:\n\nplot_cloud(M1_orth, T = True)\n\n\n                                                \n\n\nWe can find some intuition for this in terms of our interpretation of an orthogonal matrix, above, as a scale-preserving rotation.\nRoughly speaking:\n\nthe inverse of a matrix ‘reverses’ the linear transformation induced by multiplication by said matrix.\nthe transpose of a matrix reflects a rotation on the main diagonal, forming the ‘opposite’ reflection.\n\nThis ‘opposite’ reflection just reverses our original rotation - taking us from our point cloud on the left, back to itself on the right - and is hence our matrix inverse.\n\n\nNext steps\nOrthogonality is key to understanding PCA, and is how we derive the closed form expression for multi-variate linear regression."
  }
]