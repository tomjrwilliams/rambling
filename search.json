[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "rambling",
    "section": "",
    "text": "Wiggles be gone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2023\n\n\nTom Williams\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/walks/index.html",
    "href": "posts/walks/index.html",
    "title": "Wiggles be gone",
    "section": "",
    "text": "A few times recently, I’ve found myself in need of a differentiable loss function that can be used to smooth out a given time series.\nRather than doing the reading (hah!), this post is me playing around from first principles with a few different ways of specifying such a function."
  },
  {
    "objectID": "posts/walks/index.html#motivation",
    "href": "posts/walks/index.html#motivation",
    "title": "Wiggles be gone",
    "section": "Motivation",
    "text": "Motivation\nLet’s say that I’m fitting some kind of factor model via gradient descent, in which some high dimensional time series is decomposed into a lower dimensional time series of factor ‘paths’, and a function for mapping back up to the higher dimensional space.\nLet’s also assume that we intend the extracted factor paths to be meaningfully interpretible - presumably capturing some underlying structure that we’re interested in within the world.\nIn such a scenario, one reasonable constraint that we might want to impose is that the factors change value only gradually - for instance, that they don’t rapidly oscillate to and fro, or sharply jump one from level to another.\nGiven that we’re using gradient descent, to enforce this, we can simply include an additional term into our loss function: one that, when minimised, pushes a given time series towards the shape that we want.\n\n\nSetup\nimport sys\nsys.path.append(\"C:/rambling\")\n\nimport importlib\nimport functools\nimport torch\n\nfrom src import inspect\nfrom src import transforms\nfrom src import graphs\nfrom src import optim\nfrom src import tensors\nfrom src import constraints\nfrom src import walks\n\nimportlib.reload(graphs)\nimportlib.reload(transforms)\nimportlib.reload(inspect)\nimportlib.reload(optim)\nimportlib.reload(tensors)\nimportlib.reload(constraints)\nimportlib.reload(walks)\n\nDROPOUT = {}"
  },
  {
    "objectID": "posts/walks/index.html#problem-statement",
    "href": "posts/walks/index.html#problem-statement",
    "title": "Wiggles be gone",
    "section": "Problem Statement",
    "text": "Problem Statement\nThe first step is clarifying what we want our solution to a look like.\nRoughly speaking, a ‘smooth’ time series is one without abrupt changes in value, steadily moving in one direction or another.\nOne way to cheat, given such a definition, would be to flatten the time series completely.\nThis would, however, render our function useless as a constraint for the path of a factor (which we’re presumably expecting to vary over time).\nSo, we want to not just flatten out any sharp changes, but to also do so without distorting the underlying time series beyond recognition.\nTo be clear, we’re not quite looking for a function that can smooth a given fixed time series directly - like a moving average.\nRather, we’re looking for a function that can constrain a given varying time series to always look something like a moving average, when also operated on by some other fitting procedure (s)."
  },
  {
    "objectID": "posts/walks/index.html#methodology",
    "href": "posts/walks/index.html#methodology",
    "title": "Wiggles be gone",
    "section": "Methodology",
    "text": "Methodology\nOne simple way to test our solution, would thus be to:\n\nGenerate a set of test time series’, eg. random gaussian walks.\nRe-fit the time series’, by gradient descent on our loss function.\nPlot the before and after side by side, and evaluate by eye.\n\nObviously eyeballing the two lines is, in some sense, a little ‘unsystematic’.\nOn the other hand, the human eye can catch a lot of detail than summary statistics might miss.\nFurther, a metric that fully captured the quality of fit produced by our loss function, would essentially be just to define the very function we’re seeking."
  },
  {
    "objectID": "posts/walks/index.html#test-case",
    "href": "posts/walks/index.html#test-case",
    "title": "Wiggles be gone",
    "section": "Test case",
    "text": "Test case\nFor instance, let’s define a function to generate a random gaussian walk:\n\n\nGaussian walk\ninspect.render_source(tensors.gaussian_walk)\n\n\ndef gaussian_walk(shape, mu, std):\n    return torch.distributions.Normal(\n        mu, std\n    ).sample(shape).cumsum(dim=-1)\n\n\n\nWhich produces time series like the below:\n\n\nGaussian walk examples\nts = tensors.gaussian_walk([5, 100], 0., 1.)\ngraphs.render_graphs([\n    dict(\n        f=graphs.line_graph,\n        kwargs=dict(\n            xs=list(range(ts.shape[1])),\n            ys=ts[i].tolist(),\n            label=\"path\",\n        ),\n    )\n    for i in range(ts.shape[0])\n])\n\n\n\n\n\nWe can then feed this into a garden variety gradient descent optimiser together with our candidate loss function, before plotting:\n\nthe path of our loss function.\nthe original random walk.\nthe fitted result.\n\nWith the code here omitted for brevity - you can find it in /src/walks.py."
  },
  {
    "objectID": "posts/walks/index.html#rolling-mean-delta",
    "href": "posts/walks/index.html#rolling-mean-delta",
    "title": "Wiggles be gone",
    "section": "Rolling mean delta",
    "text": "Rolling mean delta\nThe above reference to a moving average naturally suggests our first candidate loss function: the average difference between the time series, and it’s own moving average.\n\n\nRolling mean delta loss\ninspect.render_source(constraints.smooth_ts_rolling_mean)\n\n\ndef smooth_ts_rolling_mean(ts, ns = [], dropout = 0.):\n    dropout = torch.nn.Dropout(p=dropout)\n    ts_rolling = torch.stack([\n        tensors.rolling_windows(ts, n)\n        for n in ns\n    ])\n    ts_rolling_mean = ts_rolling.mean(dim=-1)\n    return dropout(torch.sub(\n        ts_rolling_mean, ts\n    )).square().mean()\n\n\n\n\n\nRolling mean delta examples: n = 5\nwalks.plot_constrained_walks(\n    n=5,\n    f_walk = functools.partial(\n        tensors.gaussian_walk, [100], 0., 1.\n    ),\n    f_loss = functools.partial(\n        constraints.smooth_ts_rolling_mean,\n        ns=[5],\n    ),\n)\n\n\n\n\n\nGenerally, the shape and location of the input series seem to be mostly conserved, though the scale might be being compressed slightly.\nThe choice of window size gives us a toggle for controlling how aggressively we want to smooth.\nFor instance, compare rolling windows of length ten (double the above):\n\n\nRolling mean delta examples: n = 10\nwalks.plot_constrained_walks(\n    n=5,\n    f_walk = functools.partial(\n        tensors.gaussian_walk, [100], 0., 1.\n    ),\n    f_loss = functools.partial(\n        constraints.smooth_ts_rolling_mean,\n        ns=[10],\n    ),\n)\n\n\n\n\n\nVersus length two:\n\n\nRolling mean delta examples: n = 2\nwalks.plot_constrained_walks(\n    n=5,\n    f_walk = functools.partial(\n        tensors.gaussian_walk, [100], 0., 1.\n    ),\n    f_loss = functools.partial(\n        constraints.smooth_ts_rolling_mean,\n        ns=[2],\n    ),\n)\n\n\n\n\n\nOverall, this seems like a pretty reasonable first pass."
  },
  {
    "objectID": "posts/walks/index.html#coast-line",
    "href": "posts/walks/index.html#coast-line",
    "title": "Wiggles be gone",
    "section": "Coast line",
    "text": "Coast line\nAnother route might be to focus in on the periods of oscillation in the random walk.\nIn such periods, the gross length of the walk is much larger than the net length (whereas the two would be much closer on a ‘smooth’ walk).\nWe might then construct a loss function as the difference between the two:\n\n\nCoast line loss\ninspect.render_source(constraints.smooth_ts_rolling_coastline)\n\n\ndef smooth_ts_rolling_coastline(ts, ns = [], dropout = 0.):\n    dropout = torch.nn.Dropout(p=dropout)\n    ts_deltas = tensors.calc_deltas(ts)\n    ts_rolling = torch.stack([\n        tensors.pad(\n            tensors.rolling_windows(ts_deltas, n),\n            max(ns) - n,\n            value = 0\n        )\n        for n in ns\n    ])\n    return dropout(\n        torch.sub(\n            ts_rolling.abs().sum(dim=-1),\n            ts_rolling.sum(dim=-1).abs(),\n        )\n    ).mean()\n\n\n\nWhich we can test as before:\n\n\nCoast line examples: n = 5\nwalks.plot_constrained_walks(\n    n=5,\n    f_walk = functools.partial(\n        tensors.gaussian_walk, [100], 0., 1.\n    ),\n    f_loss = functools.partial(\n        constraints.smooth_ts_rolling_coastline,\n        ns=[5],\n    ),\n)\n\n\n\n\n\nThis also gives us pretty reasonable results. The overall shape and location seem to be mostly conserved, though the scale is arguably being compressed slightly more.\nOne key difference is that turning points are being flattened out - so we get something a little closer to a smoothed step function.\nWe can see this a little more clearly with a longer window length:\n\n\nCoast line examples: n = 20\nwalks.plot_constrained_walks(\n    n=5,\n    f_walk = functools.partial(\n        tensors.gaussian_walk, [100], 0., 1.\n    ),\n    f_loss = functools.partial(\n        constraints.smooth_ts_rolling_coastline,\n        ns=[20],\n    ),\n)\n\n\n\n\n\nThough we’d be unlikely to use a window this wide in practise, given the distorting effect it has on the underlying walk’s shape."
  },
  {
    "objectID": "posts/walks/index.html#second-derivative",
    "href": "posts/walks/index.html#second-derivative",
    "title": "Wiggles be gone",
    "section": "Second derivative",
    "text": "Second derivative\nOne final option I considered was simply to push the second derivative of the series towards zero.\n\n\nSecond derivative loss\ninspect.render_source(constraints.smooth_ts_2nd_derivative)\n\n\ndef smooth_ts_2nd_derivative(ts, dropout = 0.):\n    dropout = torch.nn.Dropout(p=dropout)\n    ts_deltas = tensors.calc_deltas(ts)\n    ts_deltas_deltas = tensors.calc_deltas(ts_deltas)\n    return dropout(ts_deltas_deltas).square().mean()\n\n\n\nWhich was unreasonably effective, given its (almost brutal) simplicity:\n\n\nSecond derivative examples\nwalks.plot_constrained_walks(\n    n=5,\n    f_walk = functools.partial(\n        tensors.gaussian_walk, [100], 0., 1.\n    ),\n    f_loss = constraints.smooth_ts_2nd_derivative\n)\n\n\n\n\n\nThough with the clear drawback of lacking any kind of parametrisation for how aggressively we want to smooth."
  },
  {
    "objectID": "posts/walks/index.html#conclusion",
    "href": "posts/walks/index.html#conclusion",
    "title": "Wiggles be gone",
    "section": "Conclusion",
    "text": "Conclusion\nEither of the first and third seem reasonable solutions to the problem as posed.\nThe second is interesting, though probably only relevant in cases where the underlying factor is assumed to move in such a step-like manner.\nThe parametrisability of the first is obviously a plus - though the third arguably balances smoothness and shape preservation slightly better.\nI think, unless I wanted to deliberately enforce a slower or faster velocity to the factor path, I would probably pick the third - but as this is based pretty much just on aesthetics, I’d say each to their own.\nI’ll have a go in a future post, at actually using each of the above to constrain the results of some other fitting procedure."
  }
]