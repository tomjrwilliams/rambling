[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "rambling",
    "section": "",
    "text": "Intuition: PCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\nTom Williams\n\n\n\n\n\n\n  \n\n\n\n\nIntuition: Orthogonality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nTom Williams\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/orthogonality/index.html",
    "href": "posts/orthogonality/index.html",
    "title": "Intuition: Orthogonality",
    "section": "",
    "text": "This post is a collection of little visualisations intended to help one build up intuition for the notions of vector and matrix orthogonality.\n\nimport operator\nimport numpy\nimport pandas\nimport jax\nimport jaxopt\nimport latexify\n\nimport xtuples as xt\nimport xfactors as xf\n\n\nOrthogonal vectors\nTwo vectors are orthogonal to one another if their dot product is zero.\n\n\n\\[ \\displaystyle \\mathrm{dot}(x1, x2) = \\sum \\left({\\left[ {x1_{i}} {x2_{i}} \\mid i \\in \\mathrm{range}\\left(\\mathrm{len}\\left(x1\\right)\\right) \\right]}\\right) \\]\n\n\nThe dot product of two vectors gives the magnitude of the projection of one onto the other.\nVisually, this represents the length of the shadow that one vector casts on the other (if both are drawn as rays from the origin).\n\ndef unit_vector_projection(v, unit_v):\n    dot = jax.numpy.dot(v, unit_v)\n    return dot * unit_v, unit_v * jax.numpy.sign(v)\n\nTo visualise this, we can plot the dot product of a set of random unit magnitude 2d vectors, with the 2d basis vectors of the same component signs:\n\ndef unit_vector_plot(rows, cols):\n    unit_v = jax.numpy.eye(2)\n    return xf.graphs.vector_ray_plot(\n        xt.iTuple.range(rows)\n        .map(lambda _: xt.iTuple.range(cols).map(\n            lambda _: (\n                lambda v: jax.numpy.stack([\n                    v,\n                    *unit_vector_projection(v, unit_v[0]),\n                    *unit_vector_projection(v, unit_v[1]),\n                ])\n            )(xf.rand.norm_gaussian((2,)))\n        )),\n        markers=True,\n        color=None,\n    )\n\nWhere the marker part-way along each basis vector (pointing along the x or y axis), is placed at the dot product of said basis vector with the respective random unit vector:\n\nunit_vector_plot(3, 3)\n\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n\n                                                \n\n\nAs one can see, given their magnitudes, the larger the dot product of two vectors, the closer they are to pointing in the same direction.\nConversely, the smaller their dot product, the closer they are to perpendicular.\nWe can make this more explicit by noting that the dot product is equal to the product of the magnitude of the two vectors and the cosine of the angle between them.\nAs such, a zero dot product indicates that the angle between the two vectors is pi / 2 radians (or 90 degrees) - ie. that they’re perpendicular to one another.\nSimilarly, as cos(0) = 1, the dot product of a vector with itself is simply equal to it’s squared magnitude.\n\n\nUnit sphere\nThe set of all 2D vectors of unit magnitude forms a circle:\n\nxf.graphs.vector_ray_plot(\n    xf.rand.norm_gaussian((25, 2,)),\n    color=None,\n)\n\n\n                                                \n\n\nThe set of all 3D unit vectors forms a sphere:\n\nxf.graphs.vector_ray_plot(\n    xf.rand.norm_gaussian((25, 3,)),\n    _3d=True,\n    color=None,\n)\n\n\n                                                \n\n\nAnd so on into higher dimensions.\n\n\nOrthogonal matrices\nIf two vectors are both unit norm and orthogonal to one another, then we say that they are orthonormal.\nIf each of the vectors in a square matrix is orthonormal w.r.t. each of the others, we say that the matrix is orthogonal.\nAs such, we can see that:\n\neach orthogonal matrix is a rotation of the equivalent dimensionality basis vector matrix.\nthe columns / rows of each orthogonal matrix sit on the relevant dimensionality unit (hyper) sphere.\n\nFor instance, in two dimensions, each of the colored pairs of vectors in the below is just a rotation of the blue unit basis vectors (along the x, y axes), around our unit circle:\n\nxf.graphs.vector_ray_plot(\n    numpy.concatenate([\n        numpy.expand_dims(numpy.eye(2), 0),\n        xf.rand.orthogonal(2, shape = (5,))\n    ]),\n    color=\"g\",\n)\n\n\n                                                \n\n\nAnd, in three dimensions, each of the colored triplets of vectors in the below is just a rotation of the blue unit basis vectors (along the x, y, z axes), around our unit sphere:\n\nxf.graphs.vector_ray_plot(\n    numpy.concatenate([\n        numpy.expand_dims(numpy.eye(3), 0),\n        xf.rand.orthogonal(3, shape = (5,))\n    ]),\n    _3d=True,\n    color=\"g\",\n)\n\n\n                                                \n\n\nWe can thus interpret an orthogonal matrix as a rotation, mapping us from a space in which our axes point in the directions of our basis vectors, to a space in which our axes point in the directions of our matrix.\n\n\nMatrix multiplication\nA matrix multiplication is just lots of separate dot products.\nIn terms of the above, it is the size of the shadow cast by each row of the left matrix, on each column of the right matrix.\nI like to think of it as shooting particles in the directions and magnitudes of the rows of the left matrix, onto ‘deflectors’ with the orientation and magnitudes of the columns of the right matrix.\nFor instance, given a couple of random unit norm matrices:\n\nN = 2\nM1 = xf.rand.norm_gaussian((N, N,))\nM2 = xf.rand.norm_gaussian((N, N,))\ndef render_matrix(m):\n    return xf.rendering.render_df_color_range(\n        pandas.DataFrame(m),\n        v_min=-1.,\n        v_max=.1,\n    )\ndisplay(render_matrix(M1))\ndisplay(render_matrix(M2))\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n-00.1\n-01.0\n\n\n1\n0.79\n-0.62\n\n\n\n\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n0.82\n0.57\n\n\n1\n-01.0\n-0.09\n\n\n\n\n\nWe can plot out each of the separate dot product projections, akin to how we did above, as so:\n\ndef vector_projection(v1, v2):\n    dot = jax.numpy.dot(v1, v2)\n    return abs(dot) * v2, v2\n\nxf.graphs.vector_ray_plot(\n    [\n        [\n            numpy.stack([\n                M1[r],\n                *vector_projection(M1[r], M2[:, c]),\n            ])\n            for c in range(N)\n        ]\n        for r in range(N)\n    ],\n    share_x=True,\n    share_y=True,\n    markers=True,\n    color=None,\n)\n\n\n                                                \n\n\nBefore comparing the magnitude of the projections above (how far they are from the origin), to the cells of the below:\n\nrender_matrix(numpy.matmul(M1, M2))\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n0.91\n0.03\n\n\n1\n1.26\n0.5\n\n\n\n\n\nConfirming that each cell of the matrix multiplication is just the magnitude of the projection of the relevant row / column of our input matrices.\n\n\nOrthogonality constraint\nGiven the definition of an orthogonal matrix, we can see that multiplication by its transpose will return the relevant size identity matrix:\n\neach of the pairs outside of the main diagonal are orthogonal, and so will return a dot product zero.\neach pair in the main diagonal will just be the dot product of a unit norm vector with itself: ie. 1.\n\n\nM_orth = xf.rand.orthogonal(2)\nrender_matrix(numpy.matmul(M_orth, M_orth.T))\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n1.0\n-0.0\n\n\n1\n-0.0\n1.0\n\n\n\n\n\nAs such, we can construct a rough measure for how orthogonal a given matrix is: the mean squared error between the matmul of a matrix with its transpose, and the relevant size identity matrix.\n\ndef orthogonality_loss(X):\n    XXt = jax.numpy.matmul(X, X.T)\n    I = jax.numpy.eye(XXt.shape[0])\n    return jax.numpy.square(XXt - I).mean()\n\nWe can then ‘orthogonalise’ a matrix using gradient descent, by minisiming this loss function:\n\ndef orthogonalise(X):\n    solver = jaxopt.GradientDescent(fun=orthogonality_loss)\n    res = solver.run(X)\n    params, state = res\n    return params\n\nWhich we can use on each of our random matrices above, as so:\n\nM1_orth = orthogonalise(M1)\nM2_orth = orthogonalise(M2)\nxf.graphs.vector_ray_plot(\n    [\n        numpy.stack([M_orig, M_orth])\n        for M_orig, M_orth in zip([M1, M2], [M1_orth, M2_orth])\n    ],\n    markers=True,\n    color=\"g\",\n)\n\n\n                                                \n\n\nAs you can see, the non-orthogonal blue matrix has been morphed into the orthogonal red matrix.\nWe can verify their orthogonality by taking their matmul with their transpose:\n\ndisplay(render_matrix(numpy.matmul(M1_orth, M1_orth.T)))\ndisplay(render_matrix(numpy.matmul(M2_orth, M2_orth.T)))\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n1.0\n0.0\n\n\n1\n0.0\n1.0\n\n\n\n\n\n\n\n\n\n\n \n0\n1\n\n\n\n\n0\n1.0\n-0.0\n\n\n1\n-0.0\n1.0\n\n\n\n\n\nAnd confirming that we’re left with the relevant size identity matrix.\n\n\nRotation\nTurning back to the interpretation of an orthogonal matrix as a rotation, let’s now plot the transformation of a bunch of random points by each of our two orthogonal matrices.\n\ncloud = xf.rand.gaussian((10, 2,))\ndef plot_cloud(M_, T = False):\n    return xf.graphs.vector_ray_plot(\n        [\n            [\n                cloud, M_, numpy.matmul(cloud, M_),\n            ] + ([] if not T else [\n                M_.T,\n                numpy.matmul(numpy.matmul(cloud, M_), M_.T)\n            ])\n        ],\n        markers=True,\n        range_x=[-3., 3.],\n        range_y=[-3., 3.],\n    )\n\nWhere the left graph is our point cloud, the middle is our orthogonal rotation matrix, and the right is the point cloud after rotation by said matrix:\n\nplot_cloud(M1_orth)\n\n\n                                                \n\n\n\nplot_cloud(M2_orth)\n\n\n                                                \n\n\nAs one can see:\n\nthe magnitude of each member of our cloud has been preserved (they’re the same length relative to the origin)\nthe angles between each member of our cloud have been preserved (pick a pair of colors, and compare their angles in left vs right).\n\nConfirming that (our) orthogonal matrices represent a scale and angle preserving rotation.\n\n\nTranspose and Inverse\nAs we saw above, multiplying an orthogonal matrix by its transpose leaves us with the identity matrix.\nAs such, an orthogonal matrix’s tranpose must be equal to its inverse: as M M.T = I = M M-1.\nWe can confirm this by rotating our point cloud back, by multiplication with M.T:\n\nplot_cloud(M1_orth, T = True)\n\n\n                                                \n\n\nWe can find some intuition for this in terms of our interpretation of an orthogonal matrix, above, as a scale-preserving rotation.\nRoughly speaking:\n\nthe inverse of a matrix ‘reverses’ the linear transformation induced by multiplication by said matrix.\nthe transpose of a matrix reflects a rotation on the main diagonal, forming the ‘opposite’ reflection.\n\nThis ‘opposite’ reflection just reverses our original rotation - taking us from our point cloud on the left, back to itself on the right - and is hence our matrix inverse.\n\n\nNext steps\nOrthogonality is key to understanding PCA, and is how we derive the closed form expression for multi-variate linear regression."
  },
  {
    "objectID": "posts/pca/index.html",
    "href": "posts/pca/index.html",
    "title": "Intuition: PCA",
    "section": "",
    "text": "This is a reference post on PCA in which we walk through a toy example on random data (with lots of graphs), to help one build intuition for the linear algebra involved.\n\nimport operator\nimport numpy\nimport pandas\nimport jax\nimport jaxopt\n\nimport xtuples as xt\nimport xfactors as xf\n\n\nGenerative model\nWe’ll start by defining the generative model assumed by PCA, assuming a 3D latent space, a 4D feature space, and 100 example data points:\n\nN_FACTORS = 3\nFACTORS = list(range(N_FACTORS))\n\nN_SAMPLES = 100\nSAMPLES = list(range(N_SAMPLES))\n\nN_FEATURES = 4\nFEATURES = list(range(N_FEATURES))\n\n\nFactors\nFirst, we’ll generate some random, diagonal covariance, gaussian noise, that we’ll refer to as our ‘factors’:\n\nfactors = xf.rand.gaussian((N_SAMPLES, N_FACTORS))\nfactors_df = pandas.DataFrame(\n    factors, index=SAMPLES, columns=FACTORS,\n)\n\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nAs we can see, the sample covariance is (roughly), unit diagonal:\n\nxf.rendering.render_df_color_range(factors_df.cov())\n\n\n\n\n\n\n \n0\n1\n2\n\n\n\n\n0\n0.98\n0.04\n-0.01\n\n\n1\n0.04\n1.53\n-0.01\n\n\n2\n-0.01\n-0.01\n0.91\n\n\n\n\n\nThe covariance between two variables increases (decreases) the more one variable is above (below) it’s mean, conditional on the other variable also being above (below) it’s mean.\nAs such, if we plot our individual factors against one another in 2D, we can see their zero-covariance in the spherical shape of the resulting densities:\n\nxf.graphs.df_density_pair_chart(factors_df, key=\"factor\")\n\n\n                                                \n\n\nSimply follow either the x or y axis, and note that the conditional average of the other axis (the other variable) remains constant: implying (roughly) zero covariance.\nIf we plot the 1D density, we can see that each factor also has (roughly) the same unit variance gaussian distribution:\n\nxf.graphs.df_line_chart(\n    xf.densities.gaussian_kde_1d_df(\n        {\n            i: factors[:, i]\n            for i in range(N_FACTORS)\n        },\n    ),\n    x=\"position\",\n    y=\"density\",\n    color=\"key\",\n)\n\n\n                                                \n\n\n\n\nFeatures\nGiven this, we’ll now generate another random gaussian weight matrix:\n\nweights = xf.rand.gaussian((N_FACTORS, N_FEATURES))\n\nWhich we can render as so:\n\nweights_df = pandas.DataFrame(\n    weights, columns=FEATURES, index=FACTORS\n)\nxf.rendering.render_df_color_range(weights_df)\n\n\n\n\n\n\n \n0\n1\n2\n3\n\n\n\n\n0\n0.81\n-2.98\n0.19\n0.72\n\n\n1\n-01.0\n0.97\n0.89\n0.89\n\n\n2\n-0.01\n-1.45\n1.15\n0.61\n\n\n\n\n\nAnd which we’ll then multiply with our factors, to get some features:\n\nfeatures = factors @ weights\nfeatures_df = pandas.DataFrame(\n    features, columns=FEATURES, index=SAMPLES\n)\n\nAs one can see in the resulting 2D density plot, we’ve now added some linear dependence to our dataset:\n\nfor chart in xf.graphs.df_density_pair_chart(\n    features_df, key=\"feature\", separate=True\n):\n    display(chart)\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nWhich we can confirm by calculating our feature (sample) covariance matrix:\n\nxf.rendering.render_df_color_range(features_df.cov())\n\n\n\n\n\n\n \n0\n1\n2\n3\n\n\n\n\n0\n2.12\n-3.69\n-01.2\n-0.79\n\n\n1\n-3.69\n11.7\n-0.79\n-1.62\n\n\n2\n-01.2\n-0.79\n2.42\n1.97\n\n\n3\n-0.79\n-1.62\n1.97\n2.08\n\n\n\n\n\n\n\nProblem statement\nGiven our feature matrix, and assuming the distribution of our factors (but not the values), our goal is to impute sensible values for both the weights and the factors.\n\n\n\nRotation and Scaling\nVisually, we can separate out the distributional changes affected by our weight matrix into two steps:\n\na rescaling\na rotation\n\nSo, to reverse our weight matrix (taking us from features back to factors), we presumably need to:\n\nun-rotate our features, so that - in terms of the 2D densities above - they ‘sit’ back on the original x, y axes.\nde-scale them back to unit variance.\n\nTo visualise this, we can cheat slightly and, assuming the methodology we’re in the process of deriving, de-rotate those 2D densities from above:\n\n\nDe-Rotate\ndef threshold(df, k, quantile):\n    return df[df[k] &gt; numpy.quantile(df[k], quantile)]\n\ndef de_rotate(df):\n    df = xf.graphs.f_df_density_pair_df(\n        FEATURES, \"pair\",\n    )(df)\n    z = \"density\"\n    dfs = {\n        pair: df[df[\"pair\"] == pair]\n        for pair in df[\"pair\"].unique()\n    }\n    dfs_threshold = {\n        pair: threshold(_df, z, .7)\n        for pair, _df in dfs.items()\n    }\n    ws = {\n        pair: jax.numpy.linalg.eig(\n            jax.numpy.cov(numpy.stack([\n                _df[\"x\"].values, _df[\"y\"].values\n            ]))\n        )[1].real\n        for pair, _df in dfs_threshold.items()\n    }\n    return [\n        pandas.DataFrame(\n            numpy.matmul(\n                _df[[\"x\", \"y\"]].values, ws[pair]\n            ),\n            columns=[\"x\", \"y\"],\n            index=_df.index,\n        ).assign(density=numpy.exp(_df[z]), pair=_df[\"pair\"])\n        for pair, _df in dfs.items()\n    ]\n\n\nWhich we’ll plot with separate color scales, so you can see more clearly how the density is now aligned with the conventional x, y axes:\n\nfor derotated_df in de_rotate(features_df):\n    display(xf.graphs.df_scatter_chart(\n        derotated_df,\n        x=\"x\",\n        y=\"y\",\n        color=\"density\",\n        height=400,\n        width=600,\n    ))\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nAs one can see, where the distributions are non-spherical, they now ‘point’ out along the conventional x, y axes.\nWhere they were roughly spherical to begin with, the rotation hasn’t actually changed the (bi-variate) distribution, as a sphere is invariant under rotation.\nThen, to get back to our factors, we simply have to re-scale the de-rotated distributions above, back to unit variance.\n\nSolution\nLinear transformations are composable, which means that we can separate out these two steps into separate matrices, before multiplying them back together (to get a single embedding matrix).\nSo, to go from features to factors, we’re looking for two matrices:\n\none to de-rotate our features back into factors.\none to de-scale our factors back to unit variance.\n\nA scaling matrix just stretches each dimension individually, so we can presume that to be (non-unit) diagonal.\nAssuming we use an orthogonal rotation matrix, we can use the same matrix to rotate back and forth between our features and our factors.\nAs such, we’re looking for:\n\nan orthogonal rotation matrix\na diagonal scaling matrix.\n\n\n\n\nCovariance\nCovariance under a linear transformation is defined as so:\n\ndef linear_cov(W, X):\n    return W @ numpy.cov(X) @ W.T\n\nAs we saw above, we’re going to assume that we can model our weight matrix as the product of an orthogonal rotation matrix, and a diagonal scaling matrix.\nAs such, we can write our covariance function (for our features), as so:\n\ndef feature_cov(Orthog, Diag, X):\n    return (Orthog @ Diag) @ numpy.cov(X) @ (Orthog @ Diag).T\n\nWhich we can simplify to:\n\ndef feature_cov(Orthog, Diag, X):\n    return Orthog @ numpy.cov(X) @ (Diag ** 2) @ Orthog.T\n\nThis, intuitively, makes sense:\n\nFirst we multiply our factor covariance by the square of our scaling matrix, as (co) variance is a quadratic dispersion measure.\nThen, we multiply through by our rotation matrix, twice: one to capture any change in scale from the projection itself, and then once to actually apply the rotation in question.\n\n\n\nEigen vectors\nGiven our assumption that the factors have diagonal covariance, we can make one further simplication to the above:\n\ndef feature_cov(Orthog, Diag, X):\n    return Orthog @ numpy.diag(numpy.cov(X)) @ (Diag ** 2) @ Orthog.T\n\nIf we right multiply through both sides by our rotation matrix:\n\ndef rotated_feature_cov(Orthog, Diag, X, features):\n    return numpy.cov(features) @ Orthog == (\n        Orthog @ numpy.diag(numpy.cov(X)) @ (Diag ** 2)\n        @ Orthog.T @ Orthog\n    )\n\nThe Orthog.T @ Orthog term (on the far right) cancels to I.\n\ndef rotated_feature_cov(Orthog, Diag, X, features):\n    return numpy.cov(features) @ Orthog == (\n        Orthog @ numpy.diag(numpy.cov(X)) @ (Diag ** 2)\n    )\n\nAs such, we can see that multiplication of our feature covariance by each of the vectors in our rotation matrix simply returns a scaled version of said vector, with the scaling term given by the relevant diagonal element in diag(cov(X)) @ (Diag ** 2).\nThis, in turn, then tells us that our rotation matrix is in fact comprised of eigenvectors of our feature covariance matrix.\nGiven that our factor distributions were assumed to have unit variance, we can then also see that the diagonal scaling matrix we’re looking for is simply comprised of the square root of the respective eigenvalues.\n\n\nSolution\nAs such, to find our matrices Orthog and Diag we simply need to do an eigen-decomposition of our feature covariance matrix, which we can do as so:\n\neigvals, eigvecs = jax.numpy.linalg.eig(\n    jax.numpy.cov(features.T)\n)\nscale = jax.numpy.sqrt(eigvals.real)\northog = eigvecs.real\n\nAs one can see, we’re only left with 3 (roughly) non-zero eigenvalues:\n\nnumpy.round(eigvals.real, 2)\n\nArray([13.139999  ,  4.94      ,  0.        ,  0.22999999], dtype=float32)\n\n\nThese capture the degree of variation in the feature space, explained by each component (how ‘spread’ out the features are along the axis given by the relevant eigenvector).\nAs such, we can then first sort our matrices by their eigenvalue, before disregarding the smallest.\nHere, we’re assuming we know how many latent factors there were (so we keep only the first N_FACTORS components), but in a more general problem, we could infer this number by the count of non (nearly) zero eigenvalues.\n\norder = jax.numpy.flip(jax.numpy.argsort(eigvals))\nscale = scale[order][:N_FACTORS]\northog = orthog[:, order[:N_FACTORS]]\n\nWe can de-rotate our features into factor space, by multiplication with our orthogonal rotation matrix, as so:\n\nfactors_scaled = features @ orthog\nxf.graphs.df_density_pair_chart(\n    pandas.DataFrame(\n        factors_scaled,\n        columns=FACTORS,\n        index=SAMPLES,\n    ), \n    key=\"factor\"\n)\n\n\n                                                \n\n\nBefore de-scaling them, leaving us with our original unit variance noise, by dividing by the sqrt of the eigenvalues:\n\nfactors_descaled = numpy.divide(\n    factors_scaled,\n    xf.expand_dims(scale, 0, 1)\n)\nxf.graphs.df_density_pair_chart(\n    pandas.DataFrame(\n        factors_descaled,\n        columns=FACTORS,\n        index=SAMPLES,\n    ), \n    key=\"factor\"\n)\n\n\n                                                \n\n\nTo go back up to our features, we can simply re-scale up the factors, and then rotate back into feature space:\n\nfeatures_roundtrip = orthog @ numpy.multiply(\n    factors_descaled,\n    xf.expand_dims(scale, 0, 1)\n).T\nfor chart in xf.graphs.df_density_pair_chart(\n    pandas.DataFrame(\n        features_roundtrip.T,\n        columns=FEATURES,\n        index=SAMPLES,\n    ), \n    key=\"feature\",\n    separate=True,\n):\n    display(chart)\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nWhich, as you can see, returns us back to where we started.\n\n\nDicussion\nOur rotation matrix, above, allows us to map our feature space into the (presumably) much smaller (implied) factor space, whilst nonetheless retaining most of the ‘information’ of the original features.\nThis helps us combat the ‘curse of dimensionality’ (the tendency for distance measures to explode as the number of dimensions increases), as we can train downstream models using the smaller dimensional (implied) factors, rather than the (potentially much) larger dimensional features.\nThe embedding matrix itself also encodes useful information about the correlation structure of our features, which can often be interpreted directly (for instance, see the below two posts analysing single name equity returns and bond / swap yield curves, respectively).\nPCA also naturally lends itself to a number of different extensions - particularly once we move from a strict eigen-decomposition, as above, onto a more general gradient based optimisation approach, such as in this post on ppca.\n\n\nNext steps\nGiven this, we’ll next turn to some practical applications of PCA in financial markets:\n\nHere on daily single-name equity returns.\nHere on daily per-tenor bond and swap curve yields."
  }
]