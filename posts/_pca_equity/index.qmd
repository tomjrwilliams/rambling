---
title: "Example: PCA, Equity Returns"
author: "Tom Williams"
date: "2023-07-17"
categories: []
draft: true
execute:
  daemon: false
---

I'm currently working on open-sourcing some of the factor decomposition utilities that I use in my day-job.

This post is a first simple example of how to use said utilities to do a PCA of some close-to-close equity returns.

## Setup

```{python}
#| code-fold: true
#| code-summary: "Auto reload"
%load_ext autoreload
%autoreload 2
```

```{python}
#| code-fold: true
#| code-summary: "Environment"
import os
import sys
import importlib
sys.path.append("C:/hc/src")
sys.path.append("C:/hc/hc-core/src")
sys.path.append("C:/hc/xfactors/src/xfactors")
sys.path.append("C:/hc/xtuples/src/xtuples")
os.environ["MODULE"] = "c:/hc/src/"
```

```{python}
#| code-fold: true
#| code-summary: "Imports"
import datetime
import functools
import itertools

import numpy
import pandas
import jax
import jax.numpy

import xtuples
import xfactors

import hc_core.imports as imports
import hc_core.rendering as rendering
import hc_core.dfs as dfs
import hc_core.densities as densities
import hc_core.graphs as graphs
import hc_core.dates as dates

import bt.data.prices.int
import bt.algos.universe.int

# graphs.set_rendering(graphs.HTML)
graphs.set_rendering("NULL")
```

## Data

We'll work on daily close to close total returns, on a universe of major european equity indices:

```{python}
#| code-fold: false
#| code-summary: "Data"
df_returns = bt.data.prices.int.returns_df(
    dates.y(2005),
    dates.y(2023, m=4),
    indices=bt.algos.universe.configs.INDICES,
) 
```

The returns are from bloomberg including all cash adjustments (even if in rpodcution we actually have a separate internal ticker for dividend stream from a given name).

Vanilla pca doesn't accomodate missing data, so given a particular date range of interest, we'll filter to only those tickers over a certain proportion within relevant equity indices.

First we'll load the relevant index membership mapping tables:

```{python}
#| code-fold: false
#| code-summary: "Data"
dfs_indices = bt.algos.universe.int.rolling_indices(
    dates.y(2005),
    dates.y(2023, m=4),
    indices=bt.algos.universe.configs.INDICES,
)
df_universe = bt.algos.universe.int.index_union(dfs_indices)
```

With which we'll only include those oer a certain proportion of index membership in our date range:

```{python}
#| code-fold: false
#| code-summary: "Universe"
equity.in_universe
```

As so:

```{python}
#| code-fold: false
#| code-summary: "Universe"
def get_returns( d_start, d_end, threshold=1.):
    tickers = xtuples.iTuple(df_returns.columns).filter(
        equity.in_universe,
        df=dfs.index_date_filter(df_universe, d_start, d_end),
        threshold=threshold,
    ).pipe(list) 
    return dfs.index_date_filter(
        df_returns, d_start, d_end
    )[tickers].dropna()
```

For instance, looking at 2022:

```{python}
#| code-fold: false
#| code-summary: "PCA"
get_returns(dates.y(2022), dates.y(2023)).columns
```

We'll also loads a GICS sector mapping as this will be useful later:

```{python}
#| code-fold: false
#| code-summary: "Data"
dfs_sectors = bt.algos.universe.int.rolling_indices(
    dates.y(2005),
    dates.y(2023, m=4),
    sectors=bt.algos.universe.configs.GICS_SECTORS,
)
```

## Model

PCA takes a matrix of (n_tickers, n_dates)

Returns

Weight matrix of (n_tickers, n_factors)
Eigenvalues (n_factors)
Factors (n_factors, n_dates)

Where it does this by eigendecomposition of the covariance matrix of the input data, as so:

```{python}
#| code-fold: false
#| code-summary: "PCA"
rendering.render_source(xfactors.PCA.fit, cls_method=True)
```

Which xfactors can do as follows (wrapped with an lru cache so we can dynamically call for the pca given dates, but without recalc each time):

```{python}
#| code-fold: false
#| code-summary: "PCA"
@functools.lru_cache(maxsize=10)
def fit_pca(d_start, d_end, n):
    df = get_returns(d_start, d_end)
    return xfactors.PCA.fit(df, n = n), df
_ = fit_pca(dates.y(2022), dates.y(2023), n=3)
```

## Results


```{python}
#| code-fold: false
#| code-summary: ""
def pca_factor_cov(d_start, d_end, n):
    pca, df = fit_pca(d_start, d_end, n)
    factors = pca.encode(df)
    print(numpy.cov(factors.T))
    return rendering.render_df_color_range(
        pandas.DataFrame(
            factors,
            # columns=list(range(3)),
            # index=list(range(3)),
        ).corr(),
        v_min=-1.,
        v_max=.1,
    )
pca_factor_cov(dates.y(2022), dates.y(2023), n=3)
```

And the factor density:

```{python}
#| code-fold: false
#| code-summary: ""
def pca_factor_density(d_start, d_end, n):
    pca, df = fit_pca(d_start, d_end, n)
    factors = pca.encode(df)
    return densities.gaussian_kde_1d_df(
        {
            i: factors.real[..., i]
            for i in range(n)
        }, 
        clip_quantile=.01
    )
```

We can see that they're (roughly) gaussian distributed:

```{python}
#| code-fold: false
#| code-summary: ""
def pca_factor_density_chart(d_start, d_end, n):
    return graphs.df_chart(
        pca_factor_density(d_start, d_end, n),
        x="position",
        y="density",
        color="key",
        title="Factor Density (0.01 Clip)"
    )
pca_factor_density_chart(dates.y(2022), dates.y(2023), n=3)
```

Plotting the resulting factor weights:

```{python}
#| code-fold: false
#| code-summary: ""
def pca_weights(d_start, d_end, n):
    pca, _ = fit_pca(d_start, d_end, n)
    return pca.weights_df()
pca_weights(dates.y(2022), dates.y(2023), n=3)
```

^^ summary not the whole df

Given a helper function for plotting bars:

```{python}
#| code-fold: false
#| code-summary: ""
def factor_bar_chart(weights):
    n = len(weights.index)
    return graphs.df_facet_bar_chart(
        dfs.melt_with_index(weights, index_as="factor"),
        x="variable",
        y="value",
        facet="factor",
    )
```

We cna plot the weihts:

```{python}
#| code-fold: false
#| code-summary: ""
def pca_weights_chart(d_start, d_end, n):
    weights = pca_weights(d_start, d_end, n)
    return factor_bar_chart(weights)
pca_weights_chart(dates.y(2022), dates.y(2023), n=3)
```

The weights can be interprted as loadings in factor portfolios

The factor series themselves then being the returns of a portfolio with those loadings

Given the factors, can reconstitute the originalr eturns with:

```{python}
#| code-fold: false
#| code-summary: ""
def pca_pred(d_start, d_end, n):
    pca, df = fit_pca(d_start, d_end, n)
    factors = pca.encode(df)
    return pandas.DataFrame(
        pca.decode(factors),
        columns=pca.columns,
        index=df.index,
    )
pca_pred(dates.y(2022), dates.y(2023), n=3)
```

That is to say, they're both loadings, and when transposed (which is inversion as orthogonal), betas back up from the factor returns to the ticker return

print the decode / encode as just transpose of one another

But a lossy reconstruction if we only select n < len(tickers) factors

```{python}
#| code-fold: false
#| code-summary: ""
```

Can see in above scatter / hist whatever

could even plot the mse as a function of n_factors (declining presumably)

## Sectors

Interpreting is a little difficult - as we can see above, very large weight vector.

We could try re-weighting back against industries.

For instance, given a function to map from a ticker to a sector sector:

```{python}
#| code-fold: false
#| code-summary: ""
sector_tickers = equity.f_ticker_map(
    dfs_sectors,
    fit_pca,
    lambda pca: pca.columns,
)
```

But, some industries are overrpresented (below is proportion of universe per industry)

```{python}
#| code-fold: false
#| code-summary: ""
def sector_map_chart(d_start, d_end, n):
    sector_map = sector_tickers(d_start, d_end, n)
    return graphs.df_bar_chart(
        pandas.DataFrame({
            s: [len(v)] for s, v in sector_map.items()
        }).melt(),
        x="variable",
        y="value",
    )
sector_map_chart(dates.y(2022), dates.y(2023), n=3)
```

GICS denominator:

```{python}
#| code-fold: false
#| code-summary: ""
def pca_sector_weights(d_start, d_end, n):
    weights = pca_weights(d_start, d_end, n)
    sector_map = sector_tickers(d_start, d_end, n)
    sector_weights = pandas.DataFrame({
        s: weights[tickers.pipe(list)].sum(axis=1)
        for s, tickers in sector_map.items()
    })
    return pandas.DataFrame({
        s: sector_weights[s] / len(ts)
        for s, ts in sector_map.items()
    })
```

So have to divide by the proportion in the whole:

```{python}
#| code-fold: false
#| code-summary: ""
def pca_sector_weights_chart(d_start, d_end, n):
    sector_weights = pca_sector_weights(d_start, d_end, n)
    return factor_bar_chart(sector_weights)
pca_sector_weights_chart(dates.y(2022), dates.y(2023), n=3)
```

## Indices

```{python}
#| code-fold: false
#| code-summary: ""
equity.index_tickers
```

same by country

But, some are overrpresented (below is proportion of universe per industry)

```{python}
#| code-fold: false
#| code-summary: ""
def index_map_chart(d_start, d_end, n):
    index_map = index_tickers(d_start, d_end, n)
    return graphs.df_bar_chart(
        pandas.DataFrame({
            i: [len(v)] for i, v in index_map.items()
        }).melt(),
        x="variable",
        y="value",
    )
index_map_chart(dates.y(2022), dates.y(2023), n=3)
```

GICS denominator:

```{python}
#| code-fold: false
#| code-summary: ""
def pca_index_weights(d_start, d_end, n):
    index_map = index_tickers(d_start, d_end, n)
    weights = pca_index_weights(d_start, d_end, n)
    index_w = {
        i: 1 / (len(ts) / len(weights.columns))
        for i, ts in index_map.items()
        if len(ts)
    }
    return pandas.DataFrame({
        i: weights[i] * index_w[i]
        for i in weights.columns
        if i in index_w
    })
```

So have to divide by the proportion in the whole:

```{python}
#| code-fold: false
#| code-summary: ""
def pca_index_weights_chart(d_start, d_end, n):
    index_weights = pca_index_weights(d_start, d_end, n)
    return factor_bar_chart(index_weights)
pca_index_weights_chart(dates.y(2022), dates.y(2023), n=3)
```

## Interpretation

We can see that it did pick out ... during 2022

If we run over the covid period, we can see ...

```{python}
#| code-fold: false
#| code-summary: ""
pca_sector_weights_chart(dates.y(2020), dates.y(2022), n=3)
```

For instance, can see fnancials GC

```{python}
#| code-fold: false
#| code-summary: ""
pca_sector_weights_chart(dates.y(2007), dates.y(2010), n=3)
```

We can see the energy criss 2015

```{python}
#| code-fold: false
#| code-summary: ""
pca_sector_weights_chart(dates.y(2014), dates.y(2016), n=3)
```
 
Also visible in the norway weighting for the same period:

```{python}
#| code-fold: false
#| code-summary: ""
pca_index_weights_chart(dates.y(2014), dates.y(2016), n=3)
```

Whereas we can see the financials / periphery (spain and portgual) for the euro crisis:

```{python}
#| code-fold: false
#| code-summary: ""
pca_index_weights_chart(dates.y(2010), dates.y(2013), n=3)
```

## Limitations

However, as we can see above - the weights aren't necessarily meaningful

Depending on the year, they can then be manually interpreted - as we saw above

But only where such struccture happens to be orthogonal variance maximising

Simply picks out the dimensions of maximum spreading (after adjusting for previous such dimensions)

Also, can't account for missing data (would need to null fill, for instance)

## Next Steps

One way to deal better with missing data, is to look at instrumenting our weights

So, instead of a weight per ticker per factor, we have a function from a set of features per ticker to a weight per factor.

This will also have the nice benefit of potentially making the factors more meaningful (as fucntions of the features).

However, to get there, we'll first have to rephase the above as a gradient descent problem.

Which we'll look at next.