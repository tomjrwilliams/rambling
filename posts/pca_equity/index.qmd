---
title: "PCA: Equity Returns"
author: "Tom Williams"
date: "2023-08-08"
categories: []
draft: false
execute:
  daemon: false
---

In this post, we try out using PCA to extract market factors from daily equity returns.

## Setup

```{python}
#| echo: false
#| code-fold: true
#| code-summary: "Auto reload"
%load_ext autoreload
%autoreload 2
```

```{python}
#| echo: false
#| code-fold: true
#| code-summary: "Environment"
import os
import sys
import importlib
sys.path.append("C:/hc/src")
sys.path.append("C:/hc/rambling")
sys.path.append("C:/hc/xfactors/src")
sys.path.append("C:/hc/xtuples/src")
os.environ["MODULE"] = "c:/hc/src/"
```

```{python}
#| code-fold: false
#| code-summary: "Imports"
import numpy
import pandas
import jax
import jax.numpy

import xtuples as xt
import xfactors as xf

import bt.data.prices.int as prices
import bt.algos.universe.int as universe
import bt.algos.universe.configs as configs
```

### Data

We'll start by loading some daily close to close total returns data, given a rolling universe of major european equity indices:

```{python}
#| code-fold: false
#| code-summary: "Returns"
df_returns = prices.returns_df(
    xf.dates.y(2005),
    xf.dates.y(2023, m=4),
    indices=configs.INDICES,
) 
```

The returns are from bloomberg, and include all cash and non-cash adjustments (in production we have a separate internal ticker for the dividend stream from a given name, but that's a little over-complicated for our purposes here).

We'll also load the relevant index membership mapping tables for our universe:

```{python}
#| code-fold: false
#| code-summary: "Index Membership"
dfs_indices = universe.rolling_indices(
    xf.dates.y(2005),
    xf.dates.y(2023, m=4),
    indices=configs.INDICES,
)
df_universe = universe.index_union(dfs_indices)
```

And some GICS sector mapping tables, as these will be useful later:

```{python}
#| code-fold: false
#| code-summary: "Sector Membership"
dfs_sectors = universe.rolling_indices(
    xf.dates.y(2005),
    xf.dates.y(2023, m=4),
    sectors=configs.GICS_SECTORS,
)
```

Vanilla PCA doesn't accomodate missing data, so given a particular target date range, we'll filter to only those tickers within our universe for the entire period, as so:

```{python}
#| code-fold: false
#| code-summary: "Universe"
def in_universe(ticker, df, threshold = 1.):
    if ticker not in df.columns:
        return False
    return (
        df[ticker].sum() / len(df.index)
    ) >= threshold
```

Which we'll then wrap into a convenience function.

```{python}
#| code-fold: true
#| code-summary: "get_returns"
def get_returns(d_start, d_end, threshold=1.):
    tickers = xt.iTuple(df_returns.columns).filter(
        in_universe,
        df=xf.dfs.index_date_filter(df_universe, d_start, d_end),
        threshold=threshold,
    ).pipe(list) 
    return xf.dfs.index_date_filter(
        df_returns, d_start, d_end
    )[tickers].dropna()
```

### Model

[PCA](../pca/index.qmd) uses eigendecomposition to extract an [orthogonal](../orthogonality/index.qmd) embedding matrix, together with a diagonal scaling matrix, from the covariance matrix of a given data set.

```{python}
#| code-fold: false
#| code-summary: "PCA"
import functools
@functools.lru_cache(maxsize=10)
def fit_pca(d_start, d_end, n):
    df = get_returns(d_start, d_end)
    eigvals, eigvecs = jax.numpy.linalg.eig(
        jax.numpy.cov(df.values.T)
    )
    order = jax.numpy.flip(jax.numpy.argsort(eigvals))
    eigvals = eigvals.real[order[:n]]
    eigvecs = eigvecs.real[:, order[:n]]
    return eigvals, eigvecs, df
_ = fit_pca(xf.dates.y(2022), xf.dates.y(2023), n=3)
```

The embedding matrix can then be used to project the data into a lower dimensional factor space, where the relative 'importance' of each factor is given by the relevant term in the diagonal scaling matrix (importance being defined as how much of the original data variance each factor 'explains').

```{python}
#| code-fold: false
#| code-summary: "Apply PCA"
import functools
def apply_pca(d_start, d_end, n):
    eigvals, eigvecs, df = fit_pca(d_start, d_end, n)
    return eigvals, eigvecs, df.values @ eigvecs, df
```

See the reference post [PCA](../pca/index.qmd) for a more detailed walk-through of the linear algebra.

### Results

As expected, the factors we extract are linearly independent (have diagonal covariance / correlation):

```{python}
#| code-fold: false
#| code-summary: "PCA Factor Corr"
def pca_factor_corr(d_start, d_end, n):
    _, _, factors, _ = apply_pca(d_start, d_end, n)
    return xf.rendering.render_df_color_range(
        pandas.DataFrame(factors).corr(),
        v_min=-1.,
        v_max=.1,
    )
pca_factor_corr(xf.dates.y(2022), xf.dates.y(2023), n=3)
```

And are each univariate gaussian, with descending variance from PC0 down to PC2 (seen in the increasing concentration of the gaussians in the plot below):

```{python}
#| code-fold: false
#| code-summary: "PCA Factor Density"
def pca_factor_density_chart(d_start, d_end, n):
    _, _, factors, df = apply_pca(d_start, d_end, n)
    return xf.graphs.df_density_chart(
        xf.dfs.melt_with_index(
            pandas.DataFrame(
                factors,
                columns=list(range(factors.shape[1])),
                index=df.index,
            ),
            variable_as="factor",
        ),
        "factor",
        "value",
    )
pca_factor_density_chart(xf.dates.y(2022), xf.dates.y(2023), n=3)
```

Turning to our factor weights:

```{python}
#| code-fold: true
#| code-summary: "Factors"
def pca_weights(d_start, d_end, n):
    _, eigvecs, _, df = apply_pca(d_start, d_end, n)
    return pandas.DataFrame(
        eigvecs.T,
        columns=df.columns,
        index=list(range(eigvecs.shape[1]))
    )
pca_weights(xf.dates.y(2022), xf.dates.y(2023), n=3)
```

One way to visualise them is with a simple bar chart:

```{python}
#| code-fold: false
#| code-summary: "Factor Chart"
def pca_factor_weight_chart(d_start, d_end, n):
    weights = pca_weights(d_start, d_end, n)
    n = len(weights.index)
    return xf.graphs.df_facet_bar_chart(
        xf.dfs.melt_with_index(weights, index_as="factor"),
        x="variable",
        y="value",
        facet="factor",
    )
pca_factor_weight_chart(xf.dates.y(2022), xf.dates.y(2023), n=3)
```

However, as one can see, given that we end up with a single weight per ticker per factor, it's fairly difficult to know how to interpret each of the above.

The one thing we can tell from the above, is that our first factor is (more or less) the same sign for equity ticker in our universe, representing some kind of general market beta factor (a point we'll return to in the discussion below).

### Sector & Index Aggregation

To try and make sense of our weights, we can try averaging them up into something more manageable.

For instance, using a function mapping us from a ticker to a GICS sector:

```{python}
#| code-fold: true
#| code-summary: "Sector Tickers"
def sector_tickers(d_start, d_end, n):
    _, _, df = fit_pca(d_start, d_end, n)
    tickers = xt.iTuple(df.columns)
    return {
        s: tickers.filter(
            in_universe, df = dfs_sectors[s], threshold=1.
        ) for s in sorted(dfs_sectors.keys())
    }
```

Certain industries will be under or over-represented in our universe, in a given period:

```{python}
#| code-fold: false
#| code-summary: "Sector Representation"
def sector_map_chart(d_start, d_end, n):
    sector_map = sector_tickers(d_start, d_end, n)
    return xf.graphs.df_bar_chart(
        pandas.DataFrame({
            s: [len(v)] for s, v in sector_map.items()
        }).melt(),
        x="variable",
        y="value",
    )
sector_map_chart(xf.dates.y(2022), xf.dates.y(2023), n=3)
```

So, we'll scale our average relative to the sector representation during the period in question, returning something like the below:

```{python}
#| code-fold: true
#| code-summary: "Sector Weights Chart"
def pca_sector_weights_chart(d_start, d_end, n):
    weights = pca_weights(d_start, d_end, n)
    sector_map = sector_tickers(d_start, d_end, n)
    sector_weights = pandas.DataFrame({
        s: weights[tickers.pipe(list)].sum(axis=1)
        for s, tickers in sector_map.items()
    })
    scaled_weights = pandas.DataFrame({
        s: sector_weights[s] / len(ts)
        for s, ts in sector_map.items()
    })
    return xf.graphs.df_facet_bar_chart(
        xf.dfs.melt_with_index(scaled_weights, index_as="factor"),
        x="variable",
        y="value",
        facet="factor",
    )
pca_sector_weights_chart(xf.dates.y(2022), xf.dates.y(2023), n=3)
```

As one can see, this is much more easily interpretible.

We can go through a similar process for averaging our weights by equity index.

```{python}
#| code-fold: true
#| code-summary: "Index Tickers"
def index_tickers(d_start, d_end, n):
    _, _, df = fit_pca(d_start, d_end, n)
    tickers = xt.iTuple(df.columns)
    return {
        i: tickers.filter(
            in_universe, df = dfs_indices[i], threshold=1.
        ) for i in sorted(dfs_indices.keys())
    }
```

Where, again, certain indices will be over or under-represented in our universe:

```{python}
#| code-fold: false
#| code-summary: "Index Representation"
def index_map_chart(d_start, d_end, n):
    index_map = index_tickers(d_start, d_end, n)
    return xf.graphs.df_bar_chart(
        pandas.DataFrame({
            i: [len(v)] for i, v in index_map.items()
        }).melt(),
        x="variable",
        y="value",
    )
index_map_chart(xf.dates.y(2022), xf.dates.y(2023), n=3)
```

Which we'll account for in our averaging just as we did for our sectors, giving us back something like the below:

```{python}
#| code-fold: true
#| code-summary: "Index Weights Chart"
def pca_index_weights_chart(d_start, d_end, n):
    weights = pca_weights(d_start, d_end, n)
    index_map = index_tickers(d_start, d_end, n)
    index_weights = pandas.DataFrame({
        i: weights[tickers.pipe(list)].sum(axis=1)
        for i, tickers in index_map.items()
    })
    scaled_weights = pandas.DataFrame({
        i: index_weights[i] / len(ts)
        for i, ts in index_map.items()
    })
    return xf.graphs.df_facet_bar_chart(
        xf.dfs.melt_with_index(index_weights, index_as="factor"),
        x="variable",
        y="value",
        facet="factor",
    )
pca_index_weights_chart(xf.dates.y(2022), xf.dates.y(2023), n=3)
```

With which, we can get back to the fun part - interpretation!

### Interpretation - By Sector

Let's try plotting our factor sector weights for the period 2020 - 2022:

```{python}
#| code-fold: false
#| code-summary: "Sector Weights: 2020 - 2022"
pca_sector_weights_chart(xf.dates.y(2020), xf.dates.y(2022), n=3)
```

As one can see, the first factor has the same sign for every sector, and if we dig into particular components, seems to roughly match our intuitive notion of index beta (ie. higher beta sectors having larger weights):

- Higher beta: 10 (Energy), 20 (Industrials), 25 (Discretionary), 40 (Financials).
- Lower beta: 30 (Staples), 35 (Health), 50 (Media / Telcos), 55 (Utilities).

Whilst the exact loadings will vary from period to period, the first - and most important (in a variance-explanatory sense) - component will nearly always tend to be some kind of general market-beta factor.

The second and third factors will, however, vary a little more from period to period.

For instance, whilst for 2020 - 2022, the second factor seems to be picking up something like a risk-on vs risk-off factor, separating between:

- Energy (10), Financials (40), Real Estate (60).
- Materials (15), Health(35), Tech (45), Utilities (55).

If we run the same procedure over 2007 to 2010:

```{python}
#| code-fold: false
#| code-summary: "Sector Weights: 2007 - 2010"
pca_sector_weights_chart(xf.dates.y(2007), xf.dates.y(2010), n=3)
```

Our second factor is almost entirely dominated by Financials and Real Estate (40 and 60) versus everything else, and our third by Energy and Materials (10 and 15) versus everything else.

This, given the 2007-2010 (housing driven) financial crisis (and the ensuing commodity market reaction to global growth concerns), does actually make sense.

Let's now compare to the period 2014 to 2016:

```{python}
#| code-fold: false
#| code-summary: "Sector Weights: 2014 - 2016"
pca_sector_weights_chart(xf.dates.y(2014), xf.dates.y(2016), n=3)
```
 
Here, the Energy and Materials (10 and 15) vs everything else is much more prominent, given the global commodity wobble (at least partly driven by the rise in US shale oil production). 

We also see an extremely concentrated third component of financials (40) versus everything else, probably driven by another flare up of the 2010's EU sovereign debt crises.

What we're seeing here is that because PCA doesn't have any underlying model for how different equities relate together - it just strips out the ways in which they happened to co-move during a given period - it doesn't have anything to anchor itself on, and thus returns (samples of) somewhat different factors, depending on which period we run it on.

Whilst this might reflect a genuine causal difference in the factors driving overall single-name equity returns during different periods, it also might reflect sheer co-occurence (correlation, as they say, does not necessarily mean causation).

### Interpretation - By Index

We get similar results (though personally much less useful), if we instead aggregate by index.

For instance, we can see the same commodity wobble in the Norwegian (OBX) and to some extent UK (UKX) weighting over 2014 to 2016 (given their respective commodity exposures):

```{python}
#| code-fold: false
#| code-summary: "Index Weights: 2014 - 2016"
pca_index_weights_chart(xf.dates.y(2014), xf.dates.y(2016), n=3)
```

Whereas we can see something like a peripherals (Spain & Portugal) vs European core factor, if we run over the peak of the EU sovereign debt crisis (2010-2013):

```{python}
#| code-fold: false
#| code-summary: "Index Weights: 2010 - 2013"
pca_index_weights_chart(xf.dates.y(2010), xf.dates.y(2013), n=3)
```

Where we can see how the EU financials index (SX7E) is moving with the IBEX and PSI (lending support to the idea that this is some kind of financial stress factor).

### Limitations

As noted above, vanilla PCA - at least in the eigendecomposition form above - requires a full data matrix, with no missing values. This can be quite limiting, especially when dealing with real data (as opposed to the above, somewhat stylised, daily equity returns).

That being said, this is fairly simple to deal with via, for instance, an [irregular covariance](../pca_equity_irregulary/index.qmd) matrix, or via some kind of [kernel](../kernel_pca/index.qmd) function.

More concering, as we've seen, is that without any kind of prior model for how different features 'should' be related to one another, the factors we extract can often vary quite significantly when ran over different samples periods.

Indeed, given that we fit a single weight for each ticker in our universe in a given period, the problem is in fact somewhat worse. Under the hood of the sector / index aggregation above, our weights were defined over entirely different sets of tickers (specifically, their different daily return covariances), as our universe rolled through time.

As our universe changes, we're thus left without any factor weights for new tickers entering our universe (forcing us, for instance, to continually refit our factor matrices).

There are, however, a number of ways that we can deal with this - for instance, by first mapping our tickers into some kind of latent space, before assigning them factor weights (see [here](../lgp_pca_equity/index.qmd)).

### Related posts

See [here](../pca_rates/index.qmd) for a similar post using PCA to decompose bond / swap yield curve factors.

See [here](../pca/index.qmd) for a toy PCA example intended to help build up intuition for the geometry of the eigendecomposition above, and [here](../orthogonality/index.qmd) for some notes on orthogonality (on which said decomposition relies).