---
title: "Rates: Tenor Kernel"
author: "Tom Williams"
date: "2023-08-08"
categories: []
draft: false
execute:
  daemon: false
---

In this post, we demonstrate how to design a yield curve covariance kernel function over tenor space.

We then show both how to use such a kernel to derive synthetic covariance matrices (including tenors for which we have no market data), and hence also how to generate synthetic factor loadings (again, including tenors for which we have no market data) using the eigen decomposition form of [pca](../pca).

## Setup

```{python}
#| echo: false
#| code-fold: true
#| code-summary: "Auto reload"
%load_ext autoreload
%autoreload 2
```

```{python}
#| echo: false
#| code-fold: true
#| code-summary: "Environment"
import os
import sys
import importlib
sys.path.append("C:/rambling")
sys.path.append("C:/xtuples/src")
sys.path.append("C:/xfactors/src")
os.chdir("c:/xfactors")
```

```{python}
#| code-fold: false
#| code-summary: "Imports"
import functools
import numpy
import pandas
import jax
import jax.numpy

import xtuples as xt
import xfactors as xf
```

### Data

We'll begin by loading the same yield curve data as for our post on [rates pca](../rates_pca).

```{python}
#| code-fold: false
#| code-summary: "Returns"
dfs_curves = xf.bt.data.curves.curve_dfs(
    curves=xt.iTuple([
        "YCSW0023",
        "YCGT0025",
        "YCSW0045",
        "YCGT0016",
    ]),
    dp="../xfactors/__local__/csvs"
)
dfs_curves = {
    curve: xf.utils.dfs.apply_na_threshold(
        df, na_threshold=(0., 0.,), na_padding=(0.2, 0.4,)
    )
    for curve, df in dfs_curves.items()
}
```

### Covariance

As one can see, rates curves generally have a particular structure to their covariance matrix.

```{python}
#| code-fold: false
#| code-summary: "Yield Curve Covariance"
def curve_cov(curve, d_start, d_end, v_min = 0.):
    df = dfs_curves[curve]
    df = xf.utils.dfs.index_date_filter(df, date_start=d_start, date_end=d_end)
    return xf.visuals.rendering.render_df_color_range(
        df.cov(),
        v_min=v_min,
    )
curve_cov("USD-S", xf.utils.dates.y(2005), xf.utils.dates.y(2023))
```

In particular:

- (co-) variance is generally downward sloping with maturity
- tenors closer together generally covary more

In the previous post on [rates pca](../rates_pca), we used this covariance matrix to extract a set of factors, able to describe the path of the yield curve through time in terms of it's level, slope, and curvature.

Crucially, our decomposition only directly depended on the covariance matrix - and not the raw data itself.

```{python}
#| code-fold: true
#| code-summary: "Sector Weights Chart"
def extract_loadings(cov, n):
    eigvals, eigvecs = jax.numpy.linalg.eig(cov)
    order = numpy.flip(numpy.argsort(eigvals))[:n]
    eigvecs = xf.utils.funcs.set_signs_to(
        eigvecs[..., order].real, axis=1, i=0, signs=[1, -1, -1]
    )
    return eigvals[order].real, eigvecs
```

As such, if we can find a way to generate reasonable covariance matrices, then we can use this to also generate factor loading matrices.

### Kernels

A covariance kernel is a function from pairs of locations in a given (potentially infinite dimensional) input space, to their covariance in a given data space.

Let's say that we can define such a kernel over an input space consisting of all the possible tenors on a rate curve (market traded or not).

This would then allow us to generate a covariance matrix between pairs of such tenors - again, whether market traded or not.

With this, we can then derive a matrix of factor loadings for our tenor set - even for those for which we have no observed market data.

### Linear kernel

As we can see above, there's a clear downward slope in our covariance matrix, with the tenor maturity.

As such, we might start with a simple linear kernel, wherein the covariance between two points is proportional to the product of their differences from a given fixed point.

We will not yet add our constraints, for reasons that will become apparent below.

```{python}
#| code-fold: true
#| code-summary: "Sector Weights Chart"
def tenor_kernel_linear_model(n_kernels):
    model = xf.Model()

    model, loc_cov = model.add_node(xf.inputs.dfs.DataFrame_Wide(
        allow_missing_columns=True,
        allow_missing_indices=False,
        allow_new_columns=True,
        allow_new_indices=False,
        na_threshold_columns=0.,
        na_threshold_indices=0.,
    ), input = True)
    model, loc_tenors = model.add_node(xf.inputs.nds.NDArray(), input = True)
    
    model, loc_a = model.add_node(xf.params.random.Gaussian(
        (n_kernels,)
    ))
    model, loc_c_raw = model.add_node(xf.params.random.Gaussian(
        (n_kernels,)
    ))
    model, loc_sigma = model.add_node(xf.params.random.Gaussian(
        (n_kernels,)
    ))
    model, loc_c = model.add_node(xf.transforms.scaling.Expit(
        loc_c_raw.param(),
    ))
    model, loc_kernel = model.add_node(xf.cov.kernels.VKernel_Linear(
        a=loc_a.param(),
        c=loc_c.result(),
        sigma=loc_sigma.param(),
        data=loc_tenors.result(),
        sigma_sq=False,
    ))
    return model, dict(
        data=loc_data,
        tenors=loc_tenors,
        cov=loc_cov,
        kernel = dict(
            a=loc_a,
            c_raw=loc_c_raw,
            sigma=loc_sigma,
            c=loc_c,
            kernel=loc_kernel,
        )
    )
```
    
We can fit this model as so:

```{python}
#| code-fold: true
#| code-summary: "Sector Weights Chart"
def fit_tenor_kernel_linear(curve, d_start, d_end, n = 3):
    df = dfs_curves[curve]
    df = xf.utils.dfs.index_date_filter(df, date_start=d_start, date_end=d_end)

    model, locs = tenor_kernel_linear_model(1)
    model, loc_agg = model.add_node(xf.cov.kernels.Kernel_Sum(
        kernel=locs["kernel"]["kernel"].result()
    ))
    model, _ = model.add_node(xf.constraints.loss.MSE(
        loc_agg.result(), locs["cov"].result()
    ), constraint=True)

    data = (df.cov(), numpy.linspace(0, 1, len(df.columns)),)

    model = model.init(data).optimise(
        data,
        iters = 5000,
        rand_init=10, 
        max_error_unchanged=0.5,
    )
    results = model.apply(data)
    
    cov = loc_agg.result().access(results)

    return model, {
        **locs,
        "agg": loc_agg,
    }, cov
```

Which results in something like the below:

```{python}
#| code-fold: true
#| code-summary: "Sector Weights Chart"
def plot_kernel_linear_cov(curve, d_start, d_end):
    model, locs, cov = fit_tenor_kernel_linear(curve, d_start, d_end)
    return model, locs, xf.visuals.rendering.render_df_color_range(
        pandas.DataFrame(
            cov,
            index=df.columns,
            columns=df.columns,
        ),
    )
_, _, p = plot_kernel_linear_cov("USD-S", xf.utils.dates.y(2005), xf.utils.dates.y(2023))
p
```

Which, if we compare to our original covariance matrix (this time, dropping the v_min clamp so we can see the differences more clearly), is already a pretty good approximation:

```{python}
#| code-fold: true
#| code-summary: "Sector Weights Chart"
curve_cov("USD-S", xf.utils.dates.y(2005), xf.utils.dates.y(2023), v_min=None)
```

### Linear 

However, as we can see, ...

As such, we'll try adding a second linear kernel, with the idea being that where the first ..., this will ...

```{python}
#| code-fold: true
#| code-summary: "Sector Weights Chart"
def tenor_kernel_linear_2_model(n_kernels):

    model, locs = tenor_kernel_linear_model(1)

    model, loc_a = model.add_node(xf.params.random.Gaussian(
        (1,) # a
    ))
    model, loc_c_raw = model.add_node(xf.params.random.Gaussian(
        (1, 2,) # c
    ))
    model, loc_sigma = model.add_node(xf.params.random.Gaussian(
        (1,) # sigma
    ))
    model, loc_c = model.add_node(xf.scaling.scalar.Scale_Expit(
        data=loc_c_raw.param(),
    ))
    model, loc_kernel = model.add_node(
        xf.cov.kernels.VKernel_VLinear(
            a=loc_a.param(),
            c=loc_c.result(),
            sigma=loc_sigma.result(),
            data=locs["tenors"].result(),
            sigma_sq=False,
        )
    )
    return model, {
        **{
            k: v for k, v in locs.items()
            if k != "kernel"
        },
        **dict(kernels = xt.iTuple([
            locs["kernel"],
            dict(
                a=loc_a,
                c_raw=loc_c_raw,
                sigma=loc_sigma,
                c=loc_c,
                kernel=loc_kernel,
            )
        ]))
    }

```

Which we can fit in a similar way to before:

```{python}
#| code-fold: true
#| code-summary: "Sector Weights Chart"
def fit_tenor_kernel_linear_2(curve, d_start, d_end, n = 3):
    df = dfs_curves[curve]
    df = xf.utils.dfs.index_date_filter(df, date_start=d_start, date_end=d_end)

    model, locs = tenor_kernel_linear_2_model(1)

    model, loc_agg = model.add_node(KERNEL_AGG, xf.cov.kernels.Kernel_Sum(
        kernels=locs["kernels"].map(lambda d: d["kernel"].result())
    ))
    model, _ = model.add_npdexf.constraints.loss.Constraint_MSE(
        l=xf.Loc.result(KERNEL_AGG, 0),
        r=xf.Loc.result(SETUP, 0),
    ), constraint=True)

    data = (df.cov(), numpy.linspace(0, 1, len(df.columns)),)

    model = model.init(data).optimise(
        data,
        iters = 5000,
        rand_init=10, 
        max_error_unchanged=0.5,
    )
    results = model.apply(data)

    cov = loc_agg.result().access(results)

    return model, {
        **locs,
        "agg": loc_agg,
    }, cov
```

And which results in something like the below:

```{python}
#| code-fold: true
#| code-summary: "Sector Weights Chart"
def plot_kernel_linear_2_cov(curve, d_start, d_end):
    model, locs, cov = fit_tenor_kernel_linear_2(curve, d_start, d_end)
    return model, locs, xf.visuals.rendering.render_df_color_range(
        pandas.DataFrame(
            cov,
            index=df.columns,
            columns=df.columns,
        ),
    )
_, _, p = plot_kernel_linear_2_cov("USD-S", xf.utils.dates.y(2005), xf.utils.dates.y(2023))
p
```

And which, as we can see, does seem to have ...

### Kernel Linear + RBF

However, as we can see, we're still ...

As such, we might try adding an RBF kernel, which increases the covariance of pairs of nearby points.

```{python}
#| code-fold: true
#| code-summary: "Sector Weights Chart"
def tenor_kernel_rbf_linear_2_model(n_kernels):

    model, locs = tenor_kernel_linear_model_2(1)

    model, loc_sigm = model.add_node(xf.params.random.Gaussian(
        (1,) # sigma (rbf)
    ))
    model, loc_l = model.add_node(xf.params.random.Gaussian(
        (1,) # l (rbf)
    ))
    model, loc_kernel = model.add_node(
        xf.cov.kernels.Kernel_RBF(
            sigma=loc_sigma.param(),
            l=loc_l.param(),
            data=locs["tenors"].result(),
        ))
    )
    return model, {
        **{
            k: v for k, v in locs.items()
            if k != "kernels"
        },
        **dict(kernels = locs["kernels"].append(dict(
            sigma=loc_sigma,
            l = loc_l,
            kernel=loc_kernel,
        )))
    }
```

Which we can fit as before:

```{python}
#| code-fold: true
#| code-summary: "Sector Weights Chart"
def fit_tenor_kernel_rbf_linear_2(curve, d_start, d_end, n = 3):
    df = dfs_curves[curve]
    df = xf.utils.dfs.index_date_filter(df, date_start=d_start, date_end=d_end)

    model, locs = tenor_kernel_rbf_linear_2_model(1)

    model, loc_agg = model.add_node(xf.cov.kernels.Kernel_Sum(
        kernels=locs["kernels"].map(lambda d: d["kernel"].result())
    ))
    model, _ = model.add_node(xf.constraints.loss.Constraint_MSE(
        l=loc_agg.result(),
        r=locs["cov"].result(),
    ))
    
    data = (df.cov(), numpy.linspace(0, 1, len(df.columns)),)

    model = model.init(data).optimise(
        data,
        iters = 5000,
        rand_init=10, 
        max_error_unchanged=0.5,
    )
    results = model.apply(data)

    cov = loc_agg.result().access(results)

    return model, {
        **locs,
        "agg": loc_agg,
    }, cov
```

And which results in something like the below:

```{python}
#| code-fold: true
#| code-summary: "Sector Weights Chart"
def plot_kernel_rbf_linear_2_cov(curve, d_start, d_end):
    model, locs, cov = fit_tenor_kernel_rbf_linear_2(curve, d_start, d_end)
    return model, locs, xf.visuals.rendering.render_df_color_range(
        pandas.DataFrame(
            cov,
            index=df.columns,
            columns=df.columns,
        ),
    )
model, locs, p = plot_kernel_rbf_linear_2_cov(
    "USD-S", xf.utils.dates.y(2005), xf.utils.dates.y(2023)
)
p
```

As we can see, this ...

### Kernel interpolation

Let's now try applying our model on an expanded tenor set.

```{python}
#| code-fold: true
#| code-summary: "Sector Weights Chart"
@functools.lru_cache(max_size=5)
def plot_synthetic_covariance(tenors):

    results = model.apply((None, numpy.linspace(0, 1, len(tenors))))
    
    cov = locs["agg"].result().access(results)

    return cov, xf.visuals.rendering.render_df_color_range(
        pandas.DataFrame(
            cov,
            index=tenors,
            columns=tenors,
        ),
    )
_, p = plot_synthetic_covariance(xt.iTuple([

]))
p
```

As one can see, ...

Let's now try deriving a set of synthetic factor loadings from our synthetic covariance:

```{python}
#| code-fold: true
#| code-summary: "Sector Weights Chart"
def plot_synthetic_loadings(tenors):
    cov, _ = plot_synthetic_covariance(tenors)

    eigvals, eigvecs = extract_loadings(cov, 3)

    weights = pandas.DataFrame(
        eigvecs.T,
        columns=tenors,
        index=list(range(3))
    )
    return xf.visuals.graphs.df_facet_bar_chart(
        xf.utils.dfs.melt_with_index(weights, index_as="factor"),
        x="variable",
        y="value",
        facet="factor",
    )
plot_synthetic_covariance(xt.iTuple([

]))
```

As one can see, ...

### Conclusion

...