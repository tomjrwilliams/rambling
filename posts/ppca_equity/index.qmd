---
title: "Example: PPCA, Equity Returns"
author: "Tom Williams"
date: "2023-07-17"
categories: []
draft: false
execute:
  daemon: false
---

This is the second post in a series of examples I'm writing for the factor decomposition library xfactors, that I'm currently in the process of open-sourcing.

Here, we'll run PPCA on daily equity returns, demonstrating handle missing data, recover the same factors as standard pca.

## Setup

```{python}
#| code-fold: true
#| code-summary: "Auto reload"
%load_ext autoreload
%autoreload 2
```

```{python}
#| code-fold: true
#| code-summary: "Environment"
import os
import sys
import importlib
sys.path.append("C:/hc/src")
sys.path.append("C:/hc/rambling")
sys.path.append("C:/hc/hc-core/src")
sys.path.append("C:/hc/xfactors/src/xfactors")
sys.path.append("C:/hc/xtuples/src/xtuples")
os.environ["MODULE"] = "c:/hc/src/"
```

```{python}
#| code-fold: true
#| code-summary: "Imports"
import datetime
import functools
import itertools

import numpy
import pandas
import jax
import jax.numpy

import xtuples
import xfactors

import src.equity as equity

import hc_core.imports as imports
import hc_core.rendering as rendering
import hc_core.dfs as dfs
import hc_core.densities as densities
import hc_core.graphs as graphs
import hc_core.dates as dates
import hc_core.caching as caching

import bt.data.prices.int
import bt.algos.universe.int

graphs.set_rendering("NA")
```

## Data

As before, work on daily close to close total returns, on a universe of major european equity indices:

```{python}
#| code-fold: false
#| code-summary: "Data"
df_returns = bt.data.prices.int.returns_df(
    dates.y(2005),
    dates.y(2023, m=4),
    indices=bt.algos.universe.configs.INDICES,
) 
```

We don't need to worry about missing data this time, so no need for a universe filtering functon, though we will define a simple date filtering wrapper function:

```{python}
#| code-fold: false
#| code-summary: "Universe"
def get_returns( d_start, d_end, threshold=1.):
    tickers = xtuples.iTuple(df_returns.columns).filter(
        equity.in_universe,
        df=dfs.index_date_filter(df_universe, d_start, d_end),
        threshold=threshold,
    ).pipe(list) 
    return dfs.index_date_filter(
        df_returns, d_start, d_end
    )[tickers].dropna()
```

We will, however, still need the index and sector data for aggregating up the weights into more interpretible buckets.

First, the indices:

```{python}
#| code-fold: false
#| code-summary: "Data"
dfs_indices = bt.algos.universe.int.rolling_indices(
    dates.y(2005),
    dates.y(2023, m=4),
    indices=bt.algos.universe.configs.INDICES,
)
df_universe = bt.algos.universe.int.index_union(dfs_indices)
```

Then the sectors:

```{python}
#| code-fold: false
#| code-summary: "Data"
dfs_sectors = bt.algos.universe.int.rolling_indices(
    dates.y(2005),
    dates.y(2023, m=4),
    sectors=bt.algos.universe.configs.GICS_SECTORS,
)
```

Though we'll leave building up the sector maps to once we have our model.

## Model

As we saw in the previous post, PCA takes a matrix of (n_tickers, n_dates)

Returns

Weight matrix of (n_tickers, n_factors)
Eigenvalues (n_factors)
Factors (n_factors, n_dates)

Where it does this by eigendecomposition of the covariance matrix of the input data.

PPCA operates slightly differently, fitting an equivalent set of weights usign gradient descent:

```{python}
#| code-fold: false
#| code-summary: "PCA"
rendering.render_source(xfactors.PPCA.fit, cls_method=True)
```

Each step, we update our parameters by moving in the oposite direction to the gradient of our loss fucntion:

```{python}
#| code-fold: false
#| code-summary: "PCA"
rendering.render_source(xfactors.PPCA.update, cls_method=True)
```

Which can be found here:

```{python}
#| code-fold: false
#| code-summary: "PCA"
rendering.render_source(xfactors.PPCA.loss, cls_method=True)
```

We can break this down step by step.

This pushes the eigvals to be descending:

```{python}
#| code-fold: false
#| code-summary: "PCA"
rendering.render_source(xfactors.loss_descending)
```

So that the most important is factor zero.

This then maximises them (as we subtract, not add):

```{python}
#| code-fold: false
#| code-summary: "PCA"
rendering.render_source(xfactors.loss_mse_zero)
```

So we maximise the variance of the projection of our data into factor space.

This pushes the weights to be orthogonal to one another (zero dot product, unit norm)

```{python}
#| code-fold: false
#| code-summary: "PCA"
rendering.render_source(xfactors.loss_orthogonal)
```

This means that we end up with only a signal weight matrix, as orthongaol transpose = inverse.

So, interpreted one way, ticker loadings of a factor portfolio, transposed, beta of tickers to the factor (versus eg. SVD where we end up with two different matrices).

This pushes the factors to be zero center:

```{python}
#| code-fold: false
#| code-summary: "PCA"
rendering.render_source(xfactors.loss_mean_zero)
```

And this to have their variance given by the eigvalues:

```{python}
#| code-fold: false
#| code-summary: "PCA"
rendering.render_source(xfactors.loss_cov_diag)
```

Which together push them to be independent multivariate gaussian, with variance in descending order, as given by the eigenvalues.

Finally, we add a mse term between our original returns and the lossy reconstruction, formed from the betas of our tickers to the factor paths:

```{python}
#| code-fold: false
#| code-summary: "PCA"
rendering.render_source(xfactors.loss_mse)
```

Which, together, should allow us to reproduce the original eigenvalue decompositon approach, as follows:

todo: remove all the pca / ppca
change to eg. get weights
fit model
weights chart etc.

```{python}
#| code-fold: false
#| code-summary: "PCA"
importlib.reload(xfactors)
@caching.lru_cache()
def fit_ppca(d_start, d_end, n):
    df = get_returns(d_start, d_end)
    return xfactors.PPCA.fit(df, n = n), df
_ = fit_ppca(dates.y(2022), dates.y(2023), 3)
```

Note that this isn't the format in the original ppca paper - instead, there they ....

## Results

plot unit norm

Plotting the resulting factor weights:

```{python}
#| code-fold: false
#| code-summary: ""
def ppca_weights(d_start, d_end, n):
    ppca, _ = fit_ppca(d_start, d_end, n)
    return ppca.weights_df()
ppca_weights(dates.y(2022), dates.y(2023), n=3)
```

^^ summary not the whole df

```{python}
#| code-fold: false
#| code-summary: ""
def ppca_factor_cov(d_start, d_end, n):
    ppca, df = fit_ppca(d_start, d_end, n)
    factors = ppca.encode(df)
    print(numpy.cov(factors.T))
    return rendering.render_df_color_range(
        pandas.DataFrame(
            factors,
            # columns=list(range(3)),
            # index=list(range(3)),
        ).corr(),
        v_min=-1.,
        v_max=.1,
    )
ppca_factor_cov(dates.y(2022), dates.y(2023), n=3)
```

And the factor density:

```{python}
#| code-fold: false
#| code-summary: ""
def ppca_factor_density(d_start, d_end, n):
    ppca, df = fit_ppca(d_start, d_end, n)
    factors = ppca.encode(df)
    return densities.gaussian_kde_1d_df(
        {
            i: factors.real[..., i]
            for i in range(n)
        }, 
        clip_quantile=.01
    )
```

We can see that they're (roughly) gaussian distributed:

```{python}
#| code-fold: false
#| code-summary: ""
def ppca_factor_density_chart(d_start, d_end, n):
    return graphs.df_chart(
        ppca_factor_density(d_start, d_end, n),
        x="position",
        y="density",
        color="key",
        title="Factor Density (0.01 Clip)"
    )
ppca_factor_density_chart(dates.y(2022), dates.y(2023), n=3)
```

## Sectors

In universe filter to use for sector / index maps:

```{python}
#| code-fold: false
#| code-summary: "Universe"
rendering.render_source(equity.in_universe)
```


And sector maps:

```{python}
#| code-fold: false
#| code-summary: ""
sector_tickers = equity.f_ticker_map(
    dfs_sectors,
    fit_ppca,
    lambda ppca: ppca.columns,
)
```

Given a helper function for plotting bars:

```{python}
#| code-fold: false
#| code-summary: ""
def factor_bar_chart(weights):
    n = len(weights.index)
    return graphs.df_facet_bar_chart(
        dfs.melt_with_index(weights, index_as="factor"),
        x="variable",
        y="value",
        facet="factor",
    )
```


```{python}
#| code-fold: false
#| code-summary: ""
def ppca_sector_weights(d_start, d_end, n):
    weights = ppca_weights(d_start, d_end, n)
    sector_map = sector_tickers(d_start, d_end, n)
    sector_weights = pandas.DataFrame({
        s: weights[tickers.pipe(list)].sum(axis=1)
        for s, tickers in sector_map.items()
    })
    return pandas.DataFrame({
        s: sector_weights[s] / len(ts)
        for s, ts in sector_map.items()
    })
```

So have to divide by the proportion in the whole:

```{python}
#| code-fold: false
#| code-summary: ""
def ppca_sector_weights_chart(d_start, d_end, n):
    sector_weights = ppca_sector_weights(d_start, d_end, n)
    return factor_bar_chart(sector_weights)
ppca_sector_weights_chart(dates.y(2022), dates.y(2023), n=3)
```



## Indices

Which we can use to build up index maps:

```{python}
#| code-fold: false
#| code-summary: ""
importlib.reload(equity)
index_tickers = equity.f_ticker_map(
    dfs_indices,
    fit_ppca,
    lambda ppca: ppca.columns,
)
```

## Interpretation

Same as for pca

...

## Limitations

Still orthogonal - no 'meaning' just variance maximising projection

Also still can't deal with missing data (one weight per ticker per factor)

## Next steps

And, critically, we're now just in gradient descent land

So extending is straightforward - eg. making the weights functions of other features (like our sector / index labels)