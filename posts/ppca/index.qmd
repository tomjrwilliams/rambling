---
title: "Example: PCA & PPCA"
author: "Tom Williams"
date: "2023-07-31"
categories: []
draft: false
execute:
  daemon: false
---

I'm currently in the process of splitting out some of the factor modelling code that I've written as part of my [day job](https://havencove.com/) into the open-source package [xfactors](https://pypi.org/project/xfactors).

In the previous post, we walked through a first toy example of using xfactors for linear regression - which, frankly, was over-kill given the simplicity of the problem.

So, in this post, we'll take the slightly more involved example of (a very hacky) probabilistic PCA, which should demonstrate the value of the package slightly better.

Note, it's not the optimal EM way to do it from [], but it arguably does at least help build up the intuition for the solution structure.

## Problem

The problem we're trying to solve with PCA is that of finding a weight matrix Q, able to take a given large dimensional feature matrix X, and map it into a much smaller dimensional matrix QX, which nonethless still contains most of the information of X.

- dimensions

### Generative model

We'll begin by walking through a generative model of the matrix X.

First, we'll start by randomly generating some independent gaussian noise, with some random variances:

- code

These are our latent dimensions / factors.

What we actually observe, however, is in observation space, so we'll then generate a set of betas from the factors, to the observations:

- betas

As so:

- Observations

Our goal is now to find a mtrix that can capture most of the information of observations, but in fewer dimensions.

## Naive solution

One should already see the similarity to the previous linear regression toy problem: difference is, there we were given the factors, and we just had to learn weights.

### Factor distribution

Here, we don't know the factor values ctors, and need to learn weights.

What we do know, however, is how we want them to be distributed: we assume that (we can model them) as independent, gaussian.

Note, here, they are by definition, whereas in reality, this is just an implicit assumption we sort of make when we use pca (akin to how when we use linear regresion, we assume ...).

### Solution

So, a naive first soltuino would look like:

- minimise mse between Q X and gaussian noise
- so even just covar(Q, X) is diag
- actual model

Such that each factor to capture a different 'cause' of vriation in the output data.

Model:

- ...

Run:

- ...

### Results

Plot weights:

- weights

Plot factors:

- factors (2d density)

See that it sort of works.

The issue, however, is how to we get back up from the latent space to our original space.

Ideally, we could use a single set of weights to both project down and back up.

## Intermediate Solution

One interesting such set of matrices are orthogonal matrix: it's inverse is it's transpose (can see more here).

- steal a graph from other post.

### Orthogonality

As such, we might constrain our weights to be orthogonal:

- constraint

But, if different variances, need a term for that:

- eigenvalue

Model:

- diag cov
- mse on reproduction
- orthogonal weights (needed given mse on reprod?)
- ...

Run:

- ...

### Results

Weights:

- ...

Factors (now scaled - 1d density overlay - and 2d?):

- ...

Re-map back up:

- Densities

But, we can see that we've lost some variance?

## Final solution

Maybe not needed?

### Variance max

But if is: maximise eigenvals, sorted by size.

- code for constaint

Model:

- ...

Run:

- ...

### Results

Weights

- ...

Factors

- ...

Back up densities:

- ...

## Eigen vector

Given the form of the above, we can come more directly:

- code below
obs = Q diag Qt
qt obs = qt q diag qt
qt obs = I diag qt

Ie. matrix by vector = scaled vector -> Definition of an eigen vector.

Eigen value gives us the cov scaling (given the factor is orthonormal, so no scale change there, but we want a gaussian factor)

### Solution

PCA code:
 
- 

Model: 

- ...

Run:

- 

### Results

Weights

- 

Factors

- 

Re constitute:

- ...

As can see, gives us what we'd get above.

Given that we reproduced a bunch of hacky optimisations to push toewards eigenvectors, that's what we'd expect.

## Loadings

Beta is just cov / var

- 

So can calculate via loadings:

- cov / eigen val (root?)

Giving:

- 

Given the factors, can mat mul back up, compare to obs:

- 

To see that they're essentially betas.

## Next steps

Now that ..., we'll [next](...) turn to real data: equity returns, [here]() for more efficient EM implementation.